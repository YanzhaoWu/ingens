diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/arch/x86/kernel/sys_x86_64.c linux-4.3-osa/arch/x86/kernel/sys_x86_64.c
--- linux-4.3-org/arch/x86/kernel/sys_x86_64.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/arch/x86/kernel/sys_x86_64.c	2016-03-04 19:57:43.645958822 -0600
@@ -140,7 +140,10 @@
 		return -ENOMEM;
 
 	if (addr) {
-		addr = PAGE_ALIGN(addr);
+		if (flags & MAP_HPAGE) 
+			addr = HPAGE_ALIGN(addr);
+		else
+			addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (end - len >= addr &&
 		    (!vma || addr + len <= vma->vm_start))
@@ -148,6 +151,10 @@
 	}
 
 	info.flags = 0;
+	if (flags & MAP_HPAGE) 
+		info.map_flags = MAP_HPAGE;
+	else
+		info.map_flags = 0;
 	info.length = len;
 	info.low_limit = begin;
 	info.high_limit = end;
@@ -183,7 +190,10 @@
 
 	/* requesting a specific address */
 	if (addr) {
-		addr = PAGE_ALIGN(addr);
+		if (flags & MAP_HPAGE) 
+			addr = HPAGE_ALIGN(addr);
+		else
+			addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len >= addr &&
 				(!vma || addr + len <= vma->vm_start))
@@ -191,6 +201,11 @@
 	}
 
 	info.flags = VM_UNMAPPED_AREA_TOPDOWN;
+	if (flags & MAP_HPAGE) 
+		info.map_flags = MAP_HPAGE;
+		//info.map_flags = 0;
+	else
+		info.map_flags = 0;
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = mm->mmap_base;
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/arch/x86/kvm/kvm_osa.c linux-4.3-osa/arch/x86/kvm/kvm_osa.c
--- linux-4.3-org/arch/x86/kvm/kvm_osa.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/arch/x86/kvm/kvm_osa.c	2016-03-04 19:57:43.649958822 -0600
@@ -0,0 +1,27 @@
+#include <osa/kvm.h>
+#include <linux/printk.h>
+#include <linux/mm_types.h>
+#include <linux/page-flags.h>
+
+int osa_get_hpage_info(struct kvm *kvm, unsigned long gfn)
+{
+	unsigned long pfn;
+	struct page *page;
+	pfn = gfn_to_pfn(kvm, gfn);
+
+	if (is_error_noslot_pfn(pfn))
+		return -EPERM;
+
+	//printk("%lx %lx\n", gfn, pfn);
+	
+	page = pfn_to_page(pfn);
+
+	if (page) {
+		if (PageTransHuge(page))
+			return 1;
+	} else {
+		return -EFAULT;
+	}
+
+	return 0;
+}
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/arch/x86/kvm/Makefile linux-4.3-osa/arch/x86/kvm/Makefile
--- linux-4.3-org/arch/x86/kvm/Makefile	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/arch/x86/kvm/Makefile	2016-03-04 19:57:43.645958822 -0600
@@ -9,6 +9,9 @@
 
 kvm-y			+= $(KVM)/kvm_main.o $(KVM)/coalesced_mmio.o \
 				$(KVM)/eventfd.o $(KVM)/irqchip.o $(KVM)/vfio.o
+
+kvm-y  += kvm_osa.o
+
 kvm-$(CONFIG_KVM_ASYNC_PF)	+= $(KVM)/async_pf.o
 
 kvm-y			+= x86.o mmu.o emulate.o i8259.o irq.o lapic.o \
@@ -19,6 +22,6 @@
 kvm-intel-y		+= vmx.o pmu_intel.o
 kvm-amd-y		+= svm.o pmu_amd.o
 
-obj-$(CONFIG_KVM)	+= kvm.o
+obj-$(CONFIG_KVM)	+= kvm.o 
 obj-$(CONFIG_KVM_INTEL)	+= kvm-intel.o
 obj-$(CONFIG_KVM_AMD)	+= kvm-amd.o
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/arch/x86/kvm/x86.c linux-4.3-osa/arch/x86/kvm/x86.c
--- linux-4.3-org/arch/x86/kvm/x86.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/arch/x86/kvm/x86.c	2016-03-04 19:57:43.653958823 -0600
@@ -52,6 +52,7 @@
 #include <linux/timekeeper_internal.h>
 #include <linux/pvclock_gtod.h>
 #include <trace/events/kvm.h>
+#include <osa/kvm.h>
 
 #define CREATE_TRACE_POINTS
 #include "trace.h"
@@ -5728,10 +5729,12 @@
 		a3 &= 0xFFFFFFFF;
 	}
 
+	/* Allowing user-level hypercall 
 	if (kvm_x86_ops->get_cpl(vcpu) != 0) {
 		ret = -KVM_EPERM;
 		goto out;
 	}
+	*/
 
 	switch (nr) {
 	case KVM_HC_VAPIC_POLL_IRQ:
@@ -5741,11 +5744,14 @@
 		kvm_pv_kick_cpu_op(vcpu->kvm, a0, a1);
 		ret = 0;
 		break;
+	case KVM_HC_OSA_GET_HUGEINFO:
+		ret = osa_get_hpage_info(vcpu->kvm, a0);
+		break;
 	default:
 		ret = -KVM_ENOSYS;
 		break;
 	}
-out:
+//out:
 	if (!op_64_bit)
 		ret = (u32)ret;
 	kvm_register_write(vcpu, VCPU_REGS_RAX, ret);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/arch/x86/mm/fault.c linux-4.3-osa/arch/x86/mm/fault.c
--- linux-4.3-org/arch/x86/mm/fault.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/arch/x86/mm/fault.c	2016-10-07 00:07:54.745405225 -0500
@@ -1012,7 +1012,6 @@
 		return 0;
 	}
 
-	/* read, present: */
 	if (unlikely(error_code & PF_PROT))
 		return 1;
 
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/build_tags.sh linux-4.3-osa/build_tags.sh
--- linux-4.3-org/build_tags.sh	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/build_tags.sh	2016-03-04 19:57:43.693958824 -0600
@@ -0,0 +1,17 @@
+#! /bin/bash
+
+PWD=`pwd`\/;
+find $PWD \( -name '*.c' -o -name '*.cpp' -o -name '*.cc' -o -name '*.h' -o -name '*.s' -o -name '*.S' -o -name '*.java' \) -type f -print > cscope.tmp
+egrep -v arch cscope.tmp > cscope.files
+egrep arch cscope.tmp | egrep x86 >> cscope.files
+rm -rf cscope.tmp
+cscope -bR
+
+ctags="ctags --c-kinds=+defgpstux -R"
+for l in `ls ./arch | egrep -v x86`;
+do
+	ctags+=" --exclude=arch/$l"
+done
+
+echo $ctags
+$ctags
Binary files linux-4.3-org/drivers/staging/ft1000/ft1000-pcmcia/ft1000.img and linux-4.3-osa/drivers/staging/ft1000/ft1000-pcmcia/ft1000.img differ
Binary files linux-4.3-org/drivers/staging/ft1000/ft1000-usb/ft3000.img and linux-4.3-osa/drivers/staging/ft1000/ft1000-usb/ft3000.img differ
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/drivers/virtio/virtio_balloon.c linux-4.3-osa/drivers/virtio/virtio_balloon.c
--- linux-4.3-org/drivers/virtio/virtio_balloon.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/drivers/virtio/virtio_balloon.c	2016-03-04 19:57:45.749958878 -0600
@@ -31,6 +31,9 @@
 #include <linux/oom.h>
 #include <linux/wait.h>
 
+#include <linux/cgroup.h>
+#include <linux/vmpressure.h>
+
 /*
  * Balloon device works in 4K page units.  So each page is pointed to by
  * multiple balloon pages.  All memory counters in this driver are in balloon
@@ -45,9 +48,11 @@
 module_param(oom_pages, int, S_IRUSR | S_IWUSR);
 MODULE_PARM_DESC(oom_pages, "pages to free on OOM");
 
+#define VIRTIO_BALLOON_MSG_PRESSURE 1
+
 struct virtio_balloon {
 	struct virtio_device *vdev;
-	struct virtqueue *inflate_vq, *deflate_vq, *stats_vq;
+	struct virtqueue *inflate_vq, *deflate_vq, *stats_vq, *message_vq;
 
 	/* Where the ballooning thread waits for config to change. */
 	wait_queue_head_t config_change;
@@ -58,6 +63,8 @@
 	/* Waiting for host to ack the pages we released. */
 	wait_queue_head_t acked;
 
+	wait_queue_head_t message_acked;
+
 	/* Number of balloon pages we've told the Host we're not using. */
 	unsigned int num_pages;
 	/*
@@ -81,6 +88,9 @@
 
 	/* To register callback in oom notifier call chain */
 	struct notifier_block nb;
+
+	/* Message virtqueue */
+	atomic_t guest_pressure;
 };
 
 static struct virtio_device_id id_table[] = {
@@ -88,6 +98,41 @@
 	{ 0 },
 };
 
+static inline bool guest_under_pressure(const struct virtio_balloon *vb)
+{
+	return atomic_read(&vb->guest_pressure) == 1;
+}
+
+static void vmpressure_event_handler(void *data, int level)
+{
+	struct virtio_balloon *vb = data;
+
+	atomic_set(&vb->guest_pressure, 1);
+	wake_up(&vb->config_change);
+}
+
+static void tell_host_pressure(struct virtio_balloon *vb)
+{
+	const uint32_t msg = VIRTIO_BALLOON_MSG_PRESSURE;
+	struct scatterlist sg;
+	unsigned int len;
+	int err;
+
+	sg_init_one(&sg, &msg, sizeof(msg));
+
+	err = virtqueue_add_outbuf(vb->message_vq, &sg, 1, vb, GFP_KERNEL);
+	if (err < 0) {
+		printk(KERN_WARNING "virtio-balloon: failed to send host message (%d)\n", err);
+		goto out;
+	}
+	virtqueue_kick(vb->message_vq);
+
+	wait_event(vb->message_acked, virtqueue_get_buf(vb->message_vq, &len));
+
+out:
+	atomic_set(&vb->guest_pressure, 0);
+}
+
 static u32 page_to_balloon_pfn(struct page *page)
 {
 	unsigned long pfn = page_to_pfn(page);
@@ -110,6 +155,13 @@
 	wake_up(&vb->acked);
 }
 
+static void message_ack(struct virtqueue *vq)
+{
+	struct virtio_balloon *vb = vq->vdev->priv;
+
+	wake_up(&vb->message_acked);
+}
+
 static void tell_host(struct virtio_balloon *vb, struct virtqueue *vq)
 {
 	struct scatterlist sg;
@@ -145,8 +197,13 @@
 	mutex_lock(&vb->balloon_lock);
 	for (vb->num_pfns = 0; vb->num_pfns < num;
 	     vb->num_pfns += VIRTIO_BALLOON_PAGES_PER_PAGE) {
-		struct page *page = balloon_page_enqueue(vb_dev_info);
+	     	struct page *page;
+
+	     	if (guest_under_pressure(vb)) {
+			break;
+		}
 
+		page = balloon_page_enqueue(vb_dev_info);
 		if (!page) {
 			dev_info_ratelimited(&vb->vdev->dev,
 					     "Out of puff! Can't get %u pages\n",
@@ -359,6 +416,7 @@
 		add_wait_queue(&vb->config_change, &wait);
 		for (;;) {
 			if ((diff = towards_target(vb)) != 0 ||
+				guest_under_pressure(vb) ||
 			    vb->need_stats_update ||
 			    kthread_should_stop() ||
 			    freezing(current))
@@ -375,6 +433,9 @@
 			leak_balloon(vb, -diff);
 		update_balloon_size(vb);
 
+		if (guest_under_pressure(vb))
+			tell_host_pressure(vb);
+
 		/*
 		 * For large balloon changes, we could spend a lot of time
 		 * and always have work to do.  Be nice if preempt disabled.
@@ -386,25 +447,28 @@
 
 static int init_vqs(struct virtio_balloon *vb)
 {
-	struct virtqueue *vqs[3];
-	vq_callback_t *callbacks[] = { balloon_ack, balloon_ack, stats_request };
-	const char *names[] = { "inflate", "deflate", "stats" };
-	int err, nvqs;
+	struct virtqueue *vqs[4];
+	vq_callback_t *callbacks[] = { balloon_ack, balloon_ack,
+		stats_request, message_ack };
+	const char *names[] = { "inflate", "deflate", "stats", "pressure" };
+	int err, nvqs, idx;
 
 	/*
 	 * We expect two virtqueues: inflate and deflate, and
-	 * optionally stat.
+	 * optionally stat and message.
 	 */
-	nvqs = virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_STATS_VQ) ? 3 : 2;
+	nvqs = 2 + virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_STATS_VQ) +
+		virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_MESSAGE_VQ);
 	err = vb->vdev->config->find_vqs(vb->vdev, nvqs, vqs, callbacks, names);
 	if (err)
 		return err;
 
-	vb->inflate_vq = vqs[0];
-	vb->deflate_vq = vqs[1];
+	idx = 0;
+	vb->inflate_vq = vqs[idx++];
+	vb->deflate_vq = vqs[idx++];
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_STATS_VQ)) {
 		struct scatterlist sg;
-		vb->stats_vq = vqs[2];
+		vb->stats_vq = vqs[idx++];
 
 		/*
 		 * Prime this virtqueue with one buffer so the hypervisor can
@@ -416,6 +480,9 @@
 			BUG();
 		virtqueue_kick(vb->stats_vq);
 	}
+	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_MESSAGE_VQ))
+		vb->message_vq = vqs[idx];
+
 	return 0;
 }
 
@@ -502,9 +569,11 @@
 	vb->num_pages = 0;
 	mutex_init(&vb->balloon_lock);
 	init_waitqueue_head(&vb->config_change);
+	init_waitqueue_head(&vb->message_acked);
 	init_waitqueue_head(&vb->acked);
 	vb->vdev = vdev;
 	vb->need_stats_update = 0;
+	atomic_set(&vb->guest_pressure, 0);
 
 	balloon_devinfo_init(&vb->vb_dev_info);
 #ifdef CONFIG_BALLOON_COMPACTION
@@ -523,6 +592,12 @@
 
 	virtio_device_ready(vdev);
 
+	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_MESSAGE_VQ)) {
+		err = vmpressure_register_kernel_event(NULL, vmpressure_event_handler, vb);
+		if (err)
+			goto out_free_vb;
+	}
+
 	vb->thread = kthread_run(balloon, vb, "vballoon");
 	if (IS_ERR(vb->thread)) {
 		err = PTR_ERR(vb->thread);
@@ -599,6 +674,7 @@
 	VIRTIO_BALLOON_F_MUST_TELL_HOST,
 	VIRTIO_BALLOON_F_STATS_VQ,
 	VIRTIO_BALLOON_F_DEFLATE_ON_OOM,
+	VIRTIO_BALLOON_F_MESSAGE_VQ,
 };
 
 static struct virtio_driver virtio_balloon_driver = {
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/fs/proc/base.c linux-4.3-osa/fs/proc/base.c
--- linux-4.3-org/fs/proc/base.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/fs/proc/base.c	2016-11-01 00:24:48.726089091 -0500
@@ -2763,6 +2763,11 @@
 	REG("smaps",      S_IRUGO, proc_pid_smaps_operations),
 	REG("pagemap",    S_IRUSR, proc_pagemap_operations),
 #endif
+#ifdef CONFIG_OSA
+	REG("hugepage",	S_IRUSR, osa_hpage_proc_operations),
+	REG("hugepage_madvise",	S_IRUSR, osa_hpage_madvise_operations),
+	REG("hugepage_stats",	S_IRUSR|S_IWUSR, osa_hpage_stats_operations),
+#endif
 #ifdef CONFIG_SECURITY
 	DIR("attr",       S_IRUGO|S_IXUGO, proc_attr_dir_inode_operations, proc_attr_dir_operations),
 #endif
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/fs/proc/internal.h linux-4.3-osa/fs/proc/internal.h
--- linux-4.3-org/fs/proc/internal.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/fs/proc/internal.h	2016-11-01 00:24:48.726089091 -0500
@@ -297,6 +297,12 @@
 extern const struct file_operations proc_tid_smaps_operations;
 extern const struct file_operations proc_clear_refs_operations;
 extern const struct file_operations proc_pagemap_operations;
+#ifdef CONFIG_OSA
+extern struct file_operations osa_hpage_proc_operations;
+extern struct file_operations osa_hpage_madvise_operations;
+extern struct file_operations osa_hpage_stats_operations;
+extern struct file_operations osa_dump_lru_operations;
+#endif
 
 extern unsigned long task_vsize(struct mm_struct *);
 extern unsigned long task_statm(struct mm_struct *,
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/.get_maintainer.ignore linux-4.3-osa/.get_maintainer.ignore
--- linux-4.3-org/.get_maintainer.ignore	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/.get_maintainer.ignore	1969-12-31 18:00:00.000000000 -0600
@@ -1 +0,0 @@
-Christoph Hellwig <hch@lst.de>
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/compaction.h linux-4.3-osa/include/linux/compaction.h
--- linux-4.3-org/include/linux/compaction.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/compaction.h	2016-03-15 05:18:42.287158072 -0500
@@ -28,6 +28,13 @@
 struct alloc_context; /* in mm/internal.h */
 
 #ifdef CONFIG_COMPACTION
+struct contig_page_info {
+	unsigned long free_pages;
+	unsigned long free_blocks_total;
+	unsigned long free_pages_suitable;
+	unsigned long free_blocks_suitable;
+};
+
 extern int sysctl_compact_memory;
 extern int sysctl_compaction_handler(struct ctl_table *table, int write,
 			void __user *buffer, size_t *length, loff_t *ppos);
@@ -37,10 +44,16 @@
 extern int sysctl_compact_unevictable_allowed;
 
 extern int fragmentation_index(struct zone *zone, unsigned int order);
+extern void fill_contig_page_info(struct zone *zone,
+				unsigned int suitable_order,
+				struct contig_page_info *info);
 extern unsigned long try_to_compact_pages(gfp_t gfp_mask, unsigned int order,
 			int alloc_flags, const struct alloc_context *ac,
 			enum migrate_mode mode, int *contended);
 extern void compact_pgdat(pg_data_t *pgdat, int order);
+#ifdef CONFIG_OSA
+extern void compact_pgdat_periodic(pg_data_t *pgdat, int order);
+#endif
 extern void reset_isolation_suitable(pg_data_t *pgdat);
 extern unsigned long compaction_suitable(struct zone *zone, int order,
 					int alloc_flags, int classzone_idx);
@@ -60,6 +73,12 @@
 	return COMPACT_CONTINUE;
 }
 
+#ifdef CONFIG_OSA
+static inline void compact_pgdat_periodic(pg_data_t *pgdat, int order)
+{
+}
+#endif
+
 static inline void compact_pgdat(pg_data_t *pgdat, int order)
 {
 }
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/huge_mm.h linux-4.3-osa/include/linux/huge_mm.h
--- linux-4.3-org/include/linux/huge_mm.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/huge_mm.h	2016-11-01 00:24:48.726089091 -0500
@@ -14,7 +14,7 @@
 				  pmd_t orig_pmd, int dirty);
 extern int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
 			       unsigned long address, pmd_t *pmd,
-			       pmd_t orig_pmd);
+			       pmd_t orig_pmd, unsigned int flags);
 extern struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
 					  unsigned long addr,
 					  pmd_t *pmd,
@@ -69,6 +69,12 @@
 
 extern bool is_vma_temporary_stack(struct vm_area_struct *vma);
 
+#define thp_enabled                                    \
+	(transparent_hugepage_flags &                  \
+	 (1<<TRANSPARENT_HUGEPAGE_FLAG) ||             \
+	 (transparent_hugepage_flags &                 \
+	  (1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG)))
+
 #define transparent_hugepage_enabled(__vma)				\
 	((transparent_hugepage_flags &					\
 	  (1<<TRANSPARENT_HUGEPAGE_FLAG) ||				\
@@ -220,4 +226,15 @@
 
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
+extern struct kobject *sysfs_hugepage_kobj; 
+
+#ifdef CONFIG_OSA
+extern struct list_head osa_hpage_scan_list;
+extern unsigned int deferred_mode;
+extern unsigned int util_threshold;
+extern unsigned int hugepage_fairness;
+extern unsigned long distance_divisor;
+#endif
+
+
 #endif /* _LINUX_HUGE_MM_H */
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/khugepaged.h linux-4.3-osa/include/linux/khugepaged.h
--- linux-4.3-org/include/linux/khugepaged.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/khugepaged.h	2016-03-05 00:06:04.762351086 -0600
@@ -9,6 +9,8 @@
 extern int khugepaged_enter_vma_merge(struct vm_area_struct *vma,
 				      unsigned long vm_flags);
 
+static DECLARE_WAIT_QUEUE_HEAD(khugepaged_wait);
+
 #define khugepaged_enabled()					       \
 	(transparent_hugepage_flags &				       \
 	 ((1<<TRANSPARENT_HUGEPAGE_FLAG) |		       \
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/migrate.h linux-4.3-osa/include/linux/migrate.h
--- linux-4.3-org/include/linux/migrate.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/migrate.h	2016-09-20 00:21:29.166619531 -0500
@@ -23,7 +23,9 @@
 	MR_SYSCALL,		/* also applies to cpusets */
 	MR_MEMPOLICY_MBIND,
 	MR_NUMA_MISPLACED,
-	MR_CMA
+	MR_CMA,
+	MR_HUGEPAGE_RECLAIM,
+	MR_GFN_AGGREGATE,
 };
 
 #ifdef CONFIG_MIGRATION
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/mm.h linux-4.3-osa/include/linux/mm.h
--- linux-4.3-org/include/linux/mm.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/mm.h	2016-09-20 00:21:29.166619531 -0500
@@ -84,11 +84,28 @@
 
 #define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))
 
+/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */
+#define PAGE_ALIGNED(addr)	IS_ALIGNED((unsigned long)addr, PAGE_SIZE)
+#define HPAGE_ALIGNED(addr)	IS_ALIGNED((unsigned long)addr, HPAGE_SIZE)
+
 /* to align the pointer to the (next) page boundary */
 #define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)
+static inline unsigned long PAGE_ALIGN_FLOOR(unsigned long addr) 
+{
+	if (PAGE_ALIGNED(addr))
+		return addr;
+	else
+		return PAGE_ALIGN(addr) - PAGE_SIZE;
+}
 
-/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */
-#define PAGE_ALIGNED(addr)	IS_ALIGNED((unsigned long)addr, PAGE_SIZE)
+#define HPAGE_ALIGN(addr) ALIGN(addr, HPAGE_SIZE)
+static inline unsigned long HPAGE_ALIGN_FLOOR(unsigned long addr) 
+{
+	if (HPAGE_ALIGNED(addr))
+		return addr;
+	else
+		return HPAGE_ALIGN(addr) - HPAGE_SIZE;
+}
 
 /*
  * Linux kernel virtual memory manager primitives.
@@ -215,6 +232,7 @@
 #define FAULT_FLAG_KILLABLE	0x10	/* The fault task is in SIGKILL killable region */
 #define FAULT_FLAG_TRIED	0x20	/* Second try */
 #define FAULT_FLAG_USER		0x40	/* The fault originated in userspace */
+#define FAULT_FLAG_INST     0x100   /* The fault is caused by instruction access */
 
 /*
  * vm_fault is filled by the the pagefault handler and passed to the vma's
@@ -1526,6 +1544,10 @@
 }
 #endif /* CONFIG_MMU && !__ARCH_HAS_4LEVEL_HACK */
 
+#ifdef CONFIG_OSA
+void __init osa_kmem_cache_init(void);
+#endif
+
 #if USE_SPLIT_PTE_PTLOCKS
 #if ALLOC_SPLIT_PTLOCKS
 void __init ptlock_cache_init(void);
@@ -1602,6 +1624,9 @@
 {
 	ptlock_cache_init();
 	pgtable_cache_init();
+#ifdef CONFIG_OSA
+    osa_kmem_cache_init();
+#endif
 }
 
 static inline bool pgtable_page_ctor(struct page *page)
@@ -1972,7 +1997,9 @@
 
 struct vm_unmapped_area_info {
 #define VM_UNMAPPED_AREA_TOPDOWN 1
-	unsigned long flags;
+	//unsigned long flags;
+	unsigned int flags;
+	unsigned int map_flags;
 	unsigned long length;
 	unsigned long low_limit;
 	unsigned long high_limit;
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/mm_inline.h linux-4.3-osa/include/linux/mm_inline.h
--- linux-4.3-org/include/linux/mm_inline.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/mm_inline.h	2016-11-02 00:29:15.881345842 -0500
@@ -31,6 +31,53 @@
 	__mod_zone_page_state(lruvec_zone(lruvec), NR_LRU_BASE + lru, nr_pages);
 }
 
+/* lurvec->lists[lru].prev : tail of lru list
+ * pages are checked from the tail.
+ * lurvec->lists[lru].next : head of lru list
+ */
+static __always_inline void
+//static void __attribute__((optimize("O0"))) 
+add_page_to_lru_list_tail(struct page *page,
+				struct lruvec *lruvec, enum lru_list lru)
+{
+	int nr_pages = hpage_nr_pages(page);
+	struct list_head *l, *head;
+	unsigned long i, inactive_lru_size, active_lru_size, lru_size;
+	struct page *lru_page;
+	//int from_tail = 1;
+
+	mem_cgroup_update_lru_size(lruvec, lru, nr_pages);
+
+	inactive_lru_size = mem_cgroup_get_lru_size(lruvec, 
+			LRU_INACTIVE_ANON);
+	active_lru_size = mem_cgroup_get_lru_size(lruvec, 
+			LRU_INACTIVE_ANON + LRU_ACTIVE);
+
+	head = &lruvec->lists[lru];
+
+	//BUG_ON(is_active_lru(lru));
+
+    if (is_active_lru(lru))
+        lru_size = active_lru_size;
+    else
+        lru_size = inactive_lru_size;
+
+	lru_size = lru_size / 10;
+    lru_size = distance_divisor * lru_size;
+
+    for (i = 0, l = head->next; i < lru_size; l = l->next) {
+        if (l == head)
+            break;
+
+        lru_page = list_entry(l, struct page, lru);
+        i += hpage_nr_pages(page);
+    }
+
+	list_add(&page->lru, l);
+
+	__mod_zone_page_state(lruvec_zone(lruvec), NR_LRU_BASE + lru, nr_pages);
+}
+
 static __always_inline void del_page_from_lru_list(struct page *page,
 				struct lruvec *lruvec, enum lru_list lru)
 {
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/mm_types.h linux-4.3-osa/include/linux/mm_types.h
--- linux-4.3-org/include/linux/mm_types.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/mm_types.h	2016-11-01 00:24:48.726089091 -0500
@@ -14,6 +14,9 @@
 #include <linux/page-flags-layout.h>
 #include <asm/page.h>
 #include <asm/mmu.h>
+#ifdef CONFIG_OSA
+#include <linux/radix-tree.h>
+#endif
 
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
@@ -30,6 +33,23 @@
 
 typedef void compound_page_dtor(struct page *);
 
+#ifdef CONFIG_OSA
+//#define FREQ_BITMAP_SIZE 16
+//#define PRI_HISTORY_SIZE 5
+#define FREQ_BITMAP_SIZE 8
+#define PRI_HISTORY_SIZE 3
+
+/* structure for tracking temporal utilization */
+typedef struct utilmap_node {
+	struct list_head link;
+    DECLARE_BITMAP(freq_bitmap, FREQ_BITMAP_SIZE);
+    int32_t frequency[PRI_HISTORY_SIZE];
+	// for debugging
+	unsigned long addr;
+	struct page *page;
+} util_node_t;
+#endif
+
 /*
  * Each physical page in the system has a struct page associated with
  * it to keep track of whatever it is we are using the page for at the
@@ -163,8 +183,10 @@
 #endif
 #endif
 		struct kmem_cache *slab_cache;	/* SL[AU]B: Pointer to slab */
-		struct page *first_page;	/* Compound tail pages */
 	};
+	/* Moved out from previous union block 
+	 * to support huge slab page */
+	struct page *first_page;	/* Compound tail pages */
 
 #ifdef CONFIG_MEMCG
 	struct mem_cgroup *mem_cgroup;
@@ -192,6 +214,10 @@
 	 */
 	void *shadow;
 #endif
+#ifdef CONFIG_OSA
+	unsigned long osa_flag;
+	util_node_t util_info;
+#endif
 
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
@@ -366,6 +392,19 @@
 	atomic_long_t count[NR_MM_COUNTERS];
 };
 
+
+#ifdef CONFIG_OSA
+struct osa_hpage_stats {
+	unsigned int hpage_requirement;
+	unsigned int total_hpage_count;
+	unsigned long total_bpage_count;
+	unsigned int idle_hpage_count;
+	unsigned long idle_bpage_count;
+	unsigned int idle_tau; //idle page penalty parameter
+	unsigned int weight;
+};
+#endif
+
 struct kioctx_table;
 struct mm_struct {
 	struct vm_area_struct *mmap;		/* list of VMAs */
@@ -486,6 +525,12 @@
 	/* address of the bounds directory */
 	void __user *bd_addr;
 #endif
+#ifdef CONFIG_OSA
+	struct osa_hpage_stats hpage_stats;
+	struct list_head osa_hpage_reclaim_link;
+	struct list_head osa_hpage_scan_link;
+    struct radix_tree_root root_popl_map;
+#endif
 };
 
 static inline void mm_init_cpumask(struct mm_struct *mm)
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/page_ext.h linux-4.3-osa/include/linux/page_ext.h
--- linux-4.3-org/include/linux/page_ext.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/page_ext.h	2016-09-20 00:21:29.166619531 -0500
@@ -4,6 +4,10 @@
 #include <linux/types.h>
 #include <linux/stacktrace.h>
 
+#ifdef CONFIG_OSA
+#include <osa/osa.h>
+#endif 
+
 struct pglist_data;
 struct page_ext_operations {
 	bool (*need)(void);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/page_idle.h linux-4.3-osa/include/linux/page_idle.h
--- linux-4.3-org/include/linux/page_idle.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/page_idle.h	2016-03-04 19:57:46.101958887 -0600
@@ -107,4 +107,9 @@
 
 #endif /* CONFIG_IDLE_PAGE_TRACKING */
 
+extern struct page *page_idle_get_page(unsigned long pfn);
+extern int page_idle_clear_pte_refs_one(struct page *page,
+					struct vm_area_struct *vma,
+					unsigned long addr, void *arg);
+extern void page_idle_clear_pte_refs(struct page *page);
 #endif /* _LINUX_MM_PAGE_IDLE_H */
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/slab.h linux-4.3-osa/include/linux/slab.h
--- linux-4.3-org/include/linux/slab.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/slab.h	2016-11-01 00:24:48.726089091 -0500
@@ -90,6 +90,7 @@
 /* The following flags affect the page allocator grouping pages by mobility */
 #define SLAB_RECLAIM_ACCOUNT	0x00020000UL		/* Objects are reclaimable */
 #define SLAB_TEMPORARY		SLAB_RECLAIM_ACCOUNT	/* Objects are short-lived */
+
 /*
  * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
  *
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/linux/vmpressure.h linux-4.3-osa/include/linux/vmpressure.h
--- linux-4.3-org/include/linux/vmpressure.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/linux/vmpressure.h	2016-03-04 19:57:46.141958888 -0600
@@ -20,6 +20,9 @@
 	/* Have to grab the lock on events traversal or modifications. */
 	struct mutex events_lock;
 
+	/* False if only kernel users want to be notified, true otherwise. */
+	bool notify_userspace;
+
 	struct work_struct work;
 };
 
@@ -34,9 +37,13 @@
 extern void vmpressure_cleanup(struct vmpressure *vmpr);
 extern struct vmpressure *memcg_to_vmpressure(struct mem_cgroup *memcg);
 extern struct cgroup_subsys_state *vmpressure_to_css(struct vmpressure *vmpr);
+extern struct vmpressure *css_to_vmpressure(struct cgroup_subsys_state *css);
 extern int vmpressure_register_event(struct mem_cgroup *memcg,
 				     struct eventfd_ctx *eventfd,
 				     const char *args);
+extern int vmpressure_register_kernel_event(struct cgroup_subsys_state *css,
+		             void (*fn)(void *data, int level),
+		             void *data);
 extern void vmpressure_unregister_event(struct mem_cgroup *memcg,
 					struct eventfd_ctx *eventfd);
 #else
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/osa/kvm.h linux-4.3-osa/include/osa/kvm.h
--- linux-4.3-org/include/osa/kvm.h	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/include/osa/kvm.h	2016-03-04 19:57:46.165958889 -0600
@@ -0,0 +1,7 @@
+#ifndef _OSA_KVM_H_
+#define _OSA_KVM_H_
+#include <linux/kvm_host.h>
+
+int osa_get_hpage_info(struct kvm *kvm, unsigned long gfn);
+
+#endif
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/osa/osa.h linux-4.3-osa/include/osa/osa.h
--- linux-4.3-org/include/osa/osa.h	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/include/osa/osa.h	2016-11-01 00:24:48.726089091 -0500
@@ -0,0 +1,82 @@
+#ifndef _OSA_H_
+#define _OSA_H_
+#include <linux/mm.h>
+
+#define AGGR_BITMAP_SIZE 4
+
+//Page flags for osa_flag in struct page
+#define OSA_PF_AGGR 0x1 //Page is aggregated by osa_hpage_scand
+
+extern spinlock_t osa_hpage_list_lock;
+
+/* SLAB cache for worklist node */
+extern struct kmem_cache *osa_poplmap_node_cachep;
+extern struct kmem_cache *osa_work_node_cachep;
+extern struct kmem_cache *osa_aggregate_node_cachep;
+
+typedef struct osa_aggregation_node {
+	struct list_head link;
+	struct page *page;
+	DECLARE_BITMAP(access_bitmap, AGGR_BITMAP_SIZE);
+} aggr_node_t;
+
+typedef struct population_node {
+    //bitmap for tracking spatial utilization of 2MB region.
+    DECLARE_BITMAP(popl_bitmap, 512);
+    u8 committed;
+} popl_node_t;
+
+void *osa_popl_node_lookup(struct mm_struct *mm,
+		unsigned long address);
+int osa_popl_node_insert(struct mm_struct *mm, 
+		unsigned long address,
+		popl_node_t *node);
+void *osa_popl_node_delete(struct mm_struct *mm, 
+		unsigned long address);
+
+extern spinlock_t osa_poplmap_lock;
+
+popl_node_t *osa_poplmap_node_alloc(void);
+void osa_poplmap_node_free(popl_node_t *node);
+
+int osa_util_node_insert(struct mm_struct *mm, 
+		unsigned long address,
+		util_node_t *node);
+void *osa_util_node_delete(struct mm_struct *mm, 
+		unsigned long address);
+void *osa_util_node_lookup(struct mm_struct *mm,
+		unsigned long address);
+void osa_clear_poplmap_range(struct mm_struct *mm, 
+		unsigned long start, unsigned long end); 
+void frequency_update(util_node_t *node);
+
+unsigned long osa_get_hpage_count(struct mm_struct *mm);
+void osa_hpage_enter_list(struct mm_struct *mm);
+void osa_hpage_exit_list(struct mm_struct *mm);
+
+extern long madvise_vma(struct vm_area_struct *vma, 
+		struct vm_area_struct **prev,
+		unsigned long start, unsigned long end, int behavior);
+
+int is_member_of_scan_list(struct mm_struct *mm);
+unsigned int osa_compute_fairness_metric(struct mm_struct *mm);
+
+#define MAX_WORKLIST_SIZE 513
+extern struct list_head hugepage_worklist;
+
+typedef struct work_node {
+    struct list_head list;
+    struct mm_struct *mm;
+    unsigned long address;
+} work_node_t;
+
+work_node_t *osa_work_node_alloc(void);
+void osa_work_node_free(work_node_t *node);
+
+extern spinlock_t worklist_lock;
+
+extern wait_queue_head_t osa_hpage_scand_wait;
+extern wait_queue_head_t osa_aggregationd_wait;
+extern int osa_inst_get_page_owner(struct page *page, int references);
+
+#endif 
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/trace/events/vmscan.h linux-4.3-osa/include/trace/events/vmscan.h
--- linux-4.3-org/include/trace/events/vmscan.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/trace/events/vmscan.h	2016-03-04 19:57:46.185958889 -0600
@@ -384,6 +384,233 @@
 		show_reclaim_flags(__entry->reclaim_flags))
 );
 
+TRACE_EVENT(mm_vmscan_lru_reclaim_stat, 
+
+	TP_PROTO(int flag, int zone_idx, unsigned long nr_reclaimed, unsigned long anon_lru_size, 
+		unsigned long file_lru_size, struct zone_reclaim_stat reclaim_stat),
+
+	TP_ARGS(flag, zone_idx, nr_reclaimed, anon_lru_size, file_lru_size, reclaim_stat),
+
+	TP_STRUCT__entry(
+		__field(int, flag)
+		__field(int, zone_idx)
+		__field(unsigned long, nr_reclaimed)
+		__field(unsigned long, anon_lru_size)
+		__field(unsigned long, file_lru_size)
+		__field(unsigned long, anon_nr_scanned)
+		__field(unsigned long, anon_nr_rotated)
+		__field(unsigned long, file_nr_scanned)
+		__field(unsigned long, file_nr_rotated)
+	),
+
+	TP_fast_assign(
+		__entry->flag = flag;
+		__entry->zone_idx = zone_idx;
+		__entry->nr_reclaimed = nr_reclaimed;
+		__entry->anon_lru_size = anon_lru_size;
+		__entry->file_lru_size = file_lru_size;
+		__entry->anon_nr_scanned = reclaim_stat.recent_scanned[0];
+		__entry->anon_nr_rotated = reclaim_stat.recent_rotated[0];
+		__entry->file_nr_scanned = reclaim_stat.recent_scanned[1];
+		__entry->file_nr_rotated = reclaim_stat.recent_rotated[1];
+	),
+
+	TP_printk("flag %d zone %d nr_reclaimed %lu anon lru_size %lu nr_scanned %lu nr_rotated %lu "
+		"file lru_size %lu nr_scanned %lu nr_rotated %lu",
+		__entry->flag,
+		__entry->zone_idx,
+		__entry->nr_reclaimed,
+		__entry->anon_lru_size, 
+		__entry->anon_nr_scanned, __entry->anon_nr_rotated,
+		__entry->file_lru_size, 
+		__entry->file_nr_scanned, __entry->file_nr_rotated
+	)
+);
+
+TRACE_EVENT(mm_vmscan_reclaim_stat, 
+
+	TP_PROTO(int flag, unsigned long nr_scanned, unsigned long nr_reclaimed),
+
+	TP_ARGS(flag, nr_scanned, nr_reclaimed),
+
+	TP_STRUCT__entry(
+		__field(int, flag)
+		__field(unsigned long, nr_scanned)
+		__field(unsigned long, nr_reclaimed)
+	),
+
+	TP_fast_assign(
+		__entry->flag = flag;
+		__entry->nr_scanned = nr_scanned;
+		__entry->nr_reclaimed = nr_reclaimed;
+	),
+
+	TP_printk("flag %d nr_scanned %lu nr_reclaimed %lu",
+		__entry->flag,
+		__entry->nr_scanned,
+		__entry->nr_reclaimed
+	)
+);
+
+/* flag 0 : reclaimed
+ * flag 1 : under writeback (waiting for being reclaimed)
+ * flag 5 : keep in the list 
+ */
+TRACE_EVENT(mm_vmscan_shrink_page_list,
+
+	TP_PROTO(int flag, unsigned int owner, unsigned long pfn, 
+		unsigned int is_huge, unsigned int ref_action),
+
+	TP_ARGS(flag, owner, pfn, is_huge, ref_action),
+
+	TP_STRUCT__entry(
+		__field(int, flag)
+		__field(unsigned int, owner)
+		__field(unsigned long, pfn)
+		__field(unsigned int, is_huge)
+		__field(unsigned int, ref_action)
+	),
+
+	TP_fast_assign(
+		__entry->flag = flag;
+		__entry->owner = owner;
+		__entry->pfn = pfn;
+		__entry->is_huge = is_huge;
+		__entry->ref_action = ref_action;
+	),
+
+	TP_printk("flag %d owner %u pfn %lx huge %u ref_action %u",
+			__entry->flag, __entry->owner,
+			__entry->pfn, __entry->is_huge, __entry->ref_action)
+);
+
+TRACE_EVENT(mm_vmscan_printk1,
+
+	TP_PROTO(const char *str, unsigned long val),
+
+	TP_ARGS(str, val),
+
+	TP_STRUCT__entry(
+		__array(char, str, 40)
+		__field(unsigned long, val)
+	),
+
+	TP_fast_assign(
+		strncpy(__entry->str, str, 40);
+		__entry->str[40] = 0;
+		__entry->val = val;
+	),
+
+	TP_printk("%s %lu", __entry->str, __entry->val)
+);
+
+TRACE_EVENT(mm_vmscan_printk2,
+
+	TP_PROTO(const char *str1, unsigned long val1, 
+		const char *str2, unsigned long val2),
+
+	TP_ARGS(str1, val1, str2, val2),
+
+	TP_STRUCT__entry(
+		__array(char, str1, 40)
+		__field(unsigned long, val1)
+		__array(char, str2, 40)
+		__field(unsigned long, val2)
+	),
+
+	TP_fast_assign(
+		strncpy(__entry->str1, str1, 40);
+		__entry->str1[40] = 0;
+		__entry->val1 = val1;
+		strncpy(__entry->str2, str2, 40);
+		__entry->str2[40] = 0;
+		__entry->val2 = val2;
+	),
+
+	TP_printk("%s %lu %s %lu", 
+			__entry->str1, __entry->val1,
+			__entry->str2, __entry->val2)
+);
+
+TRACE_EVENT(mm_vmscan_printk3,
+
+	TP_PROTO(const char *str1, unsigned long val1, 
+		const char *str2, unsigned long val2,
+		const char *str3, unsigned long val3),
+
+	TP_ARGS(str1, val1, str2, val2, str3, val3),
+
+	TP_STRUCT__entry(
+		__array(char, str1, 40)
+		__field(unsigned long, val1)
+		__array(char, str2, 40)
+		__field(unsigned long, val2)
+		__array(char, str3, 40)
+		__field(unsigned long, val3)
+	),
+
+	TP_fast_assign(
+		strncpy(__entry->str1, str1, 40);
+		__entry->str1[40] = 0;
+		__entry->val1 = val1;
+		strncpy(__entry->str2, str2, 40);
+		__entry->str2[40] = 0;
+		__entry->val2 = val2;
+		strncpy(__entry->str3, str3, 40);
+		__entry->str3[40] = 0;
+		__entry->val3 = val3;
+	),
+
+	TP_printk("%s %lu %s %lu %s %lu", 
+			__entry->str1, __entry->val1,
+			__entry->str2, __entry->val2,
+			__entry->str3, __entry->val3
+	)
+);
+
+TRACE_EVENT(mm_vmscan_printk4,
+
+	TP_PROTO(const char *str1, unsigned long val1, 
+		const char *str2, unsigned long val2,
+		const char *str3, unsigned long val3,
+		const char *str4, unsigned long val4),
+
+	TP_ARGS(str1, val1, str2, val2, str3, val3, str4, val4),
+
+	TP_STRUCT__entry(
+		__array(char, str1, 40)
+		__field(unsigned long, val1)
+		__array(char, str2, 40)
+		__field(unsigned long, val2)
+		__array(char, str3, 40)
+		__field(unsigned long, val3)
+		__array(char, str4, 40)
+		__field(unsigned long, val4)
+	),
+
+	TP_fast_assign(
+		strncpy(__entry->str1, str1, 40);
+		__entry->str1[40] = 0;
+		__entry->val1 = val1;
+		strncpy(__entry->str2, str2, 40);
+		__entry->str2[40] = 0;
+		__entry->val2 = val2;
+		strncpy(__entry->str3, str3, 40);
+		__entry->str3[40] = 0;
+		__entry->val3 = val3;
+		strncpy(__entry->str4, str4, 40);
+		__entry->str4[40] = 0;
+		__entry->val4 = val4;
+	),
+
+	TP_printk("%s %lu %s %lu %s %lu %s %lu", 
+			__entry->str1, __entry->val1,
+			__entry->str2, __entry->val2,
+			__entry->str3, __entry->val3,
+			__entry->str4, __entry->val4
+	)
+);
+
 #endif /* _TRACE_VMSCAN_H */
 
 /* This part must be outside protection */
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/uapi/asm-generic/mman-common.h linux-4.3-osa/include/uapi/asm-generic/mman-common.h
--- linux-4.3-org/include/uapi/asm-generic/mman-common.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/uapi/asm-generic/mman-common.h	2016-03-04 19:57:46.185958889 -0600
@@ -19,6 +19,7 @@
 #define MAP_TYPE	0x0f		/* Mask for type of mapping */
 #define MAP_FIXED	0x10		/* Interpret addr exactly */
 #define MAP_ANONYMOUS	0x20		/* don't use a file */
+#define MAP_HPAGE		0x100000	/* return hugepage aligned address */
 #ifdef CONFIG_MMAP_ALLOW_UNINITIALIZED
 # define MAP_UNINITIALIZED 0x4000000	/* For anonymous mmap, memory could be uninitialized */
 #else
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/uapi/linux/kvm_para.h linux-4.3-osa/include/uapi/linux/kvm_para.h
--- linux-4.3-org/include/uapi/linux/kvm_para.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/uapi/linux/kvm_para.h	2016-03-04 19:57:46.201958890 -0600
@@ -24,6 +24,8 @@
 #define KVM_HC_MIPS_EXIT_VM		7
 #define KVM_HC_MIPS_CONSOLE_OUTPUT	8
 
+#define KVM_HC_OSA_GET_HUGEINFO	10
+
 /*
  * hypercalls use architecture specific
  */
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/include/uapi/linux/virtio_balloon.h linux-4.3-osa/include/uapi/linux/virtio_balloon.h
--- linux-4.3-org/include/uapi/linux/virtio_balloon.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/include/uapi/linux/virtio_balloon.h	2016-03-04 19:57:46.217958890 -0600
@@ -34,6 +34,7 @@
 #define VIRTIO_BALLOON_F_MUST_TELL_HOST	0 /* Tell before reclaiming pages */
 #define VIRTIO_BALLOON_F_STATS_VQ	1 /* Memory Stats virtqueue */
 #define VIRTIO_BALLOON_F_DEFLATE_ON_OOM	2 /* Deflate balloon on OOM */
+#define VIRTIO_BALLOON_F_MESSAGE_VQ    3 /* Message virtqueue */
 
 /* Size of a PFN in the balloon interface. */
 #define VIRTIO_BALLOON_PFN_SHIFT 12
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/Kconfig linux-4.3-osa/Kconfig
--- linux-4.3-org/Kconfig	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/Kconfig	2016-03-04 19:57:42.825958801 -0600
@@ -9,3 +9,5 @@
 	option env="SRCARCH"
 
 source "arch/$SRCARCH/Kconfig"
+
+source "./Kconfig.osa"
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/Kconfig.osa linux-4.3-osa/Kconfig.osa
--- linux-4.3-org/Kconfig.osa	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/Kconfig.osa	2016-10-07 00:12:41.589397811 -0500
@@ -0,0 +1,7 @@
+menu "OSA"
+
+config OSA
+	tristate "Enable Hugepage dump"
+	default y
+
+endmenu
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/kernel/exit.c linux-4.3-osa/kernel/exit.c
--- linux-4.3-org/kernel/exit.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/kernel/exit.c	2016-11-01 00:24:48.726089091 -0500
@@ -59,6 +59,11 @@
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 
+#ifdef CONFIG_OSA
+#include <osa/osa.h>
+#include <linux/radix-tree.h>
+#endif
+
 static void exit_mm(struct task_struct *tsk);
 
 static void __unhash_process(struct task_struct *p, bool group_dead)
@@ -393,6 +398,36 @@
 	if (!mm)
 		return;
 	sync_mm_rss(mm);
+
+#ifdef CONFIG_OSA
+	// freeing remaining util_nodes
+	{
+		popl_node_t *_popl_node;
+		void **slot;
+		struct radix_tree_iter iter;
+		unsigned long start = mm->mmap->vm_start;
+		unsigned long count = 0;
+
+		spin_lock(&osa_poplmap_lock);
+		rcu_read_lock();
+		radix_tree_for_each_slot(slot, &mm->root_popl_map, &iter, start) {
+			_popl_node = (popl_node_t *)radix_tree_deref_slot(slot);
+			if (unlikely(!_popl_node))
+				continue;
+
+			count++;
+
+			osa_popl_node_delete(mm, iter.index);
+			osa_poplmap_node_free(_popl_node);
+		}
+		rcu_read_unlock();
+		spin_unlock(&osa_poplmap_lock);
+
+		if (count > 0xffff) 
+			trace_printk("free radix tree %lu\n", count);
+	}
+#endif
+
 	/*
 	 * Serialize with any possible pending coredump.
 	 * We must hold mmap_sem around checking core_state
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/kernel/fork.c linux-4.3-osa/kernel/fork.c
--- linux-4.3-org/kernel/fork.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/kernel/fork.c	2016-09-20 00:21:29.166619531 -0500
@@ -85,6 +85,10 @@
 
 #include <trace/events/sched.h>
 
+#ifdef CONFIG_OSA
+#include <osa/osa.h>
+#endif
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/task.h>
 
@@ -608,6 +612,14 @@
 	mm->pmd_huge_pte = NULL;
 #endif
 
+#ifdef CONFIG_OSA
+	INIT_LIST_HEAD(&mm->osa_hpage_reclaim_link);
+	INIT_LIST_HEAD(&mm->osa_hpage_scan_link);
+    INIT_RADIX_TREE(&mm->root_popl_map, GFP_ATOMIC);
+
+	mm->hpage_stats.weight = 0;
+#endif 
+
 	if (current->mm) {
 		mm->flags = current->mm->flags & MMF_INIT_MASK;
 		mm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;
@@ -678,6 +690,7 @@
 void __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
+
 	mm_free_pgd(mm);
 	destroy_context(mm);
 	mmu_notifier_mm_destroy(mm);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/Makefile linux-4.3-osa/Makefile
--- linux-4.3-org/Makefile	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/Makefile	2016-03-04 19:57:42.829958801 -0600
@@ -878,7 +878,7 @@
 
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y		+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/
+core-y		+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/ osa/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/compaction.c linux-4.3-osa/mm/compaction.c
--- linux-4.3-org/mm/compaction.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/compaction.c	2016-11-01 00:24:48.726089091 -0500
@@ -785,8 +785,12 @@
 		 * admittedly racy check.
 		 */
 		if (!page_mapping(page) &&
-		    page_count(page) > page_mapcount(page))
+		    page_count(page) > page_mapcount(page)) {
+			//trace_printk("pinned page %lx, count = %d, map_count %d\n",
+			//		page_to_pfn(page), page_count(page), page_mapcount(page));
 			continue;
+		}
+
 
 		/* If we already hold the lock, we can skip some rechecking */
 		if (!locked) {
@@ -1230,6 +1234,13 @@
 	if (cc->order == -1)
 		return COMPACT_CONTINUE;
 
+#ifdef CONFIG_OSA
+	if (cc->osa_cc_counts)
+		return COMPACT_CONTINUE;
+	else
+		return COMPACT_PARTIAL;
+#endif
+
 	/* Compaction run is not finished if the watermark is not met */
 	watermark = low_wmark_pages(zone);
 
@@ -1346,7 +1357,9 @@
 	return ret;
 }
 
-static int compact_zone(struct zone *zone, struct compact_control *cc)
+//static int compact_zone(struct zone *zone, struct compact_control *cc)
+static int __attribute__((optimize("O0"))) 
+compact_zone(struct zone *zone, struct compact_control *cc)
 {
 	int ret;
 	unsigned long start_pfn = zone->zone_start_pfn;
@@ -1356,6 +1369,12 @@
 
 	ret = compaction_suitable(zone, cc->order, cc->alloc_flags,
 							cc->classzone_idx);
+
+#ifdef CONFIG_OSA
+	if (cc->osa_cc_counts)
+		ret = COMPACT_CONTINUE;
+#endif
+
 	switch (ret) {
 	case COMPACT_PARTIAL:
 	case COMPACT_SKIPPED:
@@ -1415,6 +1434,10 @@
 			 */
 			goto check_drain;
 		case ISOLATE_SUCCESS:
+#ifdef CONFIG_OSA
+			if(cc->osa_cc_counts)
+				cc->osa_cc_counts--;
+#endif
 			;
 		}
 
@@ -1461,7 +1484,6 @@
 				cc->last_migrated_pfn = 0;
 			}
 		}
-
 	}
 
 out:
@@ -1661,7 +1683,17 @@
 		if (cc->order == -1)
 			__reset_isolation_suitable(zone);
 
+#ifdef CONFIG_OSA
+		if (cc->osa_cc_counts && compact_scanners_met(cc)) 
+			__reset_isolation_suitable(zone);
+#endif
+
+#ifdef CONFIG_OSA
+		if (cc->order == -1 || cc->osa_cc_counts || 
+				!compaction_deferred(zone, cc->order))
+#else
 		if (cc->order == -1 || !compaction_deferred(zone, cc->order))
+#endif
 			compact_zone(zone, cc);
 
 		if (cc->order > 0) {
@@ -1688,6 +1720,22 @@
 	__compact_pgdat(pgdat, &cc);
 }
 
+#ifdef CONFIG_OSA
+void compact_pgdat_periodic(pg_data_t *pgdat, int order)
+{
+	struct compact_control cc = {
+		.order = order,
+		.mode = MIGRATE_SYNC,
+		.osa_cc_counts = 20,
+	};
+
+	if (!order)
+		return;
+
+	__compact_pgdat(pgdat, &cc);
+}
+#endif
+
 static void compact_node(int nid)
 {
 	struct compact_control cc = {
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/huge_memory.c linux-4.3-osa/mm/huge_memory.c
--- linux-4.3-org/mm/huge_memory.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/huge_memory.c	2016-11-02 00:28:10.173343373 -0500
@@ -26,6 +26,10 @@
 #include <linux/hashtable.h>
 #include <linux/userfaultfd_k.h>
 #include <linux/page_idle.h>
+#include <linux/sched.h>
+#ifdef CONFIG_OSA
+#include <osa/osa.h>
+#endif
 
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
@@ -60,7 +64,7 @@
 static struct task_struct *khugepaged_thread __read_mostly;
 static DEFINE_MUTEX(khugepaged_mutex);
 static DEFINE_SPINLOCK(khugepaged_mm_lock);
-static DECLARE_WAIT_QUEUE_HEAD(khugepaged_wait);
+//static DECLARE_WAIT_QUEUE_HEAD(khugepaged_wait);
 /*
  * default collapse hugepages if there is at least one pte mapped like
  * it would have happened if the vma was large enough during page
@@ -77,16 +81,26 @@
 
 static struct kmem_cache *mm_slot_cache __read_mostly;
 
+struct kobject *sysfs_hugepage_kobj = NULL;
+
+#ifdef CONFIG_OSA
+struct list_head osa_hpage_scan_list;
+spinlock_t worklist_lock;
+#endif
+
+
 /**
  * struct mm_slot - hash lookup from mm to mm_slot
  * @hash: hash collision list
  * @mm_node: khugepaged scan list headed in khugepaged_scan.mm_head
  * @mm: the mm that this information is valid for
+ * @address: The last address scanned 
  */
 struct mm_slot {
 	struct hlist_node hash;
 	struct list_head mm_node;
 	struct mm_struct *mm;
+	unsigned long address;
 };
 
 /**
@@ -579,6 +593,8 @@
 		return -ENOMEM;
 	}
 
+	sysfs_hugepage_kobj = *hugepage_kobj;
+
 	err = sysfs_create_group(*hugepage_kobj, &hugepage_attr_group);
 	if (err) {
 		pr_err("failed to register transparent hugepage group\n");
@@ -703,11 +719,12 @@
 	return pmd;
 }
 
-static inline pmd_t mk_huge_pmd(struct page *page, pgprot_t prot)
+static pmd_t mk_huge_pmd(struct page *page, pgprot_t prot)
 {
 	pmd_t entry;
 	entry = mk_pmd(page, prot);
 	entry = pmd_mkhuge(entry);
+
 	return entry;
 }
 
@@ -1044,7 +1061,8 @@
 					unsigned long address,
 					pmd_t *pmd, pmd_t orig_pmd,
 					struct page *page,
-					unsigned long haddr)
+					unsigned long haddr,
+					unsigned int flags)
 {
 	struct mem_cgroup *memcg;
 	spinlock_t *ptl;
@@ -1149,7 +1167,7 @@
 }
 
 int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
-			unsigned long address, pmd_t *pmd, pmd_t orig_pmd)
+			unsigned long address, pmd_t *pmd, pmd_t orig_pmd, unsigned int flags)
 {
 	spinlock_t *ptl;
 	int ret = 0;
@@ -1196,7 +1214,7 @@
 			ret |= VM_FAULT_FALLBACK;
 		} else {
 			ret = do_huge_pmd_wp_page_fallback(mm, vma, address,
-					pmd, orig_pmd, page, haddr);
+					pmd, orig_pmd, page, haddr, flags);
 			if (ret & VM_FAULT_OOM) {
 				split_huge_page(page);
 				ret |= VM_FAULT_FALLBACK;
@@ -1245,6 +1263,7 @@
 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
 		pmdp_huge_clear_flush_notify(vma, haddr, pmd);
 		page_add_new_anon_rmap(new_page, vma, haddr);
+
 		mem_cgroup_commit_charge(new_page, memcg, false);
 		lru_cache_add_active_or_unevictable(new_page, vma);
 		set_pmd_at(mm, haddr, pmd, entry);
@@ -2119,8 +2138,16 @@
 	spin_unlock(&khugepaged_mm_lock);
 
 	atomic_inc(&mm->mm_count);
-	if (wakeup)
+
+#ifdef CONFIG_OSA
+	osa_hpage_enter_list(mm);
+#endif
+	if (wakeup) {
 		wake_up_interruptible(&khugepaged_wait);
+#ifdef CONFIG_OSA
+		wake_up_interruptible(&osa_hpage_scand_wait);
+#endif
+	}
 
 	return 0;
 }
@@ -2151,6 +2178,10 @@
 	struct mm_slot *mm_slot;
 	int free = 0;
 
+#ifdef CONFIG_OSA
+	osa_hpage_exit_list(mm);
+#endif
+
 	spin_lock(&khugepaged_mm_lock);
 	mm_slot = get_mm_slot(mm);
 	if (mm_slot && khugepaged_scan.mm_slot != mm_slot) {
@@ -2274,7 +2305,8 @@
 		    mmu_notifier_test_young(vma->vm_mm, address))
 			referenced = true;
 	}
-	if (likely(referenced && writable))
+	//if (likely(referenced && writable))
+	if (likely(referenced))
 		return 1;
 out:
 	release_pte_pages(pte, _pte);
@@ -2442,7 +2474,7 @@
 	return 0;
 }
 
-static inline struct page *alloc_hugepage(int defrag)
+static struct page *alloc_hugepage(int defrag)
 {
 	return alloc_pages(alloc_hugepage_gfpmask(defrag, 0),
 			   HPAGE_PMD_ORDER);
@@ -2514,7 +2546,7 @@
 	pmd_t *pmd, _pmd;
 	pte_t *pte;
 	pgtable_t pgtable;
-	struct page *new_page;
+	struct page *new_page = NULL;
 	spinlock_t *pmd_ptl, *pte_ptl;
 	int isolated;
 	unsigned long hstart, hend;
@@ -2522,6 +2554,9 @@
 	unsigned long mmun_start;	/* For mmu_notifiers */
 	unsigned long mmun_end;		/* For mmu_notifiers */
 	gfp_t gfp;
+#ifdef CONFIG_OSA
+	popl_node_t *popl_node;
+#endif
 
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
@@ -2531,6 +2566,7 @@
 
 	/* release the mmap_sem read lock. */
 	new_page = khugepaged_alloc_page(hpage, gfp, mm, vma, address, node);
+
 	if (!new_page)
 		return;
 
@@ -2632,6 +2668,13 @@
 	*hpage = NULL;
 
 	khugepaged_pages_collapsed++;
+#ifdef CONFIG_OSA
+	popl_node = osa_popl_node_lookup(mm, 
+			HPAGE_ALIGN_FLOOR(address));
+
+	if (popl_node)
+		popl_node->committed = 1;
+#endif
 out_up_write:
 	up_write(&mm->mmap_sem);
 	return;
@@ -2641,7 +2684,9 @@
 	goto out_up_write;
 }
 
-static int khugepaged_scan_pmd(struct mm_struct *mm,
+static int 
+//__attribute__((optimize("O0")))
+khugepaged_scan_pmd(struct mm_struct *mm,
 			       struct vm_area_struct *vma,
 			       unsigned long address,
 			       struct page **hpage)
@@ -2706,8 +2751,54 @@
 		    mmu_notifier_test_young(vma->vm_mm, address))
 			referenced = true;
 	}
-	if (referenced && writable)
+
+    /* XXX: Disable this condition for fair huge page reclamation */
+	//if (referenced && writable)
+	if (referenced)
 		ret = 1;
+
+#ifdef CONFIG_OSA
+	if (deferred_mode > 0) {
+		popl_node_t *popl_node;
+		unsigned int weight = 0;
+		unsigned int _util_threshold = 
+			(util_threshold * 512) / 100;
+
+        popl_node = osa_popl_node_lookup(mm, 
+				HPAGE_ALIGN_FLOOR(address));
+
+		if (unlikely(!popl_node))
+			goto out_unmap;
+
+		weight = bitmap_weight(popl_node->popl_bitmap, 512);
+
+		/*
+		rcu_read_lock();
+		trace_printk("[%d] do allocation: %d %lx\n", 
+				rcu_dereference(mm->owner)->pid,
+				weight, HPAGE_ALIGN_FLOOR(address));
+		rcu_read_unlock();
+		*/
+
+		/* does not promote under-utilized region */
+		if (weight < _util_threshold) {
+			rcu_read_lock();
+			trace_printk("[%d] - underutilized: %d < %d %lx\n", 
+					rcu_dereference(mm->owner)->pid,
+					weight, _util_threshold, 
+					HPAGE_ALIGN_FLOOR(address));
+			rcu_read_unlock();
+			ret = 0;
+		} else {
+			rcu_read_lock();
+			trace_printk("[%d] + utilized: %d < %d %lx\n", 
+					rcu_dereference(mm->owner)->pid,
+					weight, _util_threshold, 
+					HPAGE_ALIGN_FLOOR(address));
+			rcu_read_unlock();
+		}
+	}
+#endif
 out_unmap:
 	pte_unmap_unlock(pte, ptl);
 	if (ret) {
@@ -2742,12 +2833,99 @@
 	}
 }
 
-static unsigned int khugepaged_scan_mm_slot(unsigned int pages,
-					    struct page **hpage)
+static struct mm_slot 
+//__attribute__((optimize("O0")))
+*osa_khugepaged_schedule_mm_slot(struct list_head *head)
+{
+	unsigned int fairness_metric = 0, metric_max = 1;
+	struct list_head *iterator, *tmp;
+	struct mm_slot *mm_slot_scanner = NULL, *_mm_slot = NULL;
+	struct mm_struct *_mm;
+    work_node_t *node = NULL, *node_tmp = NULL;
+
+    /* prioritize deferred promotions */
+	/* TODO: optimize this search hierarchy. */
+	list_for_each_entry_safe(node, node_tmp, &hugepage_worklist, list) {
+		_mm = node->mm;
+
+		list_for_each_safe(iterator, tmp, head) {
+			mm_slot_scanner = list_entry(iterator, struct mm_slot, mm_node);
+			if (mm_slot_scanner->mm == _mm) {
+				mm_slot_scanner->address = node->address;
+
+				spin_lock(&worklist_lock);
+				list_del(&node->list);
+				spin_unlock(&worklist_lock);
+				osa_work_node_free(node);
+
+				rcu_read_unlock();
+				_mm_slot = mm_slot_scanner;
+
+				goto out;
+			}
+		}
+
+		spin_lock(&worklist_lock);
+		list_del(&node->list);
+		spin_unlock(&worklist_lock);
+		osa_work_node_free(node);
+	}
+
+	if (hugepage_fairness) {
+		list_for_each_safe(iterator, tmp, head) {
+			mm_slot_scanner = list_entry(iterator, struct mm_slot, mm_node);
+			if (!mm_slot_scanner->mm)
+				continue;
+
+			_mm = mm_slot_scanner->mm;
+
+			if (unlikely(khugepaged_test_exit(_mm)))
+				continue;
+
+			if (_mm->def_flags & VM_NOHUGEPAGE) 
+				continue;
+
+			/* TODO: current implementation for fair promotion is
+			 * simplified to only deal with applications of non-zero
+			 * weight */
+			if (_mm->hpage_stats.weight == 0) {
+				mm_slot_scanner = NULL;
+				continue;
+			}
+
+			fairness_metric = osa_compute_fairness_metric(_mm);
+
+			/*
+			rcu_read_lock();
+			if (_mm)
+				trace_printk("[%d] fairness metric %d\n", 
+						rcu_dereference(_mm->owner)->pid,
+						fairness_metric); 
+			rcu_read_unlock();
+			*/
+
+			/* enforce minimum bound */
+			if (fairness_metric == 0)
+				fairness_metric = 1;
+
+			if (fairness_metric > metric_max) {
+				metric_max = fairness_metric;
+				_mm_slot = mm_slot_scanner;
+			}
+		}
+	}
+
+out:
+	return _mm_slot;
+}
+
+static unsigned int 
+//__attribute__((optimize("O0")))
+khugepaged_scan_mm_slot(unsigned int pages, struct page **hpage)
 	__releases(&khugepaged_mm_lock)
 	__acquires(&khugepaged_mm_lock)
 {
-	struct mm_slot *mm_slot;
+	struct mm_slot *mm_slot = NULL;
 	struct mm_struct *mm;
 	struct vm_area_struct *vma;
 	int progress = 0;
@@ -2755,17 +2933,40 @@
 	VM_BUG_ON(!pages);
 	VM_BUG_ON(NR_CPUS != 1 && !spin_is_locked(&khugepaged_mm_lock));
 
+#ifdef CONFIG_OSA
+	if (deferred_mode) {
+		struct mm_slot *_mm_slot = NULL;
+		_mm_slot = osa_khugepaged_schedule_mm_slot(&khugepaged_scan.mm_head);
+		if (_mm_slot) {
+			/*
+			rcu_read_lock();
+			if (_mm_slot->mm)
+				trace_printk("choosen %d address %lx\n", 
+						rcu_dereference(_mm_slot->mm->owner)->pid,
+						_mm_slot->address);
+			rcu_read_unlock();
+			*/
+
+			khugepaged_scan.mm_slot = _mm_slot;
+			khugepaged_scan.address = _mm_slot->address;
+		} 
+	} 
+#endif
+
 	if (khugepaged_scan.mm_slot)
 		mm_slot = khugepaged_scan.mm_slot;
 	else {
+		/* FCFS scanning */
 		mm_slot = list_entry(khugepaged_scan.mm_head.next,
-				     struct mm_slot, mm_node);
-		khugepaged_scan.address = 0;
+				struct mm_slot, mm_node);
+
 		khugepaged_scan.mm_slot = mm_slot;
+		khugepaged_scan.address = 0;
 	}
 	spin_unlock(&khugepaged_mm_lock);
 
 	mm = mm_slot->mm;
+
 	down_read(&mm->mmap_sem);
 	if (unlikely(khugepaged_test_exit(mm)))
 		vma = NULL;
@@ -2814,14 +3015,16 @@
 			if (ret)
 				/* we released mmap_sem so break loop */
 				goto breakouterloop_mmap_sem;
-			if (progress >= pages)
+			if (progress >= pages) {
 				goto breakouterloop;
+			}
 		}
 	}
 breakouterloop:
 	up_read(&mm->mmap_sem); /* exit_mmap will destroy ptes after this */
 breakouterloop_mmap_sem:
 
+	mm_slot->address = khugepaged_scan.address;
 	spin_lock(&khugepaged_mm_lock);
 	VM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);
 	/*
@@ -2835,29 +3038,73 @@
 		 * mm_slot not pointing to the exiting mm.
 		 */
 		if (mm_slot->mm_node.next != &khugepaged_scan.mm_head) {
+#ifdef CONFIG_OSA
+			if (deferred_mode) {
+				/*
+				struct mm_slot *_mm_slot = NULL;
+				_mm_slot = osa_khugepaged_schedule_mm_slot(&khugepaged_scan.mm_head);
+				if (_mm_slot)
+					khugepaged_scan.mm_slot = _mm_slot;
+				else {
+					khugepaged_scan.mm_slot = list_entry(
+							mm_slot->mm_node.next, struct mm_slot, mm_node);
+				}
+				*/
+				mm_slot->address = 0;
+			} else {
+				/* FCFS scanning */
+				khugepaged_scan.mm_slot = list_entry(
+						mm_slot->mm_node.next, struct mm_slot, mm_node);
+			}
+#else
 			khugepaged_scan.mm_slot = list_entry(
-				mm_slot->mm_node.next,
-				struct mm_slot, mm_node);
+					mm_slot->mm_node.next, struct mm_slot, mm_node);
+#endif
 			khugepaged_scan.address = 0;
 		} else {
+			if (deferred_mode)
+				mm_slot->address = 0;
+
 			khugepaged_scan.mm_slot = NULL;
 			khugepaged_full_scans++;
 		}
 
+		VM_BUG_ON_MM(!mm_slot, mm);
+
 		collect_mm_slot(mm_slot);
 	}
 
 	return progress;
 }
 
+static int khugepaged_has_deferred_work(void)
+{
+    int i;
+
+    for (i = MAX_WORKLIST_SIZE; i > 0;  i--) {
+        if (!list_empty(&hugepage_worklist))
+            return 1;
+    }
+
+    return 0;
+}
+
 static int khugepaged_has_work(void)
 {
+#ifdef CONFIG_OSA
+    if (deferred_mode && khugepaged_has_deferred_work()) 
+        return 1;
+#endif
 	return !list_empty(&khugepaged_scan.mm_head) &&
 		khugepaged_enabled();
 }
 
 static int khugepaged_wait_event(void)
 {
+#ifdef CONFIG_OSA
+    if (deferred_mode && khugepaged_has_deferred_work()) 
+        return 1;
+#endif
 	return !list_empty(&khugepaged_scan.mm_head) ||
 		kthread_should_stop();
 }
@@ -2885,8 +3132,7 @@
 			pass_through_head++;
 		if (khugepaged_has_work() &&
 		    pass_through_head < 2)
-			progress += khugepaged_scan_mm_slot(pages - progress,
-							    &hpage);
+			progress += khugepaged_scan_mm_slot(pages - progress, &hpage);
 		else
 			progress = pages;
 		spin_unlock(&khugepaged_mm_lock);
@@ -2901,7 +3147,12 @@
 	if (khugepaged_has_work()) {
 		if (!khugepaged_scan_sleep_millisecs)
 			return;
-
+#ifdef CONFIG_OSA
+        /* aggressive promotion when asynchronous promotion
+         * requests exist */
+        if (deferred_mode && khugepaged_has_deferred_work()) 
+            return;
+#endif
 		wait_event_freezable_timeout(khugepaged_wait,
 					     kthread_should_stop(),
 			msecs_to_jiffies(khugepaged_scan_sleep_millisecs));
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/hugetlb.c linux-4.3-osa/mm/hugetlb.c
--- linux-4.3-org/mm/hugetlb.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/hugetlb.c	2016-10-07 00:20:04.129386372 -0500
@@ -3231,7 +3231,7 @@
  */
 static int hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, pte_t *ptep, pte_t pte,
-			struct page *pagecache_page, spinlock_t *ptl)
+			struct page *pagecache_page, unsigned int flags, spinlock_t *ptl)
 {
 	struct hstate *h = hstate_vma(vma);
 	struct page *old_page, *new_page;
@@ -3334,6 +3334,7 @@
 		mmu_notifier_invalidate_range(mm, mmun_start, mmun_end);
 		set_huge_pte_at(mm, address, ptep,
 				make_huge_pte(vma, new_page, 1));
+
 		page_remove_rmap(old_page);
 		hugepage_add_new_anon_rmap(new_page, vma, address);
 		/* Make the old page be freed below */
@@ -3507,11 +3508,12 @@
 		page_dup_rmap(page);
 	new_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)
 				&& (vma->vm_flags & VM_SHARED)));
+
 	set_huge_pte_at(mm, address, ptep, new_pte);
 
 	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
 		/* Optimization, do the COW without a second fault */
-		ret = hugetlb_cow(mm, vma, address, ptep, new_pte, page, ptl);
+		ret = hugetlb_cow(mm, vma, address, ptep, new_pte, page, flags, ptl);
 	}
 
 	spin_unlock(ptl);
@@ -3566,9 +3568,9 @@
 			unsigned long address, unsigned int flags)
 {
 	pte_t *ptep, entry;
-	spinlock_t *ptl;
+	spinlock_t *ptl = NULL;
 	int ret;
-	u32 hash;
+	u32 hash = 0x0;
 	pgoff_t idx;
 	struct page *page = NULL;
 	struct page *pagecache_page = NULL;
@@ -3579,6 +3581,7 @@
 	address &= huge_page_mask(h);
 
 	ptep = huge_pte_offset(mm, address);
+
 	if (ptep) {
 		entry = huge_ptep_get(ptep);
 		if (unlikely(is_hugetlb_entry_migration(entry))) {
@@ -3666,7 +3669,7 @@
 	if (flags & FAULT_FLAG_WRITE) {
 		if (!huge_pte_write(entry)) {
 			ret = hugetlb_cow(mm, vma, address, ptep, entry,
-					pagecache_page, ptl);
+					pagecache_page, flags, ptl);
 			goto out_put_page;
 		}
 		entry = huge_pte_mkdirty(entry);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/internal.h linux-4.3-osa/mm/internal.h
--- linux-4.3-org/mm/internal.h	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/internal.h	2016-03-15 06:10:33.931112625 -0500
@@ -99,6 +99,7 @@
 extern int isolate_lru_page(struct page *page);
 extern void putback_lru_page(struct page *page);
 extern bool zone_reclaimable(struct zone *zone);
+extern unsigned long get_lru_size(struct lruvec *lruvec, enum lru_list lru);
 
 /*
  * in mm/rmap.c:
@@ -189,6 +190,9 @@
 	const gfp_t gfp_mask;		/* gfp mask of a direct compactor */
 	const int alloc_flags;		/* alloc flags of a direct compactor */
 	const int classzone_idx;	/* zone index of a direct compactor */
+#ifdef CONFIG_OSA
+	unsigned int osa_cc_counts;
+#endif
 	struct zone *zone;
 	int contended;			/* Signal need_sched() or lock
 					 * contention detected during
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/ksm.c linux-4.3-osa/mm/ksm.c
--- linux-4.3-org/mm/ksm.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/ksm.c	2016-09-20 00:21:29.166619531 -0500
@@ -37,6 +37,7 @@
 #include <linux/freezer.h>
 #include <linux/oom.h>
 #include <linux/numa.h>
+#include <osa/osa.h>
 
 #include <asm/tlbflush.h>
 #include "internal.h"
@@ -49,6 +50,10 @@
 #define DO_NUMA(x)	do { } while (0)
 #endif
 
+#define ksm_printk(fmt, ...) \
+	do { if(ksm_debug) trace_printk(fmt, __VA_ARGS__); } while(0)
+
+
 /*
  * A few notes about the KSM scanning process,
  * to make it easier to understand the data structures below:
@@ -141,6 +146,7 @@
 	};
 	struct hlist_head hlist;
 	unsigned long kpfn;
+	uint8_t is_huge_kpage;
 #ifdef CONFIG_NUMA
 	int nid;
 #endif
@@ -169,6 +175,7 @@
 	struct mm_struct *mm;
 	unsigned long address;		/* + low bits used for flags below */
 	unsigned int oldchecksum;	/* when unstable */
+	int8_t is_huge_page;
 	union {
 		struct rb_node node;	/* when node of unstable tree */
 		struct {		/* when listed from stable tree */
@@ -182,12 +189,27 @@
 #define UNSTABLE_FLAG	0x100	/* is a node of the unstable tree */
 #define STABLE_FLAG	0x200	/* is listed from the stable tree */
 
+#define NO_HPAGE_MERGE 0
+#define HPAGE_MERGE_SPLIT 1
+#define HPAGE_MERGE_NO_SPLIT 2
+#define HPAGE_MERGE_PRESERVE 3
+#define HPAGE_MERGE_FREQUENCY 4
+#define HPAGE_MERGE_MAX 4
+
 /* The stable and unstable tree heads */
 static struct rb_root one_stable_tree[1] = { RB_ROOT };
 static struct rb_root one_unstable_tree[1] = { RB_ROOT };
 static struct rb_root *root_stable_tree = one_stable_tree;
 static struct rb_root *root_unstable_tree = one_unstable_tree;
 
+#if 1
+static struct rb_root root_stable_tree_huge[1];
+static struct rb_root root_unstable_tree_huge[1];
+#else
+static struct rb_root *root_stable_tree_huge = one_stable_tree;
+static struct rb_root *root_unstable_tree_huge = one_unstable_tree;
+#endif
+
 /* Recently migrated nodes of stable tree, pending proper placement */
 static LIST_HEAD(migrate_nodes);
 
@@ -210,6 +232,7 @@
 
 /* The number of page slots additionally sharing those nodes */
 static unsigned long ksm_pages_sharing;
+static unsigned long ksm_pages_sharing_huge;
 
 /* The number of nodes in the unstable tree */
 static unsigned long ksm_pages_unshared;
@@ -222,6 +245,17 @@
 
 /* Milliseconds ksmd should sleep between batches */
 static unsigned int ksm_thread_sleep_millisecs = 20;
+static unsigned int adaptive_sleep_interval = 20;
+static unsigned int sleep_scaling_factor = 10;
+static unsigned int hpage_preserve = 70; /* 70% */
+
+static unsigned int ksm_debug;
+static unsigned int ksm_merge_hugepage = HPAGE_MERGE_SPLIT;
+
+/* do not use kernel crypto.
+ * KSMd holds lock(rwsem) at non-sleeping context
+ * but crypto functions are sleeping functions */
+#undef USE_JHASH
 
 #ifdef CONFIG_NUMA
 /* Zeroed when merging across nodes is not allowed */
@@ -284,14 +318,20 @@
 	struct rmap_item *rmap_item;
 
 	rmap_item = kmem_cache_zalloc(rmap_item_cache, GFP_KERNEL);
+	/*
 	if (rmap_item)
 		ksm_rmap_items++;
+	*/
+
 	return rmap_item;
 }
 
 static inline void free_rmap_item(struct rmap_item *rmap_item)
 {
-	ksm_rmap_items--;
+	if (rmap_item->is_huge_page > 0)
+		ksm_rmap_items -= HPAGE_PMD_NR;
+	else
+		ksm_rmap_items--;
 	rmap_item->mm = NULL;	/* debug safety */
 	kmem_cache_free(rmap_item_cache, rmap_item);
 }
@@ -349,6 +389,29 @@
 	return atomic_read(&mm->mm_users) == 0;
 }
 
+#ifdef USE_JHASH
+static inline u32 get_page_hash_digest(struct page *page)
+{
+	void *addr;
+	u32 checksum = 17;
+	if (PageTransCompound(page)) {
+		int i;
+
+		for (i = 0; i < HPAGE_PMD_NR; i++) {
+			addr = kmap_atomic(page + i);
+			checksum = jhash2(addr, PAGE_SIZE / 4, checksum);
+			kunmap_atomic(addr);
+		}
+
+	} else {
+		addr = kmap_atomic(page);
+		checksum = jhash2(addr, PAGE_SIZE / 4, checksum);
+		kunmap_atomic(addr);
+	}
+	return checksum;
+}
+#endif
+
 /*
  * We use break_ksm to break COW on a ksm page: it's a stripped down
  *
@@ -363,7 +426,7 @@
 static int break_ksm(struct vm_area_struct *vma, unsigned long addr)
 {
 	struct page *page;
-	int ret = 0;
+	int ret = 0, trial = 0;
 
 	do {
 		cond_resched();
@@ -376,6 +439,18 @@
 		else
 			ret = VM_FAULT_WRITE;
 		put_page(page);
+
+		/* FIXME: Does khugepaged set write bit, making handle_mm_fault returns 0? */
+		if (PageTransCompound(page)) {
+			if (ret == 0) {
+				trace_printk("handle_mm_fault returns 0.. retrying %d\n", trial);
+
+				if (trial > 100)
+					break;
+			}
+		}
+
+		trial++;
 	} while (!(ret & (VM_FAULT_WRITE | VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));
 	/*
 	 * We must loop because handle_mm_fault() may back out if there's
@@ -497,10 +572,19 @@
 	struct rmap_item *rmap_item;
 
 	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
-		if (rmap_item->hlist.next)
-			ksm_pages_sharing--;
-		else
-			ksm_pages_shared--;
+		if (rmap_item->hlist.next) {
+			if (stable_node->is_huge_kpage > 0) {
+				ksm_pages_sharing -= HPAGE_PMD_NR;
+				ksm_pages_sharing_huge -= HPAGE_PMD_NR;
+			} else
+				ksm_pages_sharing--;
+		} else {
+			if (stable_node->is_huge_kpage > 0)
+				ksm_pages_shared -= HPAGE_PMD_NR;
+			else
+				ksm_pages_shared--;
+		}
+
 		put_anon_vma(rmap_item->anon_vma);
 		rmap_item->address &= PAGE_MASK;
 		cond_resched();
@@ -508,9 +592,14 @@
 
 	if (stable_node->head == &migrate_nodes)
 		list_del(&stable_node->list);
-	else
-		rb_erase(&stable_node->node,
-			 root_stable_tree + NUMA(stable_node->nid));
+	else {
+		if (stable_node->is_huge_kpage)
+			rb_erase(&stable_node->node,
+				 root_stable_tree_huge + NUMA(stable_node->nid));
+		else
+			rb_erase(&stable_node->node,
+				 root_stable_tree + NUMA(stable_node->nid));
+	}
 	free_stable_node(stable_node);
 }
 
@@ -554,6 +643,16 @@
 	if (READ_ONCE(page->mapping) != expected_mapping)
 		goto stale;
 
+	/* Ksm page was huge page when it is inserted into stable tree
+	 * but now it is split into base pages */
+	if (stable_node->is_huge_kpage) {
+		if (!PageTransCompound(page))
+			goto stale;
+	} else {
+		if (PageTransCompound(page))
+			goto stale;
+	}
+
 	/*
 	 * We cannot do anything with the page while its refcount is 0.
 	 * Usually 0 means free, or tail of a higher-order page: in which
@@ -625,10 +724,18 @@
 		unlock_page(page);
 		put_page(page);
 
-		if (stable_node->hlist.first)
-			ksm_pages_sharing--;
-		else
-			ksm_pages_shared--;
+		if (stable_node->hlist.first) {
+			if (stable_node->is_huge_kpage > 0) {
+				ksm_pages_sharing -= HPAGE_PMD_NR;
+				ksm_pages_sharing_huge -= HPAGE_PMD_NR;
+			} else
+				ksm_pages_sharing--;
+		} else {
+			if (stable_node->is_huge_kpage > 0)
+				ksm_pages_shared -= HPAGE_PMD_NR;
+			else
+				ksm_pages_shared--;
+		}
 
 		put_anon_vma(rmap_item->anon_vma);
 		rmap_item->address &= PAGE_MASK;
@@ -643,11 +750,22 @@
 		 * than left over from before.
 		 */
 		age = (unsigned char)(ksm_scan.seqnr - rmap_item->address);
-		BUG_ON(age > 1);
-		if (!age)
-			rb_erase(&rmap_item->node,
-				 root_unstable_tree + NUMA(rmap_item->nid));
-		ksm_pages_unshared--;
+		//BUG_ON(age > 1);
+		if (!age) {
+			/* FIXME: if rmap_item is not hugepage any more but
+			 * when it was for hugepage and inserted to unstable_tree_huge
+			 * or vice versa, this rb_erase occurs kernel BUG */
+			if(rmap_item->is_huge_page)
+				rb_erase(&rmap_item->node,
+						root_unstable_tree_huge + NUMA(rmap_item->nid));
+			else
+				rb_erase(&rmap_item->node,
+						root_unstable_tree + NUMA(rmap_item->nid));
+		}
+		if (rmap_item->is_huge_page)
+			ksm_pages_unshared -= HPAGE_PMD_NR;
+		else
+			ksm_pages_unshared--;
 		rmap_item->address &= PAGE_MASK;
 	}
 out:
@@ -754,7 +872,18 @@
 			}
 			cond_resched();
 		}
+
+		while (root_stable_tree_huge[nid].rb_node) {
+			stable_node = rb_entry(root_stable_tree_huge[nid].rb_node,
+						struct stable_node, node);
+			if (remove_stable_node(stable_node)) {
+				err = -EBUSY;
+				break;	/* proceed to next nid */
+			}
+			cond_resched();
+		}
 	}
+
 	list_for_each_safe(this, next, &migrate_nodes) {
 		stable_node = list_entry(this, struct stable_node, list);
 		if (remove_stable_node(stable_node))
@@ -847,11 +976,200 @@
 	return ret;
 }
 
+static int memcmp_huge_pages(struct page *page1, struct page *page2)
+{
+	if (PageTransCompound(page1) && PageTransCompound(page2)) {
+		/* Can I use get_user_huge_page? */
+		int ret;
+		char *addr1, *addr2;
+		VM_BUG_ON(!PageHead(page1));
+		VM_BUG_ON(!PageHead(page2));
+
+		addr1 = kmap_atomic(page1);
+		addr2 = kmap_atomic(page2);
+		ret = memcmp(addr1, addr2, HPAGE_PMD_SIZE);
+		kunmap_atomic(addr2);
+		kunmap_atomic(addr1);
+
+		return ret;
+	}
+
+	trace_printk("memcmp of hugepage is ignored\n");
+	return 0;
+}
+
+
+/* this function is called with page lock for page 1 but not for page 2 */
 static inline int pages_identical(struct page *page1, struct page *page2)
 {
+	unsigned int i = 0;
+	if (PageTransCompound(page1) && PageTransCompound(page2)) {
+		/* Can I use get_user_huge_page? */
+		int ret;
+
+		VM_BUG_ON(!PageHead(page1));
+		VM_BUG_ON(!PageHead(page2));
+
+		/* FIXME: In case of Huge page, is it OK only to hold 
+		 * spinlock of header page? */
+		if (!trylock_page(page2))
+			return 0;
+#if 1
+		for (i = 0; i < HPAGE_PMD_NR; i++) {
+			ret = memcmp_pages(page1 + i, page2 + i);
+
+			if (ret)
+				break;
+		}
+#else
+		ret = memcmp_huge_pages(page1, page2);
+#endif
+		unlock_page(page2);
+
+		return !ret;
+	} 
 	return !memcmp_pages(page1, page2);
 }
 
+/* This function does not consider other architectures but for x86_64
+ * so update_mmu_cache_pmd pte_unmap.. etc is not considered */
+static int write_protect_huge_page(struct vm_area_struct *vma, 
+		struct page *page, pmd_t *orig_pmd)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long addr;
+	unsigned long mmun_start, mmun_end;
+	pmd_t *pmdp;
+	spinlock_t *ptl;
+	int err = -EFAULT;
+
+	addr = page_address_in_vma(page, vma);
+	if (addr == -EFAULT)
+		goto out;
+
+	if (!PageTransCompound(page))
+		trace_printk("page is not a huge page\n");
+
+	addr &= HPAGE_PMD_MASK;
+
+	mmun_start = addr;
+	mmun_end = addr + HPAGE_PMD_SIZE;
+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
+
+	/* pmd operation is serialized by spinlock */
+	pmdp = page_check_address_pmd(page, mm, addr, 
+			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &ptl);
+
+	if (!pmdp) 
+		goto out_mn_notifier;
+
+	/* note: I think I don't need to handle the case wherein the
+	 * huge page is in swapcache since if the huge pages are reclaimed
+	 * it means the huge page is already splitted so it is not the case
+	 * for huge page sharing */
+	if (pmd_write(*pmdp) || pmd_dirty(*pmdp)){
+		pmd_t entry;
+		/* Think about the case to handle here 
+		 * Huge page is dirtied?, other cases? */
+
+		/* TLB flush */
+		entry = pmdp_huge_clear_flush_notify(vma, addr, pmdp);
+
+		if (pmd_dirty(entry))
+			set_page_dirty(page);
+
+		entry = pmd_wrprotect(entry);
+		/* why should I make it clean? it copied it from write_protect_page */
+		entry = pmd_clear_flags(entry, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
+
+		/* finally, change write bit of page table but 
+		 * I did not optimize it with pte_change mmu notifier 
+		 * as original implementation does 
+		 * so I just invalidate the corresponding entry in NPT 
+		 * later, KVM will apply this changes during the ept violation */
+		set_pmd_at(mm, addr, pmdp, entry);
+	}
+
+	*orig_pmd = *pmdp;
+	err = 0;
+
+	//ksm_printk("va: %lx - %lx\n", mmun_start, mmun_end);
+	spin_unlock(ptl);
+out_mn_notifier:
+	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
+out:
+	return err;
+}
+
+/**
+ * replace_huge_page - replace page in vma by new ksm page
+ * @vma:      vma that holds the pte pointing to page
+ * @page:     the page we are replacing by kpage
+ * @kpage:    the ksm page we replace page by
+ * @orig_pmd: the original value of the pmd for page
+ *
+ * Returns 0 on success, -EFAULT on failure.
+ */
+static int replace_huge_page(struct vm_area_struct *vma, struct page *page,
+			struct page *kpage, pmd_t orig_pmd)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	pmd_t *pmdp, entry;
+	spinlock_t *ptl;
+	unsigned long addr;
+	int err = -EFAULT;
+	unsigned long mmun_start, mmun_end;	/* For mmu_notifiers */
+
+	BUG_ON(PageTransCompound(kpage) != PageTransCompound(page));
+
+	addr = page_address_in_vma(page, vma);
+	if (addr == -EFAULT)
+		goto out;
+
+	addr &= HPAGE_PMD_MASK;
+
+	mmun_start = addr;
+	mmun_end = addr + HPAGE_PMD_SIZE;
+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
+
+	/* pmd operation is serialized by spinlock */
+	pmdp = page_check_address_pmd(page, mm, addr, 
+			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &ptl);
+
+	if (!pmdp) 
+		goto out_mn_notifier;
+
+	if (!pmd_same(*pmdp, orig_pmd)) {
+		//trace_printk("%s: pmd value does not match\n", __func__);
+		spin_unlock(ptl);
+		goto out_mn_notifier;
+	}
+
+	get_page(kpage);
+	page_add_anon_rmap(kpage, vma, addr);
+	pmdp_huge_clear_flush_notify(vma, addr, pmdp);
+
+	/* mk_huge_pmd */
+	entry = mk_pmd(kpage, vma->vm_page_prot);
+	entry = pmd_mkhuge(entry);
+	/* finally, change mapping of page's pmd to kpage's pmd */
+	set_pmd_at(mm, addr, pmdp, entry);
+
+	page_remove_rmap(page);
+	if (!page_mapped(page))
+		try_to_free_swap(page);
+	put_page(page);
+
+	spin_unlock(ptl);
+	err = 0;
+
+	//ksm_printk("va: %lx - %lx\n", mmun_start, mmun_end);
+out_mn_notifier:
+	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
+out:
+	return err;
+}
+
 static int write_protect_page(struct vm_area_struct *vma, struct page *page,
 			      pte_t *orig_pte)
 {
@@ -1074,6 +1392,82 @@
 }
 
 /*
+ * try_to_merge_huge_page - take two pages and merge them into one
+ * @vma: the vma that holds the pte pointing to page
+ * @page: the PageAnon page that we want to replace with kpage
+ * @kpage: the PageKsm page that we want to map instead of page,
+ *         or NULL the first time when we want to use page as kpage.
+ *
+ * This function returns 0 if the pages were merged, -EFAULT otherwise.
+ */
+static int try_to_merge_huge_page(struct vm_area_struct *vma,
+				 struct page *page, struct page *kpage)
+{
+	pmd_t orig_pmd  = __pmd(0);
+	int err = -EFAULT;
+
+	if (page == kpage)			/* ksm page forked */
+		return 0;
+
+	if (!(vma->vm_flags & VM_MERGEABLE))
+		goto out;
+	if (!PageAnon(page))
+		goto out;
+	if (kpage && !PageTransCompound(kpage))
+		goto out;
+
+	BUG_ON(!PageTransCompound(page));
+
+	BUG_ON(page != compound_head(page));
+
+	/*
+	 * We need the page lock to read a stable PageSwapCache in
+	 * write_protect_page().  We use trylock_page() instead of
+	 * lock_page() because we don't want to wait here - we
+	 * prefer to continue scanning and merging different pages,
+	 * then come back to this page when it is unlocked.
+	 */
+	if (!trylock_page(page))
+		goto out;
+
+	/*
+	 * If this anonymous page is mapped only here, its pte may need
+	 * to be write-protected.  If it's mapped elsewhere, all of its
+	 * ptes are necessarily already write-protected.  But in either
+	 * case, we need to lock and check page_count is not raised.
+	 */
+	if (write_protect_huge_page(vma, page, &orig_pmd) == 0) {
+		if (!kpage) {
+			/*
+			 * While we hold page lock, upgrade page from
+			 * PageAnon+anon_vma to PageKsm+NULL stable_node:
+			 * stable_tree_insert() will update stable_node.
+			 */
+			set_page_stable_node(page, NULL);
+			mark_page_accessed(page);
+			err = 0;
+		} else if (pages_identical(page, kpage)) {
+			err = replace_huge_page(vma, page, kpage, orig_pmd);
+		}
+	}
+
+	if ((vma->vm_flags & VM_LOCKED) && kpage && !err) {
+		munlock_vma_page(page);
+		if (!PageMlocked(kpage)) {
+			unlock_page(page);
+			lock_page(kpage);
+			mlock_vma_page(kpage);
+			page = kpage;		// for final unlock 
+		}
+	}
+
+	unlock_page(page);
+out:
+	BUG_ON(pmd_write(orig_pmd));
+	return err;
+}
+
+/*
  * try_to_merge_with_ksm_page - like try_to_merge_two_pages,
  * but no new kernel page is allocated: kpage must already be a ksm page.
  *
@@ -1093,7 +1487,12 @@
 	if (!vma || vma->vm_start > rmap_item->address)
 		goto out;
 
-	err = try_to_merge_one_page(vma, page, kpage);
+	if (rmap_item->is_huge_page && page &&
+			page_trans_compound_anon(page)) {
+		err = try_to_merge_huge_page(vma, page, kpage);
+	} else {
+		err = try_to_merge_one_page(vma, page, kpage);
+	}
 	if (err)
 		goto out;
 
@@ -1148,7 +1547,8 @@
  * This function returns the stable tree node of identical content if found,
  * NULL otherwise.
  */
-static struct page *stable_tree_search(struct page *page)
+static struct page *stable_tree_search(struct page *page, 
+		struct rmap_item *rmap_item)
 {
 	int nid;
 	struct rb_root *root;
@@ -1156,6 +1556,9 @@
 	struct rb_node *parent;
 	struct stable_node *stable_node;
 	struct stable_node *page_node;
+#ifdef USE_JHASH
+	u32 digest1, digest2;
+#endif
 
 	page_node = page_stable_node(page);
 	if (page_node && page_node->head != &migrate_nodes) {
@@ -1165,7 +1568,10 @@
 	}
 
 	nid = get_kpfn_nid(page_to_pfn(page));
-	root = root_stable_tree + nid;
+	if (rmap_item->is_huge_page)
+		root = root_stable_tree_huge + nid;
+	else
+		root = root_stable_tree + nid;
 again:
 	new = &root->rb_node;
 	parent = NULL;
@@ -1180,7 +1586,45 @@
 		if (!tree_page)
 			return NULL;
 
-		ret = memcmp_pages(page, tree_page);
+#ifdef USE_JHASH
+		if (rmap_item->is_huge_page) { 
+			if (PageTransCompound(page) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+			digest1 = get_page_hash_digest(page);
+			digest2 = get_page_hash_digest(tree_page);
+
+			if (digest1 > digest2)
+				ret = 1;
+			else if (digest1 < digest2)
+				ret = -1;
+			else
+				ret = 0;
+		} else {
+			ret = memcmp_pages(page, tree_page); 
+		}
+#else
+		if (rmap_item->is_huge_page) { 
+			int i, r, match;
+			if (PageTransCompound(page) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+
+			if (ksm_debug == 2) {
+				for (i = 0, match = 0; i < HPAGE_PMD_NR; i++) {
+					r = memcmp_pages(page + i, tree_page + i);
+					if (!r)
+						match++;
+				}
+
+				if (match > 0 && match < HPAGE_PMD_NR)
+					trace_printk("%lx %d\n", page_to_pfn(page), match);
+			}
+
+			ret = memcmp_huge_pages(page, tree_page);
+		} else {
+			ret = memcmp_pages(page, tree_page); 
+		}
+#endif
+
 		put_page(tree_page);
 
 		parent = *new;
@@ -1196,6 +1640,8 @@
 			 * It would be more elegant to return stable_node
 			 * than kpage, but that involves more changes.
 			 */
+			/* TODO: stable_node was huge page but it is splitted 
+			 * at some point so it stays out-of-sync state */
 			tree_page = get_ksm_page(stable_node, true);
 			if (tree_page) {
 				unlock_page(tree_page);
@@ -1248,7 +1694,8 @@
  * This function returns the stable tree node just allocated on success,
  * NULL otherwise.
  */
-static struct stable_node *stable_tree_insert(struct page *kpage)
+static struct stable_node *stable_tree_insert(struct page *kpage, 
+		struct rmap_item *rmap_item)
 {
 	int nid;
 	unsigned long kpfn;
@@ -1256,10 +1703,17 @@
 	struct rb_node **new;
 	struct rb_node *parent = NULL;
 	struct stable_node *stable_node;
+#ifdef USE_JHASH
+	u32 digest1, digest2;
+#endif
 
 	kpfn = page_to_pfn(kpage);
 	nid = get_kpfn_nid(kpfn);
-	root = root_stable_tree + nid;
+	if (rmap_item->is_huge_page)
+		root = root_stable_tree_huge + nid;
+	else
+		root = root_stable_tree + nid;
+
 	new = &root->rb_node;
 
 	while (*new) {
@@ -1272,7 +1726,45 @@
 		if (!tree_page)
 			return NULL;
 
-		ret = memcmp_pages(kpage, tree_page);
+#ifdef USE_JHASH
+		if (rmap_item->is_huge_page) {
+			if (PageTransCompound(tree_page) != PageTransCompound(kpage))
+				trace_printk("%s: page table is not matched\n", __func__);
+			digest1 = get_page_hash_digest(kpage);
+			digest2 = get_page_hash_digest(tree_page);
+
+			if (digest1 > digest2)
+				ret = 1;
+			else if (digest1 < digest2)
+				ret = -1;
+			else
+				ret = 0;
+		} else {
+			ret = memcmp_pages(kpage, tree_page);
+		}
+#else
+		if (rmap_item->is_huge_page) {
+			int i, r, match;
+			if (PageTransCompound(kpage) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+			
+			if (ksm_debug == 2) {
+				for (i = 0, match = 0; i < HPAGE_PMD_NR; i++) {
+					r = memcmp_pages(tree_page + i, kpage + i);
+					if (!r)
+						match++;
+				}
+
+				if (match > 0 && match < HPAGE_PMD_NR)
+					ksm_printk("%lx %d\n", page_to_pfn(tree_page), match);
+			}
+
+			ret = memcmp_huge_pages(kpage, tree_page);
+		} else {
+			ret = memcmp_pages(kpage, tree_page);
+		}
+#endif
+
 		put_page(tree_page);
 
 		parent = *new;
@@ -1294,6 +1786,11 @@
 	if (!stable_node)
 		return NULL;
 
+	if (rmap_item->is_huge_page)
+		stable_node->is_huge_kpage = 1;
+	else
+		stable_node->is_huge_kpage = 0;
+
 	INIT_HLIST_HEAD(&stable_node->hlist);
 	stable_node->kpfn = kpfn;
 	set_page_stable_node(kpage, stable_node);
@@ -1327,9 +1824,15 @@
 	struct rb_root *root;
 	struct rb_node *parent = NULL;
 	int nid;
+#ifdef USE_JHASH
+	u32 digest1, digest2;
+#endif
 
 	nid = get_kpfn_nid(page_to_pfn(page));
-	root = root_unstable_tree + nid;
+	if (rmap_item->is_huge_page)
+		root = root_unstable_tree_huge + nid;
+	else
+		root = root_unstable_tree + nid;
 	new = &root->rb_node;
 
 	while (*new) {
@@ -1351,8 +1854,44 @@
 			return NULL;
 		}
 
-		ret = memcmp_pages(page, tree_page);
+#ifdef USE_JHASH
+		if (rmap_item->is_huge_page) { 
+			if (PageTransCompound(page) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+			digest1 = get_page_hash_digest(page);
+			digest2 = get_page_hash_digest(tree_page);
+
+			if (digest1 > digest2)
+				ret = 1;
+			else if (digest1 < digest2)
+				ret = -1;
+			else
+				ret = 0;
+		} else {
+			ret = memcmp_pages(page, tree_page);
+		}
+#else
+		if (rmap_item->is_huge_page) {
+			int i, r, match;
+			if (PageTransCompound(page) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+
+			if (ksm_debug == 2) {
+				for (i = 0, match = 0; i < HPAGE_PMD_NR; i++) {
+					r = memcmp_pages(page + i, tree_page + i);
+					if (!r)
+						match++;
+				}
+
+				if (match > 0 && match < HPAGE_PMD_NR)
+					ksm_printk("%lx %d\n", page_to_pfn(page), match);
+			}
 
+			ret = memcmp_huge_pages(page, tree_page);
+		} else {
+			ret = memcmp_pages(page, tree_page);
+		}
+#endif
 		parent = *new;
 		if (ret < 0) {
 			put_page(tree_page);
@@ -1381,7 +1920,10 @@
 	rb_link_node(&rmap_item->node, parent, new);
 	rb_insert_color(&rmap_item->node, root);
 
-	ksm_pages_unshared++;
+	if (rmap_item->is_huge_page > 0)
+		ksm_pages_unshared += HPAGE_PMD_NR;
+	else
+		ksm_pages_unshared++;
 	return NULL;
 }
 
@@ -1397,10 +1939,24 @@
 	rmap_item->address |= STABLE_FLAG;
 	hlist_add_head(&rmap_item->hlist, &stable_node->hlist);
 
-	if (rmap_item->hlist.next)
-		ksm_pages_sharing++;
-	else
-		ksm_pages_shared++;
+	if (rmap_item->is_huge_page != stable_node->is_huge_kpage) {
+		trace_printk("%s: type mismatch %d %d\n",
+				__func__, rmap_item->is_huge_page, 
+				stable_node->is_huge_kpage);
+	}
+
+	if (rmap_item->hlist.next) {
+		if (rmap_item->is_huge_page > 0) {
+			ksm_pages_sharing += HPAGE_PMD_NR;
+			ksm_pages_sharing_huge += HPAGE_PMD_NR;
+		} else
+			ksm_pages_sharing++;
+	} else {
+		if (rmap_item->is_huge_page > 0)
+			ksm_pages_shared += HPAGE_PMD_NR;
+		else
+			ksm_pages_shared++;
+	}
 }
 
 /*
@@ -1425,8 +1981,12 @@
 	if (stable_node) {
 		if (stable_node->head != &migrate_nodes &&
 		    get_kpfn_nid(stable_node->kpfn) != NUMA(stable_node->nid)) {
-			rb_erase(&stable_node->node,
-				 root_stable_tree + NUMA(stable_node->nid));
+			if (rmap_item->is_huge_page)
+				rb_erase(&stable_node->node,
+						root_stable_tree_huge + NUMA(stable_node->nid));
+			else
+				rb_erase(&stable_node->node,
+						root_stable_tree + NUMA(stable_node->nid));
 			stable_node->head = &migrate_nodes;
 			list_add(&stable_node->list, stable_node->head);
 		}
@@ -1436,7 +1996,8 @@
 	}
 
 	/* We first start with searching the page inside the stable tree */
-	kpage = stable_tree_search(page);
+	/* kpage is refernece in the stable_tree_search via get_ksm_page */
+	kpage = stable_tree_search(page, rmap_item);
 	if (kpage == page && rmap_item->head == stable_node) {
 		put_page(kpage);
 		return;
@@ -1445,6 +2006,12 @@
 	remove_rmap_item_from_tree(rmap_item);
 
 	if (kpage) {
+		/* this is the case where head page of page and kpage has
+		 * identical contents but kpage or page is not huge page */
+		if ((ksm_merge_hugepage >= HPAGE_MERGE_NO_SPLIT) &&
+			(PageTransCompound(page) != PageTransCompound(kpage)))
+			goto unstable_search;
+
 		err = try_to_merge_with_ksm_page(rmap_item, page, kpage);
 		if (!err) {
 			/*
@@ -1459,6 +2026,7 @@
 		return;
 	}
 
+unstable_search:
 	/*
 	 * If the hash value of the page has changed from the last time
 	 * we calculated it, this page is changing frequently: therefore we
@@ -1483,7 +2051,7 @@
 			 * node in the stable tree and add both rmap_items.
 			 */
 			lock_page(kpage);
-			stable_node = stable_tree_insert(kpage);
+			stable_node = stable_tree_insert(kpage, rmap_item);
 			if (stable_node) {
 				stable_tree_append(tree_rmap_item, stable_node);
 				stable_tree_append(rmap_item, stable_node);
@@ -1528,6 +2096,7 @@
 		rmap_item->address = addr;
 		rmap_item->rmap_list = *rmap_list;
 		*rmap_list = rmap_item;
+		rmap_item->is_huge_page = -1;
 	}
 	return rmap_item;
 }
@@ -1539,6 +2108,8 @@
 	struct vm_area_struct *vma;
 	struct rmap_item *rmap_item;
 	int nid;
+	unsigned long mm_rss;
+	unsigned long count_hpage;
 
 	if (list_empty(&ksm_mm_head.mm_list))
 		return NULL;
@@ -1578,8 +2149,11 @@
 			}
 		}
 
-		for (nid = 0; nid < ksm_nr_node_ids; nid++)
+		/* re-initialze unstable tree after KSM pass (full scan) */
+		for (nid = 0; nid < ksm_nr_node_ids; nid++) {
 			root_unstable_tree[nid] = RB_ROOT;
+			root_unstable_tree_huge[nid] = RB_ROOT;
+		}
 
 		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
@@ -1620,21 +2194,115 @@
 				cond_resched();
 				continue;
 			}
+
 			if (PageAnon(*page) ||
 			    page_trans_compound_anon(*page)) {
+
+				if (ksm_merge_hugepage == NO_HPAGE_MERGE 
+						&& page_trans_compound_anon(*page))
+					goto skip;
+
 				flush_anon_page(vma, *page, ksm_scan.address);
 				flush_dcache_page(*page);
 				rmap_item = get_next_rmap_item(slot,
 					ksm_scan.rmap_list, ksm_scan.address);
 				if (rmap_item) {
-					ksm_scan.rmap_list =
-							&rmap_item->rmap_list;
-					ksm_scan.address += PAGE_SIZE;
+					ksm_scan.rmap_list = &rmap_item->rmap_list;
+
+					if (page_trans_compound_anon(*page)) {
+						void *addr;
+						util_node_t *util_node;
+
+						switch (ksm_merge_hugepage) {
+							case HPAGE_MERGE_PRESERVE:
+								mm_rss = get_mm_rss(rmap_item->mm);
+								count_hpage = osa_get_hpage_count(rmap_item->mm);
+
+								/* FIXME: subtle problem is here
+								 * at this time(scanning), hpage was over this threshold 
+								 * and the rmap_item is kept in unstable tree. Once ksmd
+								 * splits to hpages to reach this threshold so it stops 
+								 * marking as non-hugepage but items, which has been stored 
+								 * in unstable tree, can still split hpage, enabling spliting
+								 * more hpages */
+								if (count_hpage != 0 &&
+								((hpage_preserve * mm_rss / 100) < 
+										 (HPAGE_PMD_NR * count_hpage))) {
+									/* treat as base page */
+									if (rmap_item->is_huge_page < 0)
+										ksm_rmap_items++;
+
+									rmap_item->is_huge_page = 0;
+									ksm_scan.address += PAGE_SIZE;
+									adaptive_sleep_interval = ksm_thread_sleep_millisecs;
+									break;
+								}
+								// fall-through
+								/* handle as hugepage sharing */
+							case HPAGE_MERGE_NO_SPLIT:
+								if (*page != compound_head(*page))
+									goto skip;
+
+								if (rmap_item->is_huge_page < 0)
+									ksm_rmap_items += HPAGE_PMD_NR;
+
+								rmap_item->is_huge_page = 1;
+								ksm_scan.address += HPAGE_PMD_SIZE;
+
+								adaptive_sleep_interval = 
+									sleep_scaling_factor * ksm_thread_sleep_millisecs;
+								break;
+							case HPAGE_MERGE_FREQUENCY:
+								if (PageCompound(*page)) {
+									if (*page != compound_head(*page))
+										goto skip;
+
+									addr = page_address(*page);
+
+									// check frequency
+									util_node = osa_util_node_lookup(rmap_item->mm, 
+											HPAGE_ALIGN((unsigned long)addr));
+
+									if (util_node) {
+										// frequently accessed hugepage
+										if (util_node->frequency[0] > 0) {
+											if (rmap_item->is_huge_page < 0)
+												ksm_rmap_items += HPAGE_PMD_NR;
+
+											rmap_item->is_huge_page = 1;
+											ksm_scan.address += HPAGE_PMD_SIZE;
+
+											adaptive_sleep_interval = 
+												sleep_scaling_factor * ksm_thread_sleep_millisecs;
+											break;
+										}
+									}
+								}
+								// fall-through to split hugepage path.
+							default:
+								/* treat as base page */
+								if (rmap_item->is_huge_page < 0)
+									ksm_rmap_items++;
+
+								rmap_item->is_huge_page = 0;
+								ksm_scan.address += PAGE_SIZE;
+								adaptive_sleep_interval = ksm_thread_sleep_millisecs;
+								break;
+						}
+					} else {
+						if (rmap_item->is_huge_page < 0)
+							ksm_rmap_items++;
+
+						rmap_item->is_huge_page = 0;
+						ksm_scan.address += PAGE_SIZE;
+					}
 				} else
 					put_page(*page);
+
 				up_read(&mm->mmap_sem);
 				return rmap_item;
 			}
+skip:
 			put_page(*page);
 			ksm_scan.address += PAGE_SIZE;
 			cond_resched();
@@ -1701,6 +2369,7 @@
 		if (!rmap_item)
 			return;
 		cmp_and_merge_page(page, rmap_item);
+		//trace_printk("%s: page count %d\n", __func__, page->_count.counter);
 		put_page(page);
 	}
 }
@@ -1726,7 +2395,7 @@
 
 		if (ksmd_should_run()) {
 			schedule_timeout_interruptible(
-				msecs_to_jiffies(ksm_thread_sleep_millisecs));
+				msecs_to_jiffies(adaptive_sleep_interval));
 		} else {
 			wait_event_freezable(ksm_thread_wait,
 				ksmd_should_run() || kthread_should_stop());
@@ -2001,7 +2670,7 @@
 		while (node) {
 			stable_node = rb_entry(node, struct stable_node, node);
 			if (stable_node->kpfn >= start_pfn &&
-			    stable_node->kpfn < end_pfn) {
+					stable_node->kpfn < end_pfn) {
 				/*
 				 * Don't get_ksm_page, page has already gone:
 				 * which is why we keep kpfn instead of page*
@@ -2012,7 +2681,24 @@
 				node = rb_next(node);
 			cond_resched();
 		}
+
+		node = rb_first(root_stable_tree_huge + nid);
+		while (node) {
+			stable_node = rb_entry(node, struct stable_node, node);
+			if (stable_node->kpfn >= start_pfn &&
+					stable_node->kpfn < end_pfn) {
+				/*
+				 * Don't get_ksm_page, page has already gone:
+				 * which is why we keep kpfn instead of page*
+				 */
+				remove_node_from_stable_tree(stable_node);
+				node = rb_first(root_stable_tree_huge + nid);
+			} else
+				node = rb_next(node);
+			cond_resched();
+		}
 	}
+
 	list_for_each_safe(this, next, &migrate_nodes) {
 		stable_node = list_entry(this, struct stable_node, list);
 		if (stable_node->kpfn >= start_pfn &&
@@ -2070,6 +2756,107 @@
 }
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
+static inline int is_zero_page(struct page *page) 
+{
+	unsigned int count = 0;
+	int ret = 0;
+	void *addr;
+	char *mem;
+
+	if (PageTransCompound(page))
+		count = HPAGE_PMD_SIZE;
+	else
+		count = PAGE_SIZE;
+
+	addr = kmap_atomic(page);
+	mem = addr;
+
+	while (count--) {
+		if (*mem != '\0') {
+			ret = -1;
+			break;
+		}
+		mem++;
+	}
+
+	kunmap_atomic(addr);
+	return ret;
+}
+
+static inline int check_stable_tree(struct rb_root *tree, 
+		unsigned long *count, unsigned long *zero_pages)
+{
+	struct stable_node *stable_node;
+	struct rb_node *node;
+	struct hlist_node *iter, *tmp;	
+	int nid;
+
+	for (nid = 0, *count = 0, *zero_pages = 0; nid < ksm_nr_node_ids; nid++) {
+		node = rb_first(tree + nid);
+		while (node) {
+			stable_node = rb_entry(node, struct stable_node, node);
+			/* print necessary info from stable_node 
+			ksm_printk("pfn %lx hugepage %d\n", 
+					stable_node->kpfn, stable_node->is_huge_kpage);
+			*/
+
+			hlist_for_each_safe(iter, tmp, &stable_node->hlist) {
+				if (is_zero_page(pfn_to_page(stable_node->kpfn))) 
+					(*zero_pages)++;
+				(*count)++;
+			}
+
+			node = rb_next(node);
+		}
+	}
+
+	return 0;
+}
+
+static inline int check_unstable_tree(struct rb_root *tree,
+		unsigned long *count)
+{
+	int nid;
+	struct rb_node *node;
+	struct rmap_item *tree_rmap_item;
+
+	for (nid = 0, *count = 0; nid < ksm_nr_node_ids; nid++) {
+		node = rb_first(tree + nid);
+		while (node) {
+			tree_rmap_item = rb_entry(node, struct rmap_item, node);
+			/* print necessary info from rmap_item */	
+			node = rb_next(node);
+			(*count)++;
+		}
+	}
+
+	return 0;
+}
+
+/* functions for debugging */
+static int check_stable_unstable_tree(void)
+{
+	unsigned long count;
+	unsigned long zero_pages;
+
+
+	check_stable_tree(root_stable_tree, &count, &zero_pages);
+	trace_printk("stable_tree:count = %lu zero_pages = %lu\n", 
+			count, zero_pages);
+
+	check_stable_tree(root_stable_tree_huge, &count, &zero_pages);
+	trace_printk("stable_tree_huge:count = %lu zero_pages = %lu\n", 
+			count, zero_pages);
+
+	check_unstable_tree(root_unstable_tree, &count);
+	trace_printk("unstable_tree:count = %lu\n", count);
+
+	check_unstable_tree(root_unstable_tree_huge, &count);
+	trace_printk("unstable_tree_huge:count = %lu\n", count);
+
+	return 0;
+}
+
 #ifdef CONFIG_SYSFS
 /*
  * This all compiles without CONFIG_SYSFS, but is a waste of space.
@@ -2104,6 +2891,52 @@
 }
 KSM_ATTR(sleep_millisecs);
 
+static ssize_t sleep_scaling_show(struct kobject *kobj,
+                    struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%u\n", sleep_scaling_factor);
+}
+
+static ssize_t sleep_scaling_store(struct kobject *kobj,
+                     struct kobj_attribute *attr,
+                     const char *buf, size_t count)
+{
+    unsigned long scaling;
+    int err;
+
+    err = kstrtoul(buf, 10, &scaling);
+    if (err || scaling > UINT_MAX)
+        return -EINVAL;
+
+    sleep_scaling_factor = scaling;
+
+    return count;
+}
+KSM_ATTR(sleep_scaling);
+
+static ssize_t preserve_factor_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hpage_preserve);
+}
+
+static ssize_t preserve_factor_store(struct kobject *kobj,
+		struct kobj_attribute *attr,
+		const char *buf, size_t count)
+{
+	long param;
+	int err;
+
+	err = kstrtol(buf, 10, &param);
+	if (err || param > 100 || param < 0)
+		return -EINVAL;
+
+	hpage_preserve = param;
+
+	return count;
+}
+KSM_ATTR(preserve_factor);
+
 static ssize_t pages_to_scan_show(struct kobject *kobj,
 				  struct kobj_attribute *attr, char *buf)
 {
@@ -2238,14 +3071,51 @@
 {
 	return sprintf(buf, "%lu\n", ksm_pages_shared);
 }
-KSM_ATTR_RO(pages_shared);
+
+static ssize_t pages_shared_store(struct kobject *kobj, 
+		struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int err;
+	unsigned long pages_shared;
+
+	err = kstrtoul(buf, 10, &pages_shared);
+	if (err || pages_shared > UINT_MAX)
+		return -EINVAL;
+
+	ksm_pages_shared = pages_shared;
+
+	return count;
+}
+KSM_ATTR(pages_shared);
 
 static ssize_t pages_sharing_show(struct kobject *kobj,
 				  struct kobj_attribute *attr, char *buf)
 {
 	return sprintf(buf, "%lu\n", ksm_pages_sharing);
 }
-KSM_ATTR_RO(pages_sharing);
+static ssize_t pages_sharing_store(struct kobject *kobj, 
+		struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int err;
+	unsigned long pages_sharing;
+
+	err = kstrtoul(buf, 10, &pages_sharing);
+	if (err || pages_sharing > UINT_MAX)
+		return -EINVAL;
+
+	ksm_pages_sharing = pages_sharing;
+	ksm_pages_sharing_huge = pages_sharing;
+
+	return count;
+}
+KSM_ATTR(pages_sharing);
+
+static ssize_t pages_sharing_huge_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", ksm_pages_sharing_huge);
+}
+KSM_ATTR_RO(pages_sharing_huge);
 
 static ssize_t pages_unshared_show(struct kobject *kobj,
 				   struct kobj_attribute *attr, char *buf)
@@ -2271,6 +3141,41 @@
 }
 KSM_ATTR_RO(pages_volatile);
 
+static ssize_t merge_hugepage_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buf)
+{
+	switch(ksm_merge_hugepage) {
+		case NO_HPAGE_MERGE:
+			return sprintf(buf, "NO_HPAGE_MERGE\n");
+		case HPAGE_MERGE_SPLIT:
+			return sprintf(buf, "HPAGE_SPLIT\n");
+		case HPAGE_MERGE_NO_SPLIT:
+			return sprintf(buf, "HPAGE_NO_SPLIT\n");
+		case HPAGE_MERGE_PRESERVE:
+			return sprintf(buf, "HPAGE_PERSERVE\n");
+		case HPAGE_MERGE_FREQUENCY:
+			return sprintf(buf, "HPAGE_FREQUENCY\n");
+		default:
+			return sprintf(buf, "%u\n", ksm_merge_hugepage);
+	}
+}
+
+static ssize_t merge_hugepage_store(struct kobject *kobj,
+		struct kobj_attribute *attr,
+		const char *buf, size_t count)
+{
+	int err;
+	unsigned int flag;
+	err = kstrtou32(buf, 10, &flag);
+	if (err || flag > HPAGE_MERGE_MAX)
+		return -EINVAL;
+
+	ksm_merge_hugepage = flag;
+
+	return count;
+}
+KSM_ATTR(merge_hugepage);
+
 static ssize_t full_scans_show(struct kobject *kobj,
 			       struct kobj_attribute *attr, char *buf)
 {
@@ -2278,15 +3183,62 @@
 }
 KSM_ATTR_RO(full_scans);
 
+/* used for debugging */
+static ssize_t debug_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "debugging command:\n"
+			"1: check stable tree and unstable tree\n"
+			);
+}
+
+/* ksm_debug:
+ * 1 - print stable tree and unstable tree
+ * 2 - print case where KSMd fail to merge hugepage 
+ *	   due to the partial matching
+ * 3 - traverse stable tree and count the case where sharing share is 
+ *     zero page
+ */
+
+static ssize_t debug_store(struct kobject *kobj,
+		struct kobj_attribute *attr,
+		const char *buf, size_t count)
+{
+	int err;
+	unsigned int flag;
+	err = kstrtou32(buf, 10, &flag);
+
+	if (err || flag > HPAGE_MERGE_MAX)
+		return -EINVAL;
+
+	ksm_debug = flag;
+
+	switch(flag) {
+		case 1:
+			check_stable_unstable_tree();
+			break;
+		default:
+			break;
+	}
+
+	return count;
+}
+KSM_ATTR(debug);
+
 static struct attribute *ksm_attrs[] = {
 	&sleep_millisecs_attr.attr,
+	&sleep_scaling_attr.attr,
 	&pages_to_scan_attr.attr,
 	&run_attr.attr,
 	&pages_shared_attr.attr,
 	&pages_sharing_attr.attr,
+	&pages_sharing_huge_attr.attr,
 	&pages_unshared_attr.attr,
 	&pages_volatile_attr.attr,
 	&full_scans_attr.attr,
+	&merge_hugepage_attr.attr,
+	&debug_attr.attr,
+	&preserve_factor_attr.attr,
 #ifdef CONFIG_NUMA
 	&merge_across_nodes_attr.attr,
 #endif
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/madvise.c linux-4.3-osa/mm/madvise.c
--- linux-4.3-org/mm/madvise.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/madvise.c	2016-11-01 22:57:37.021139208 -0500
@@ -21,6 +21,10 @@
 #include <linux/swap.h>
 #include <linux/swapops.h>
 
+#ifdef CONFIG_OSA
+#include <osa/osa.h>
+#endif
+
 /*
  * Any behaviour which results in changes to the vma->vm_flags needs to
  * take mmap_sem for writing. Others, which simply traverse vmas, need
@@ -370,16 +374,23 @@
 }
 #endif
 
-static long
-madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,
+long madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,
 		unsigned long start, unsigned long end, int behavior)
 {
 	switch (behavior) {
 	case MADV_REMOVE:
+#ifdef CONFIG_OSA
+		/* TODO: consider the case when madvise fails */
+		osa_clear_poplmap_range(vma->vm_mm, start, end);
+#endif
 		return madvise_remove(vma, prev, start, end);
 	case MADV_WILLNEED:
 		return madvise_willneed(vma, prev, start, end);
 	case MADV_DONTNEED:
+#ifdef CONFIG_OSA
+		/* TODO: consider the case when madvise fails */
+		osa_clear_poplmap_range(vma->vm_mm, start, end);
+#endif
 		return madvise_dontneed(vma, prev, start, end);
 	default:
 		return madvise_behavior(vma, prev, start, end, behavior);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/Makefile linux-4.3-osa/mm/Makefile
--- linux-4.3-org/mm/Makefile	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/Makefile	2016-11-02 00:20:32.149326162 -0500
@@ -81,3 +81,15 @@
 obj-$(CONFIG_USERFAULTFD) += userfaultfd.o
 obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o
 obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o
+
+#for debugging
+#CFLAGS_ksm.o = -Og
+#CFLAGS_huge_memory.o = -Og
+#CFLAGS_memory.o = -Og
+#CFLAGS_migrate.o = -Og
+#CFLAGS_page_alloc.o = -Og
+#CFLAGS_vmscan.o = -Og
+#CFLAGS_slub.o = -Og
+#CFLAGS_slab_common.o = -Og
+#CFLAGS_compaction.o = -Og
+#CFLAGS_swap.o = -Og
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/memcontrol.c linux-4.3-osa/mm/memcontrol.c
--- linux-4.3-org/mm/memcontrol.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/memcontrol.c	2016-03-04 19:57:46.297958892 -0600
@@ -257,6 +257,11 @@
 	return &container_of(vmpr, struct mem_cgroup, vmpressure)->css;
 }
 
+struct vmpressure *css_to_vmpressure(struct cgroup_subsys_state *css)
+{
+	return &mem_cgroup_from_css(css)->vmpressure;
+}
+
 static inline bool mem_cgroup_is_root(struct mem_cgroup *memcg)
 {
 	return (memcg == root_mem_cgroup);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/memory.c linux-4.3-osa/mm/memory.c
--- linux-4.3-org/mm/memory.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/memory.c	2016-11-01 22:58:25.489141029 -0500
@@ -72,6 +72,11 @@
 
 #include "internal.h"
 
+#ifdef CONFIG_OSA
+#include <osa/osa.h>
+#include <linux/khugepaged.h>
+#endif
+
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 #warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.
 #endif
@@ -131,6 +136,83 @@
 }
 core_initcall(init_zero_pfn);
 
+#ifdef CONFIG_OSA
+struct kmem_cache *osa_poplmap_node_cachep;
+struct kmem_cache *osa_work_node_cachep;
+struct kmem_cache *osa_aggregate_node_cachep;
+
+void __init osa_kmem_cache_init(void)
+{
+	osa_poplmap_node_cachep = kmem_cache_create("osa_poplmap_node", 
+            sizeof(popl_node_t), 0,
+			SLAB_PANIC, NULL);
+
+	osa_aggregate_node_cachep = kmem_cache_create("osa_aggregate_node", 
+            sizeof(aggr_node_t), 0,
+			SLAB_PANIC, NULL);
+
+	osa_work_node_cachep = kmem_cache_create("osa_work_node", 
+            sizeof(work_node_t), 0,
+			SLAB_PANIC, NULL);
+
+    INIT_LIST_HEAD(&hugepage_worklist);
+    spin_lock_init(&worklist_lock);
+}
+
+popl_node_t *osa_poplmap_node_alloc(void)
+{
+    popl_node_t *node;
+	node  = kmem_cache_zalloc(osa_poplmap_node_cachep, GFP_ATOMIC);
+	if (!node)
+		return NULL;
+
+	return node;
+}
+
+void osa_poplmap_node_free(popl_node_t *node)
+{
+	kmem_cache_free(osa_poplmap_node_cachep, node);
+}
+
+work_node_t *osa_work_node_alloc(void)
+{
+    work_node_t *node;
+	node  = kmem_cache_alloc(osa_work_node_cachep, GFP_ATOMIC);
+	if (!node)
+		return NULL;
+
+	return node;
+}
+
+void osa_work_node_free(work_node_t *node)
+{
+	kmem_cache_free(osa_work_node_cachep, node);
+}
+
+void osa_clear_poplmap_range(struct mm_struct *mm, 
+		unsigned long start, unsigned long end) 
+{
+	unsigned long i;
+	unsigned int pos = 0;
+	unsigned long haddr;
+
+	popl_node_t *popl_node = NULL;
+	for (i = start; i < end; i += PAGE_SIZE) {
+		haddr = i & HPAGE_PMD_MASK;
+		pos = i & (HPAGE_PMD_SIZE - 1);
+		pos = pos >> PAGE_SHIFT;
+
+		popl_node = osa_popl_node_lookup(mm, 
+				HPAGE_ALIGN_FLOOR(i));
+
+		if (popl_node) {
+			bitmap_clear(popl_node->popl_bitmap, pos, 1);
+			popl_node->committed = 0;
+		}
+	}
+}
+
+#endif
 
 #if defined(SPLIT_RSS_COUNTING)
 
@@ -2059,7 +2141,7 @@
  */
 static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, pte_t *page_table, pmd_t *pmd,
-			pte_t orig_pte, struct page *old_page)
+			pte_t orig_pte, struct page *old_page, unsigned int flags)
 {
 	struct page *new_page = NULL;
 	spinlock_t *ptl = NULL;
@@ -2106,6 +2188,7 @@
 		flush_cache_page(vma, address, pte_pfn(orig_pte));
 		entry = mk_pte(new_page, vma->vm_page_prot);
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+
 		/*
 		 * Clear the pte entry and flush it first, before updating the
 		 * pte with the new entry. This will avoid a race condition
@@ -2284,7 +2367,7 @@
  */
 static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		unsigned long address, pte_t *page_table, pmd_t *pmd,
-		spinlock_t *ptl, pte_t orig_pte)
+		spinlock_t *ptl, pte_t orig_pte, unsigned int flags)
 	__releases(ptl)
 {
 	struct page *old_page;
@@ -2305,7 +2388,7 @@
 
 		pte_unmap_unlock(page_table, ptl);
 		return wp_page_copy(mm, vma, address, page_table, pmd,
-				    orig_pte, old_page);
+				    orig_pte, old_page, flags);
 	}
 
 	/*
@@ -2352,7 +2435,7 @@
 
 	pte_unmap_unlock(page_table, ptl);
 	return wp_page_copy(mm, vma, address, page_table, pmd,
-			    orig_pte, old_page);
+			    orig_pte, old_page, flags);
 }
 
 static void unmap_mapping_range_vma(struct vm_area_struct *vma,
@@ -2567,6 +2650,7 @@
 	flush_icache_page(vma, page);
 	if (pte_swp_soft_dirty(orig_pte))
 		pte = pte_mksoft_dirty(pte);
+
 	set_pte_at(mm, address, page_table, pte);
 	if (page == swapcache) {
 		do_page_add_anon_rmap(page, vma, address, exclusive);
@@ -2595,7 +2679,7 @@
 	}
 
 	if (flags & FAULT_FLAG_WRITE) {
-		ret |= do_wp_page(mm, vma, address, page_table, pmd, ptl, pte);
+		ret |= do_wp_page(mm, vma, address, page_table, pmd, ptl, pte,flags);
 		if (ret & VM_FAULT_ERROR)
 			ret &= VM_FAULT_ERROR;
 		goto out;
@@ -2668,6 +2752,14 @@
 	struct page *page;
 	spinlock_t *ptl;
 	pte_t entry;
+#ifdef CONFIG_OSA
+    popl_node_t *popl_node = NULL;
+    work_node_t *work_node = NULL;
+    unsigned int pos = 0;
+    unsigned int weight = 0;
+    unsigned long haddr;
+	unsigned int _util_threshold = 0;
+#endif
 
 	pte_unmap(page_table);
 
@@ -2734,10 +2826,70 @@
 	mem_cgroup_commit_charge(page, memcg, false);
 	lru_cache_add_active_or_unevictable(page, vma);
 setpte:
+
+#ifdef CONFIG_OSA
+    haddr = address & HPAGE_PMD_MASK;
+    /* check eligibility to be a hugepage. 
+     * refer to do_huge_pmd_anonymous_page */
+    if (transparent_hugepage_enabled(vma) &&
+        vma_is_anonymous(vma) &&
+        !(haddr < vma->vm_start || haddr + HPAGE_PMD_SIZE > vma->vm_end) &&
+		//anon_vma_prepare is might_sleep().
+        //!(unlikely(anon_vma_prepare(vma))) &&
+        !(unlikely(khugepaged_enter(vma, vma->vm_flags))) 
+        ) {
+        pos = address & (HPAGE_PMD_SIZE - 1);
+        pos = pos >> PAGE_SHIFT;
+        BUG_ON(pos > 512);
+
+        // lookup table 
+		popl_node = osa_popl_node_lookup(mm, HPAGE_ALIGN_FLOOR(address));
+
+		if (!popl_node) {
+			popl_node = osa_poplmap_node_alloc();
+			bitmap_set(popl_node->popl_bitmap, pos, 1);
+			/* TODO: handle the case when insert fails */
+
+			spin_lock(&osa_poplmap_lock);
+			osa_popl_node_insert(mm, 
+					HPAGE_ALIGN_FLOOR(address), popl_node);
+			spin_unlock(&osa_poplmap_lock);
+		} else {
+            /* TODO: protect popl_node by a lock */
+			bitmap_set(popl_node->popl_bitmap, pos, 1);
+
+            weight = bitmap_weight(popl_node->popl_bitmap, 512);
+			_util_threshold = (util_threshold * 512) / 100;
+            if (deferred_mode > 0 && 
+				weight >= _util_threshold && !popl_node->committed) {
+
+                popl_node->committed = 1;
+                work_node = osa_work_node_alloc();
+
+				BUG_ON(weight > 512);
+				trace_printk("add worklist: %lx %u\n", 
+						page_to_pfn(page), 
+						bitmap_weight(popl_node->popl_bitmap, 512));
+
+                work_node->mm = mm;
+                work_node->address = HPAGE_ALIGN_FLOOR(address);
+
+                /* TODO: prevent concurrent insert */
+                spin_lock(&worklist_lock);
+                list_add_tail(&work_node->list, 
+                        &hugepage_worklist);
+                spin_unlock(&worklist_lock);
+
+                wake_up_interruptible(&khugepaged_wait);
+            }
+        }
+    }
+#endif
 	set_pte_at(mm, address, page_table, entry);
 
 	/* No need to invalidate - it was non-present before */
 	update_mmu_cache(vma, address, page_table);
+
 unlock:
 	pte_unmap_unlock(page_table, ptl);
 	return 0;
@@ -3078,6 +3230,7 @@
 		page_cache_release(fault_page);
 		return ret;
 	}
+
 	do_set_pte(vma, address, fault_page, pte, true, false);
 	pte_unmap_unlock(pte, ptl);
 
@@ -3247,7 +3400,7 @@
 			unsigned int flags)
 {
 	if (vma_is_anonymous(vma))
-		return do_huge_pmd_wp_page(mm, vma, address, pmd, orig_pmd);
+		return do_huge_pmd_wp_page(mm, vma, address, pmd, orig_pmd, flags);
 	if (vma->vm_ops->pmd_fault)
 		return vma->vm_ops->pmd_fault(vma, address, pmd, flags);
 	return VM_FAULT_FALLBACK;
@@ -3309,7 +3462,7 @@
 	if (flags & FAULT_FLAG_WRITE) {
 		if (!pte_write(entry))
 			return do_wp_page(mm, vma, address,
-					pte, pmd, ptl, entry);
+					pte, pmd, ptl, entry,flags);
 		entry = pte_mkdirty(entry);
 	}
 	entry = pte_mkyoung(entry);
@@ -3351,14 +3504,21 @@
 	pud = pud_alloc(mm, pgd, address);
 	if (!pud)
 		return VM_FAULT_OOM;
+
+	pmd = pmd_offset(pud,address);
 	pmd = pmd_alloc(mm, pud, address);
 	if (!pmd)
 		return VM_FAULT_OOM;
 	if (pmd_none(*pmd) && transparent_hugepage_enabled(vma)) {
-		int ret = create_huge_pmd(mm, vma, address, pmd, flags);
-		if (!(ret & VM_FAULT_FALLBACK))
-			return ret;
+        if (!deferred_mode) {
+            int ret = create_huge_pmd(mm, vma, address, pmd, flags);
+            if (!(ret & VM_FAULT_FALLBACK))
+                return ret;
+        }
+        //else fall-through to handle_pte_fault
 	} else {
+        /* Copy-on-write path for hugepage. 
+         * e.g., access huge page after fork */
 		pmd_t orig_pmd = *pmd;
 		int ret;
 
@@ -3443,14 +3603,14 @@
 
 	if (flags & FAULT_FLAG_USER) {
 		mem_cgroup_oom_disable();
-                /*
-                 * The task may have entered a memcg OOM situation but
-                 * if the allocation error was handled gracefully (no
-                 * VM_FAULT_OOM), there is no need to kill anything.
-                 * Just clean up the OOM state peacefully.
-                 */
-                if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
-                        mem_cgroup_oom_synchronize(false);
+		/*
+		 * The task may have entered a memcg OOM situation but
+		 * if the allocation error was handled gracefully (no
+		 * VM_FAULT_OOM), there is no need to kill anything.
+		 * Just clean up the OOM state peacefully.
+		 */
+		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
+			mem_cgroup_oom_synchronize(false);
 	}
 
 	return ret;
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/migrate.c linux-4.3-osa/mm/migrate.c
--- linux-4.3-org/mm/migrate.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/migrate.c	2016-09-20 00:21:29.166619531 -0500
@@ -971,8 +971,9 @@
 			put_page(page);
 			if (!test_set_page_hwpoison(page))
 				num_poisoned_pages_inc();
-		} else
+		} else {
 			putback_lru_page(page);
+		}
 	}
 
 	/*
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/mmap.c linux-4.3-osa/mm/mmap.c
--- linux-4.3-org/mm/mmap.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/mmap.c	2016-09-20 00:21:29.170619531 -0500
@@ -50,6 +50,10 @@
 
 #include "internal.h"
 
+#ifdef CONFIG_OSA
+#include <osa/osa.h>
+#endif
+
 #ifndef arch_mmap_check
 #define arch_mmap_check(addr, len, flags)	(0)
 #endif
@@ -115,6 +119,9 @@
 int sysctl_max_map_count __read_mostly = DEFAULT_MAX_MAP_COUNT;
 unsigned long sysctl_user_reserve_kbytes __read_mostly = 1UL << 17; /* 128MB */
 unsigned long sysctl_admin_reserve_kbytes __read_mostly = 1UL << 13; /* 8MB */
+
+#define HPAGE_ALIGN_CEIL(x) HPAGE_ALIGN(x)
+
 /*
  * Make sure vm_committed_as in one cacheline and not cacheline shared with
  * other variables. It can be updated by several CPUs frequently.
@@ -1298,6 +1305,8 @@
 	if (mm->map_count > sysctl_max_map_count)
 		return -ENOMEM;
 
+	/* TODO: automatic MAP_HPAGE flag setting? */
+
 	/* Obtain the address to map to. we verify (or select) it and ensure
 	 * that it represents a valid section of the address space.
 	 */
@@ -1716,6 +1725,8 @@
 	struct vm_area_struct *vma;
 	unsigned long length, low_limit, high_limit, gap_start, gap_end;
 
+	trace_printk("not fully verified\n");
+
 	/* Adjust search length to account for worst case alignment overhead */
 	length = info->length + info->align_mask;
 	if (length < info->length)
@@ -1724,11 +1735,16 @@
 	/* Adjust search limits by the desired length */
 	if (info->high_limit < length)
 		return -ENOMEM;
+
 	high_limit = info->high_limit - length;
 
 	if (info->low_limit > high_limit)
 		return -ENOMEM;
-	low_limit = info->low_limit + length;
+
+	if (info->map_flags & MAP_HPAGE)
+		low_limit = HPAGE_ALIGN_CEIL(info->low_limit) + length;
+	else
+		low_limit = info->low_limit + length;
 
 	/* Check if rbtree root looks promising */
 	if (RB_EMPTY_ROOT(&mm->mm_rb))
@@ -1751,11 +1767,17 @@
 		}
 
 		gap_start = vma->vm_prev ? vma->vm_prev->vm_end : 0;
+
+		if (info->map_flags & MAP_HPAGE) 
+			gap_start = HPAGE_ALIGN_CEIL(gap_start);
+
 check_current:
 		/* Check if current node has a suitable gap */
 		if (gap_start > high_limit)
 			return -ENOMEM;
-		if (gap_end >= low_limit && gap_end - gap_start >= length)
+		if (gap_end >= low_limit && 
+				gap_end > gap_start &&
+				gap_end - gap_start >= length)
 			goto found;
 
 		/* Visit right subtree if it looks promising */
@@ -1778,7 +1800,12 @@
 				       struct vm_area_struct, vm_rb);
 			if (prev == vma->vm_rb.rb_left) {
 				gap_start = vma->vm_prev->vm_end;
+				if (info->map_flags & MAP_HPAGE) 
+					gap_start = HPAGE_ALIGN_CEIL(gap_start);
+
 				gap_end = vma->vm_start;
+				if (info->map_flags & MAP_HPAGE) 
+					gap_end = HPAGE_ALIGN_FLOOR(gap_end);
 				goto check_current;
 			}
 		}
@@ -1820,18 +1847,26 @@
 	 * See implementation comment at top of unmapped_area().
 	 */
 	gap_end = info->high_limit;
+	if (info->map_flags & MAP_HPAGE) 
+		gap_end = HPAGE_ALIGN_FLOOR(gap_end);
+
 	if (gap_end < length)
 		return -ENOMEM;
 	high_limit = gap_end - length;
 
 	if (info->low_limit > high_limit)
 		return -ENOMEM;
-	low_limit = info->low_limit + length;
+
+	if (info->map_flags & MAP_HPAGE) 
+		low_limit = HPAGE_ALIGN_CEIL(info->low_limit) + length;
+	else
+		low_limit = info->low_limit + length;
 
 	/* Check highest gap, which does not precede any rbtree node */
 	gap_start = mm->highest_vm_end;
-	if (gap_start <= high_limit)
+	if (gap_start <= high_limit) {
 		goto found_highest;
+	}
 
 	/* Check if rbtree root looks promising */
 	if (RB_EMPTY_ROOT(&mm->mm_rb))
@@ -1843,23 +1878,39 @@
 	while (true) {
 		/* Visit right subtree if it looks promising */
 		gap_start = vma->vm_prev ? vma->vm_prev->vm_end : 0;
+		if (info->map_flags & MAP_HPAGE) 
+			gap_start = HPAGE_ALIGN_CEIL(gap_start);
+
 		if (gap_start <= high_limit && vma->vm_rb.rb_right) {
 			struct vm_area_struct *right =
 				rb_entry(vma->vm_rb.rb_right,
 					 struct vm_area_struct, vm_rb);
-			if (right->rb_subtree_gap >= length) {
-				vma = right;
-				continue;
+			if (info->map_flags & MAP_HPAGE) {
+				if (gap_start + length <= right->vm_start) {
+					vma = right;
+					continue;
+				}
+			} else {
+				if (right->rb_subtree_gap >= length) {
+					vma = right;
+					continue;
+				}
 			}
 		}
 
 check_current:
 		/* Check if current node has a suitable gap */
 		gap_end = vma->vm_start;
+		if (info->map_flags & MAP_HPAGE) 
+			gap_end = HPAGE_ALIGN_FLOOR(gap_end);
+
 		if (gap_end < low_limit)
 			return -ENOMEM;
-		if (gap_start <= high_limit && gap_end - gap_start >= length)
+		if (gap_start <= high_limit && 
+				gap_end > gap_start &&
+				gap_end - gap_start >= length) {
 			goto found;
+		}
 
 		/* Visit left subtree if it looks promising */
 		if (vma->vm_rb.rb_left) {
@@ -1882,6 +1933,9 @@
 			if (prev == vma->vm_rb.rb_right) {
 				gap_start = vma->vm_prev ?
 					vma->vm_prev->vm_end : 0;
+
+				if (info->map_flags & MAP_HPAGE) 
+					gap_start = HPAGE_ALIGN_CEIL(gap_start);
 				goto check_current;
 			}
 		}
@@ -2613,6 +2667,10 @@
 	/* Fix up all other VM information */
 	remove_vma_list(mm, vma);
 
+#ifdef CONFIG_OSA
+	osa_clear_poplmap_range(mm, start, end);
+#endif
+
 	return 0;
 }
 
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/page_alloc.c linux-4.3-osa/mm/page_alloc.c
--- linux-4.3-org/mm/page_alloc.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/page_alloc.c	2016-11-01 00:24:52.638089238 -0500
@@ -68,6 +68,10 @@
 #include <asm/div64.h>
 #include "internal.h"
 
+#ifdef CONFIG_OSA
+#include <osa/osa.h>
+#endif
+
 /* prevent >1 _updater_ of zone percpu pageset ->high and ->batch fields */
 static DEFINE_MUTEX(pcp_batch_high_lock);
 #define MIN_PERCPU_PAGELIST_FRACTION	(8)
@@ -723,6 +727,9 @@
 		}
 	}
 
+	if (migratetype > MIGRATE_TYPES)
+		BUG();
+
 	list_add(&page->lru, &zone->free_area[order].free_list[migratetype]);
 out:
 	zone->free_area[order].nr_free++;
@@ -1377,6 +1384,10 @@
 	else
 		clear_page_pfmemalloc(page);
 
+#ifdef CONFIG_OSA
+	bitmap_clear(page->util_info.freq_bitmap, 0, FREQ_BITMAP_SIZE);
+	memset(page->util_info.frequency, 0, PRI_HISTORY_SIZE * sizeof(uint32_t));
+#endif
 	return 0;
 }
 
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/page_idle.c linux-4.3-osa/mm/page_idle.c
--- linux-4.3-org/mm/page_idle.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/page_idle.c	2016-09-20 00:21:29.170619531 -0500
@@ -27,7 +27,7 @@
  *
  * This function tries to get a user memory page by pfn as described above.
  */
-static struct page *page_idle_get_page(unsigned long pfn)
+struct page *page_idle_get_page(unsigned long pfn)
 {
 	struct page *page;
 	struct zone *zone;
@@ -50,7 +50,7 @@
 	return page;
 }
 
-static int page_idle_clear_pte_refs_one(struct page *page,
+int page_idle_clear_pte_refs_one(struct page *page,
 					struct vm_area_struct *vma,
 					unsigned long addr, void *arg)
 {
@@ -86,7 +86,7 @@
 	return SWAP_AGAIN;
 }
 
-static void page_idle_clear_pte_refs(struct page *page)
+void page_idle_clear_pte_refs(struct page *page)
 {
 	/*
 	 * Since rwc.arg is unused, rwc is effectively immutable, so we
@@ -102,7 +102,12 @@
 	    !page_rmapping(page))
 		return;
 
-	need_lock = !PageAnon(page) || PageKsm(page);
+	// does not support idle checking for ksm page for now.
+	if (PageKsm(page))
+		return;
+
+	//need_lock = !PageAnon(page) || PageKsm(page);
+	need_lock = !PageAnon(page);
 	if (need_lock && !trylock_page(page))
 		return;
 
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/slub.c linux-4.3-osa/mm/slub.c
--- linux-4.3-org/mm/slub.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/slub.c	2016-03-13 01:18:58.373839411 -0600
@@ -1403,6 +1403,7 @@
 	if (page_is_pfmemalloc(page))
 		SetPageSlabPfmemalloc(page);
 
+	/* (kernel) virtual address of the page */
 	start = page_address(page);
 
 	if (unlikely(s->flags & SLAB_POISON))
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/vmpressure.c linux-4.3-osa/mm/vmpressure.c
--- linux-4.3-org/mm/vmpressure.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/vmpressure.c	2016-03-04 19:57:46.309958892 -0600
@@ -131,8 +131,13 @@
 }
 
 struct vmpressure_event {
-	struct eventfd_ctx *efd;
+	union {
+		struct eventfd_ctx *efd;
+		void (*fn)(void *data, int level);
+	};
 	enum vmpressure_levels level;
+	void *data;
+	bool kernel_event;
 	struct list_head node;
 };
 
@@ -148,12 +153,15 @@
 	mutex_lock(&vmpr->events_lock);
 
 	list_for_each_entry(ev, &vmpr->events, node) {
-		if (level >= ev->level) {
+		if (ev->kernel_event) {
+			ev->fn(ev->data, level);
+		} else if (vmpr->notify_userspace && level >= ev->level) {
 			eventfd_signal(ev->efd, 1);
 			signalled = true;
 		}
 	}
 
+	vmpr->notify_userspace = false;
 	mutex_unlock(&vmpr->events_lock);
 
 	return signalled;
@@ -225,7 +233,7 @@
 	 * we account it too.
 	 */
 	if (!(gfp & (__GFP_HIGHMEM | __GFP_MOVABLE | __GFP_IO | __GFP_FS)))
-		return;
+		goto schedule;
 
 	/*
 	 * If we got here with no pages scanned, then that is an indicator
@@ -242,8 +250,15 @@
 	vmpr->scanned += scanned;
 	vmpr->reclaimed += reclaimed;
 	scanned = vmpr->scanned;
+	/*
+	 * If we didn't reach this point, only kernel events will be triggered.
+	 * It is the job of the worker thread to clean this up once the
+	 * notifications are all delivered.
+	 */
+	vmpr->notify_userspace = true;
 	spin_unlock(&vmpr->sr_lock);
 
+schedule:
 	if (scanned < vmpressure_win)
 		return;
 	schedule_work(&vmpr->work);
@@ -317,6 +332,43 @@
 
 	mutex_lock(&vmpr->events_lock);
 	list_add(&ev->node, &vmpr->events);
+	mutex_unlock(&vmpr->events_lock);
+
+	return 0;
+}
+
+/**
+ * vmpressure_register_kernel_event() - Register kernel-side notification
+ * @css:	css that is interested in vmpressure notifications
+ * @fn:		function to be called when pressure happens
+ *
+ * This function register in-kernel users interested in receiving notifications
+ * about pressure conditions. Pressure notifications will be triggered at the
+ * same time as userspace notifications (with no particular ordering relative
+ * to it).
+ *
+ * Pressure notifications are a alternative method to shrinkers and will serve
+ * well users that are interested in a one-shot notification, with a
+ * well-defined cgroup aware interface.
+ */
+int vmpressure_register_kernel_event(struct cgroup_subsys_state *css,
+				     void (*fn)(void *data, int level), void *data)
+{
+	struct vmpressure *vmpr;
+	struct vmpressure_event *ev;
+
+	vmpr = css ? css_to_vmpressure(css) : memcg_to_vmpressure(NULL);
+
+	ev = kzalloc(sizeof(*ev), GFP_KERNEL);
+	if (!ev)
+		return -ENOMEM;
+
+	ev->kernel_event = true;
+	ev->data = data;
+	ev->fn = fn;
+
+	mutex_lock(&vmpr->events_lock);
+	list_add(&ev->node, &vmpr->events);
 	mutex_unlock(&vmpr->events_lock);
 
 	return 0;
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/vmscan.c linux-4.3-osa/mm/vmscan.c
--- linux-4.3-org/mm/vmscan.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/vmscan.c	2016-11-02 08:39:39.738451523 -0500
@@ -104,6 +104,11 @@
 
 	/* Number of pages freed so far during a call to shrink_zones() */
 	unsigned long nr_reclaimed;
+
+	/* Similar to nr_scanned, nr_reclaimed 
+	 * but counting 1 hugepage as 512 pages */
+	unsigned long nr_scanned_pg;
+	unsigned long nr_reclaimed_pg;
 };
 
 #define lru_to_page(_head) (list_entry((_head)->prev, struct page, lru))
@@ -212,7 +217,7 @@
 		zone_reclaimable_pages(zone) * 6;
 }
 
-static unsigned long get_lru_size(struct lruvec *lruvec, enum lru_list lru)
+unsigned long get_lru_size(struct lruvec *lruvec, enum lru_list lru)
 {
 	if (!mem_cgroup_disabled())
 		return mem_cgroup_get_lru_size(lruvec, lru);
@@ -874,6 +879,65 @@
 		mapping->a_ops->is_dirty_writeback(page, dirty, writeback);
 }
 
+struct osa_inst_rmap_arg 
+{
+	int page_owner;
+};
+
+static int osa_inst_page_owner_rmap(struct page *page,
+		struct vm_area_struct *vma, unsigned long address,
+		void *arg)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	struct osa_inst_rmap_arg *_rmap_arg = arg;
+
+	if (mm && rcu_dereference(mm->owner)) {
+		rcu_read_lock();
+		_rmap_arg->page_owner = rcu_dereference(mm->owner)->pid;
+		//trace_mm_vmscan_printk1("owner", _rmap_arg->page_owner);
+		rcu_read_unlock();
+	}
+
+	return 0;
+}
+
+int osa_inst_get_page_owner(struct page *page, 
+		int references)
+{
+	int owner = 0;
+	struct osa_inst_rmap_arg _rmap_arg = {
+		.page_owner = 0,
+	};
+
+	struct rmap_walk_control rwc = {
+		.rmap_one = osa_inst_page_owner_rmap,
+		.arg = (void *)&_rmap_arg,
+	};
+
+	if (PageAnon(page) && page_mapped(page) 
+			&& page_rmapping(page)
+			&& !PageSwapCache(page)) {
+		/* does not handle the case of multiple owner (shared page) */
+
+		rmap_walk(page, &rwc);	
+
+		/* references 0 - PAGEREF_ACTIVATE
+		 * references 3 - PAGEREF_RECLAIM */
+		/*
+		if (PageCompound(page)) 
+			trace_mm_vmscan_printk3("owner(dbg)", _rmap_arg.page_owner,  
+					"hugepage", PageCompound(page), "ref", references);
+		else 
+			trace_mm_vmscan_printk3("owner(dbg)", _rmap_arg.page_owner,  
+					"hugepage", 0, "ref", references);
+		*/
+
+		owner = _rmap_arg.page_owner;
+	}
+
+	return owner;
+}
+
 /*
  * shrink_page_list() returns the number of reclaimed pages
  */
@@ -897,6 +961,7 @@
 	unsigned long nr_reclaimed = 0;
 	unsigned long nr_writeback = 0;
 	unsigned long nr_immediate = 0;
+	unsigned int page_owner = 0, is_huge = 0;
 
 	cond_resched();
 
@@ -1030,10 +1095,23 @@
 		if (!force_reclaim)
 			references = page_check_references(page, sc);
 
+#ifdef CONFIG_OSA
+		page_owner = osa_inst_get_page_owner(page, references);
+#endif
+		is_huge = PageCompound(page);
+
 		switch (references) {
-		case PAGEREF_ACTIVATE:
-			goto activate_locked;
-		case PAGEREF_KEEP:
+		case PAGEREF_ACTIVATE:	
+#ifdef CONFIG_OSA
+		if (distance_divisor) {
+			//goto activate_locked; /* back to active list */
+		    goto keep_locked;       /* back to inactive list */
+        } else
+		    goto activate_locked; 
+#else
+		goto activate_locked; 
+#endif
+		case PAGEREF_KEEP:	
 			goto keep_locked;
 		case PAGEREF_RECLAIM:
 		case PAGEREF_RECLAIM_CLEAN:
@@ -1047,6 +1125,7 @@
 		if (PageAnon(page) && !PageSwapCache(page)) {
 			if (!(sc->gfp_mask & __GFP_IO))
 				goto keep_locked;
+			/* Hugepages are split by the add_to_swap */
 			if (!add_to_swap(page, page_list))
 				goto activate_locked;
 			may_enter_fs = 1;
@@ -1113,11 +1192,14 @@
 			case PAGE_ACTIVATE:
 				goto activate_locked;
 			case PAGE_SUCCESS:
-				if (PageWriteback(page))
+				sc->nr_reclaimed_pg += hpage_nr_pages(page); 
+				trace_mm_vmscan_shrink_page_list(1, page_owner,  
+						page_to_pfn(page), is_huge, references);
+
+				if (PageWriteback(page)) 
 					goto keep;
 				if (PageDirty(page))
 					goto keep;
-
 				/*
 				 * A synchronous write - probably a ramdisk.  Go
 				 * ahead and try to reclaim the page.
@@ -1158,9 +1240,9 @@
 				goto activate_locked;
 			if (!mapping && page_count(page) == 1) {
 				unlock_page(page);
-				if (put_page_testzero(page))
+				if (put_page_testzero(page)) {
 					goto free_it;
-				else {
+				} else {
 					/*
 					 * rare race with speculative reference.
 					 * the speculative reference will free
@@ -1169,6 +1251,10 @@
 					 * leave it off the LRU).
 					 */
 					nr_reclaimed++;
+					sc->nr_reclaimed_pg += hpage_nr_pages(page); 
+					trace_mm_vmscan_shrink_page_list(0, page_owner,  
+							page_to_pfn(page), is_huge, references);
+					//trace_mm_vmscan_printk1("nr_reclaimed", nr_reclaimed);
 					continue;
 				}
 			}
@@ -1187,6 +1273,10 @@
 		__clear_page_locked(page);
 free_it:
 		nr_reclaimed++;
+		sc->nr_reclaimed_pg += hpage_nr_pages(page); 
+		trace_mm_vmscan_shrink_page_list(0, page_owner,  
+				page_to_pfn(page), is_huge, references);
+		//trace_mm_vmscan_printk1("nr_reclaimed", nr_reclaimed);
 
 		/*
 		 * Is there need to periodically free_page_list? It would
@@ -1214,6 +1304,9 @@
 keep:
 		list_add(&page->lru, &ret_pages);
 		VM_BUG_ON_PAGE(PageLRU(page) || PageUnevictable(page), page);
+
+		trace_mm_vmscan_shrink_page_list(5, page_owner,  
+				page_to_pfn(page), is_huge, references);
 	}
 
 	mem_cgroup_uncharge_list(&free_pages);
@@ -1378,6 +1471,7 @@
 			mem_cgroup_update_lru_size(lruvec, lru, -nr_pages);
 			list_move(&page->lru, dst);
 			nr_taken += nr_pages;
+			sc->nr_scanned_pg += nr_pages;
 			break;
 
 		case -EBUSY:
@@ -1509,7 +1603,15 @@
 
 		SetPageLRU(page);
 		lru = page_lru(page);
+
+#ifdef CONFIG_OSA
+		if (PageTransCompound(page) && distance_divisor) {
+			add_page_to_lru_list_tail(page, lruvec, lru);
+		} else
+			add_page_to_lru_list(page, lruvec, lru);
+#else
 		add_page_to_lru_list(page, lruvec, lru);
+#endif
 
 		if (is_active_lru(lru)) {
 			int file = is_file_lru(lru);
@@ -1739,6 +1841,9 @@
 		list_move(&page->lru, &lruvec->lists[lru]);
 		pgmoved += nr_pages;
 
+		/* drop ref count of page, which was increased
+		 * in isolate_lru_pages. Usually, this testzero is true
+		 * when kswapd shrinks active list. */
 		if (put_page_testzero(page)) {
 			__ClearPageLRU(page);
 			__ClearPageActive(page);
@@ -2079,7 +2184,7 @@
 	 */
 
 	anon  = get_lru_size(lruvec, LRU_ACTIVE_ANON) +
-		get_lru_size(lruvec, LRU_INACTIVE_ANON);
+		get_lru_size(lruvec, LRU_INACTIVE_ANON); 
 	file  = get_lru_size(lruvec, LRU_ACTIVE_FILE) +
 		get_lru_size(lruvec, LRU_INACTIVE_FILE);
 
@@ -2216,11 +2321,24 @@
 	init_tlb_ubc();
 
 	blk_start_plug(&plug);
+
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
 					nr[LRU_INACTIVE_FILE]) {
 		unsigned long nr_anon, nr_file, percentage;
 		unsigned long nr_scanned;
 
+		/*
+		trace_mm_vmscan_lru_reclaim_stat(
+				0,
+				zone_idx(lruvec_zone(lruvec)),
+				nr_reclaimed,
+				get_lru_size(lruvec, LRU_ACTIVE_ANON) + 
+				get_lru_size(lruvec, LRU_INACTIVE_ANON),
+				get_lru_size(lruvec, LRU_ACTIVE_FILE) + 
+				get_lru_size(lruvec, LRU_INACTIVE_FILE),
+				lruvec->reclaim_stat);
+		*/
+
 		for_each_evictable_lru(lru) {
 			if (nr[lru]) {
 				nr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX);
@@ -2231,10 +2349,34 @@
 			}
 		}
 
+		/*
+		trace_mm_vmscan_lru_reclaim_stat(
+				1,
+				zone_idx(lruvec_zone(lruvec)),
+				nr_reclaimed,
+				get_lru_size(lruvec, LRU_ACTIVE_ANON) + 
+				get_lru_size(lruvec, LRU_INACTIVE_ANON),
+				get_lru_size(lruvec, LRU_ACTIVE_FILE) + 
+				get_lru_size(lruvec, LRU_INACTIVE_FILE),
+				lruvec->reclaim_stat);
+		*/
+
 		if (nr_reclaimed < nr_to_reclaim || scan_adjusted)
 			continue;
 
 		/*
+		trace_mm_vmscan_lru_reclaim_stat(
+				2,
+				zone_idx(lruvec_zone(lruvec)),
+				nr_reclaimed,
+				get_lru_size(lruvec, LRU_ACTIVE_ANON) + 
+				get_lru_size(lruvec, LRU_INACTIVE_ANON),
+				get_lru_size(lruvec, LRU_ACTIVE_FILE) + 
+				get_lru_size(lruvec, LRU_INACTIVE_FILE),
+				lruvec->reclaim_stat);
+		*/
+
+		/*
 		 * For kswapd and memcg, reclaim at least the number of pages
 		 * requested. Ensure that the anon and file LRUs are scanned
 		 * proportionally what was requested by get_scan_count(). We
@@ -2379,6 +2521,7 @@
 {
 	struct reclaim_state *reclaim_state = current->reclaim_state;
 	unsigned long nr_reclaimed, nr_scanned;
+	unsigned long nr_reclaimed_pg, nr_scanned_pg;
 	bool reclaimable = false;
 
 	do {
@@ -2393,6 +2536,11 @@
 		nr_reclaimed = sc->nr_reclaimed;
 		nr_scanned = sc->nr_scanned;
 
+		nr_reclaimed_pg = sc->nr_reclaimed_pg;
+		nr_scanned_pg = sc->nr_scanned_pg;
+
+		trace_mm_vmscan_reclaim_stat(0, nr_scanned_pg, nr_reclaimed_pg);
+
 		memcg = mem_cgroup_iter(root, NULL, &reclaim);
 		do {
 			unsigned long lru_pages;
@@ -2449,6 +2597,8 @@
 			reclaim_state->reclaimed_slab = 0;
 		}
 
+		trace_mm_vmscan_reclaim_stat(1, sc->nr_scanned_pg, sc->nr_reclaimed_pg);
+
 		vmpressure(sc->gfp_mask, sc->target_mem_cgroup,
 			   sc->nr_scanned - nr_scanned,
 			   sc->nr_reclaimed - nr_reclaimed);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/mm/vmstat.c linux-4.3-osa/mm/vmstat.c
--- linux-4.3-org/mm/vmstat.c	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/mm/vmstat.c	2016-03-04 19:57:46.309958892 -0600
@@ -595,11 +595,6 @@
 
 #ifdef CONFIG_COMPACTION
 
-struct contig_page_info {
-	unsigned long free_pages;
-	unsigned long free_blocks_total;
-	unsigned long free_blocks_suitable;
-};
 
 /*
  * Calculate the number of free pages in a zone, how many contiguous
@@ -609,7 +604,7 @@
  * migrated. Calculating that is possible, but expensive and can be
  * figured out from userspace
  */
-static void fill_contig_page_info(struct zone *zone,
+void fill_contig_page_info(struct zone *zone,
 				unsigned int suitable_order,
 				struct contig_page_info *info)
 {
@@ -618,6 +613,7 @@
 	info->free_pages = 0;
 	info->free_blocks_total = 0;
 	info->free_blocks_suitable = 0;
+    info->free_pages_suitable = 0;
 
 	for (order = 0; order < MAX_ORDER; order++) {
 		unsigned long blocks;
@@ -630,9 +626,11 @@
 		info->free_pages += blocks << order;
 
 		/* Count the suitable free blocks */
-		if (order >= suitable_order)
+		if (order >= suitable_order) {
 			info->free_blocks_suitable += blocks <<
 						(order - suitable_order);
+            info->free_pages_suitable += blocks << order;
+        }
 	}
 }
 
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/osa/hugepage_proc.c linux-4.3-osa/osa/hugepage_proc.c
--- linux-4.3-org/osa/hugepage_proc.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/osa/hugepage_proc.c	2016-11-01 00:24:48.730089091 -0500
@@ -0,0 +1,596 @@
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/tty.h>      
+#include <linux/console.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/kallsyms.h>
+#include <linux/mm.h>
+#include <linux/gfp.h>
+#include <linux/rmap.h>
+#include <asm/mman.h>
+#include <linux/huge_mm.h>
+#include <asm/current.h>
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+#include <osa/osa.h>
+//#include "common.h"
+#include "../fs/proc/internal.h" /* included from fs/proc/, check EXTRA_CFLAGS in Makefile */
+
+DEFINE_SPINLOCK(osa_hpage_list_lock);
+
+struct hpage_proc_private 
+{
+	struct inode *inode;
+	struct task_struct *task;
+	struct mm_struct *mm;
+	struct vm_area_struct *tail_vma;
+	//struct mempolicy *task_mempolicy;
+};
+
+static inline int is_vm_hugetlb_page(struct vm_area_struct *vma)
+{
+	return !!(vma->vm_flags & VM_HUGETLB);
+}
+/* ------------------------------------------- */
+
+static int osa_hpage_pmd_entry(pmd_t *pmd, unsigned long addr, 
+		unsigned long end, struct mm_walk *walk)
+{
+	struct seq_file *m;
+	m = (struct seq_file *)walk->private;
+
+	if (pmd_trans_huge(*pmd)) {
+		seq_printf(m, "%lx - %lx: %llx\n", 
+				addr, end, (pmd_val(*pmd) & __PHYSICAL_MASK) >> PAGE_SHIFT);
+	}
+
+	return 0;
+}
+
+static struct vm_area_struct * osa_hpage_next_vma(
+		struct hpage_proc_private *priv, struct vm_area_struct *vma)
+{
+	if (vma == priv->tail_vma)
+		return NULL;
+	return vma->vm_next ?: priv->tail_vma;
+}
+
+static void osa_hpage_cache_vma(struct seq_file *m, struct vm_area_struct *vma)
+{
+	if (m->count < m->size) /* vma is copied successfully */
+		m->version = osa_hpage_next_vma(m->private, vma) ? vma->vm_start : -1UL;
+}
+
+int osa_hpage_show(struct seq_file *m, void *v) 
+{
+	struct vm_area_struct *vma = v;
+	struct mm_walk _hpage_walker = {
+		.pmd_entry = osa_hpage_pmd_entry,
+		.mm = vma->vm_mm,
+		.private = m,
+	};
+
+	//seq_printf(m, "%lx - %lx\n", vma->vm_start, vma->vm_end);
+
+	if (vma->vm_mm && !is_vm_hugetlb_page(vma)) {
+		walk_page_range(vma->vm_start, vma->vm_end, &_hpage_walker);
+		return 0;
+	}
+
+	osa_hpage_cache_vma(m, vma);
+	return 0;
+}
+
+static void osa_hpage_vma_stop(struct hpage_proc_private *priv)
+{
+	struct mm_struct *mm = priv->mm;
+
+	//release_task_mempolicy(priv);
+	up_read(&mm->mmap_sem);
+	mmput(mm);
+}
+
+static void *_seq_start(struct seq_file *m, loff_t *ppos)
+{
+	struct hpage_proc_private *priv = m->private;
+	unsigned long last_addr = m->version;
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	unsigned int pos = *ppos;
+
+	/* See m_cache_vma(). Zero at the start or after lseek. */
+	if (last_addr == -1UL)
+		return NULL;
+
+	priv->task = get_proc_task(priv->inode);
+	if (!priv->task)
+		return ERR_PTR(-ESRCH);
+
+	mm = priv->mm;
+	if (!mm || !atomic_inc_not_zero(&mm->mm_users))
+		return NULL;
+
+	down_read(&mm->mmap_sem);
+	//hold_task_mempolicy(priv);
+	priv->tail_vma = get_gate_vma(mm);
+
+	if (last_addr) {
+		vma = find_vma(mm, last_addr);
+		if (vma && (vma = osa_hpage_next_vma(priv, vma)))
+			return vma;
+	}
+
+	m->version = 0;
+	if (pos < mm->map_count) {
+		for (vma = mm->mmap; pos; pos--) {
+			m->version = vma->vm_start;
+			vma = vma->vm_next;
+		}
+		return vma;
+	}
+
+	/* we do not bother to update m->version in this case */
+	if (pos == mm->map_count && priv->tail_vma)
+		return priv->tail_vma;
+
+	osa_hpage_vma_stop(priv);
+
+	return NULL;
+}
+static void *_seq_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct hpage_proc_private *priv = m->private;
+	struct vm_area_struct *next;
+
+	(*pos)++;
+	next = osa_hpage_next_vma(priv, v);
+	if (!next)
+		osa_hpage_vma_stop(priv);
+	return next;
+}
+
+static void _seq_stop(struct seq_file *m, void *v)
+{
+	struct hpage_proc_private *priv = m->private;
+
+	if (!IS_ERR_OR_NULL(v))
+		osa_hpage_vma_stop(priv);
+	if (priv->task) {
+		put_task_struct(priv->task);
+		priv->task = NULL;
+	}
+}
+
+static const struct seq_operations osa_hpage_seq_ops = {
+	.start = _seq_start,
+	.next = _seq_next,
+	.stop = _seq_stop,
+	.show = osa_hpage_show
+};
+
+static int osa_hpage_proc_open(struct inode *inode, struct file *file) 
+{
+	int err = 0;
+	struct hpage_proc_private *hpage_priv;
+	
+	hpage_priv = __seq_open_private(file, &osa_hpage_seq_ops,
+			sizeof(struct hpage_proc_private));
+
+	if (!hpage_priv)
+		return -ENOMEM;
+	
+	hpage_priv->inode = inode;
+	hpage_priv->task = get_proc_task(inode); /* requires internal.h */
+
+	if (!hpage_priv->task) {
+		err = -ESRCH;
+		seq_release_private(inode, file);
+		goto out;
+	}
+
+#define PTRACE_MODE_READ 0x01
+	//hpage_priv->mm = mm_access(hpage_priv->task, PTRACE_MODE_READ);
+	hpage_priv->mm = get_task_mm(hpage_priv->task);
+
+	if (IS_ERR_OR_NULL(hpage_priv->mm)) {
+		err = PTR_ERR(hpage_priv->mm);
+		seq_release_private(inode, file);
+		goto out;
+	} 
+
+out:
+	if (hpage_priv->task)
+		put_task_struct(hpage_priv->task);
+
+	return err;
+}
+
+static int osa_hpage_proc_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *seq = file->private_data;
+	struct hpage_proc_private *priv = seq->private;
+
+	if (priv->mm)
+		mmput(priv->mm);
+
+	return seq_release_private(inode, file);
+}
+
+struct file_operations osa_hpage_proc_operations = {
+	.owner = THIS_MODULE,
+	.open = osa_hpage_proc_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = osa_hpage_proc_release,
+};
+
+////////////////////////////////////////////////////////////////////////
+static ssize_t osa_hpage_madvise_proc_write(struct file *file, 
+		const char __user *buf, size_t len, loff_t *ppos)
+{
+	char write_buf[1];
+	struct task_struct *tsk = NULL;
+	struct vm_area_struct *vma, *prev = NULL;
+	struct mm_struct *mm;
+	int is_add = 0, rc = 0;
+	unsigned long flags;
+
+	copy_from_user(write_buf, buf, sizeof(char) * 1);
+
+	if (write_buf[0] == '0') {
+		is_add = 0;
+	}
+	else if(write_buf[0] == '1') {
+		is_add = 1;
+	}
+	else
+		return -EINVAL;
+
+	if (!file->f_path.dentry->d_inode)
+		return -ESRCH;
+
+	tsk = get_proc_task(file->f_path.dentry->d_inode);
+	if (!tsk)
+		return -ESRCH;
+
+	mm = get_task_mm(tsk);
+
+	vma = mm->mmap;
+
+	vma = find_vma_prev(mm, vma->vm_start, &prev);
+
+	while(vma != NULL) {
+		flags = vma->vm_flags;
+		if (is_add) {
+			rc = madvise_vma(vma, &prev, vma->vm_start, 
+					vma->vm_end, MADV_HUGEPAGE);
+			// for debugging
+			/*
+			madvise_vma(vma, &prev, vma->vm_start, 
+					vma->vm_end, MADV_MERGEABLE);
+			*/
+		} 
+		else {
+			rc = madvise_vma(vma, &prev, vma->vm_start, 
+					vma->vm_end, MADV_NOHUGEPAGE);
+			/*
+			madvise_vma(vma, &prev, vma->vm_start, 
+					vma->vm_end, MADV_UNMERGEABLE);
+			*/
+		}
+
+		if (rc) 
+			trace_printk("error to add vma %d\n", rc);
+
+		vma = vma->vm_next;
+	}
+
+	mmput(mm);
+	put_task_struct(tsk);
+
+	return len;
+}
+
+static int osa_hpage_madvise_proc_show(struct seq_file *m, void *v)
+{
+	seq_printf(m, "do nothing\n");
+	return 0;
+}
+
+static int osa_hpage_madvise_proc_open(struct inode *inode, struct file *file) 
+{
+	return single_open(file, osa_hpage_madvise_proc_show, inode);
+}
+
+static int osa_hpage_madvise_proc_release(struct inode *inode, struct file *file)
+{
+	return single_release(inode, file);
+}
+
+struct file_operations osa_hpage_madvise_operations = {
+	.owner = THIS_MODULE,
+	.open = osa_hpage_madvise_proc_open,
+	.read = seq_read,
+	.write = osa_hpage_madvise_proc_write,
+	.release = osa_hpage_madvise_proc_release,
+};
+
+////////////////////////////////////////////////////////////////////////
+static int osa_hpage_stats_proc_show(struct seq_file *m, void *v)
+{
+	struct file *file;
+	struct task_struct *tsk = NULL;
+	struct mm_struct *mm = NULL;
+	unsigned int total_hpage_count = 1;
+
+	if (!m->private)
+		return -ESRCH;
+
+	file = (struct file *)m->private;
+
+	tsk = get_proc_task(file->f_path.dentry->d_inode);
+	if (!tsk)
+		return -ESRCH;
+
+	mm = get_task_mm(tsk);
+
+	total_hpage_count = mm->hpage_stats.total_hpage_count;
+	if (total_hpage_count == 0)
+		total_hpage_count++;
+
+	seq_printf(m, "W = %u, total_hpage_count = %u, idle_hpage_count = %u, "
+			"hpage_requirement = %u -- ", 
+			mm->hpage_stats.weight,
+			mm->hpage_stats.total_hpage_count,
+			mm->hpage_stats.idle_hpage_count,
+			mm->hpage_stats.hpage_requirement);
+
+	seq_printf(m, "M_fairness = %u\n", osa_compute_fairness_metric(mm));
+
+	mmput(mm);
+
+	put_task_struct(tsk);
+
+	return 0;
+}
+
+static ssize_t osa_hpage_stats_proc_write(struct file *file, 
+		const char __user *buf, size_t len, loff_t *ppos)
+{
+	char write_buf[4];
+	struct task_struct *tsk;
+	struct mm_struct *mm;
+	struct inode *inode;
+	unsigned int weight;
+	int err;
+
+	/* TODO: write to proc interface via 'echo' contains garbage 
+	 * character. Move this to sysfs interface */
+	memset(write_buf, 0, 4);
+	/* only allows 4 digits */
+	copy_from_user(write_buf, buf, sizeof(char) * 4);
+	write_buf[3] = '\0';
+
+	err = kstrtou32(write_buf, 4, &weight);
+	if (err)
+		return err;
+
+	inode = file->f_path.dentry->d_inode;
+	if (!inode)
+		return -ESRCH;
+
+	tsk = get_proc_task(inode);
+	if (!tsk)
+		return -ESRCH;
+
+	mm = get_task_mm(tsk);
+
+	/* this interface must not applied to kthread */
+	if (!mm)
+		goto exit;
+
+	if (!is_member_of_scan_list(mm))
+		list_add(&mm->osa_hpage_scan_link, &osa_hpage_scan_list);
+
+	mm->hpage_stats.weight = weight;
+
+	mmput(mm);
+exit:
+	put_task_struct(tsk);
+	return len;
+}
+
+static int osa_hpage_stats_proc_open(struct inode *inode, struct file *file) 
+{
+	return single_open(file, osa_hpage_stats_proc_show, file);
+}
+
+static int osa_hpage_stats_proc_release(struct inode *inode, struct file *file)
+{
+	return single_release(inode, file);
+}
+
+struct file_operations osa_hpage_stats_operations = {
+	.owner = THIS_MODULE,
+	.open = osa_hpage_stats_proc_open,
+	.read = seq_read,
+	.write = osa_hpage_stats_proc_write,
+	.release = osa_hpage_stats_proc_release,
+};
+
+////////////////////////////////////////////////////////////////////////
+
+#define osa_lru_to_page(_head) (list_entry((_head), struct page, lru))
+
+struct osa_dump_lru_private
+{
+	struct zonelist *_zonelist;
+	struct zone *zone;
+};
+
+static void dump_lru_show_print(struct seq_file *m, pg_data_t *pgdat,
+							struct zone *zone)
+{
+	struct list_head *head, *tmp, *iter;
+	struct page *page;
+	struct lruvec *lruvec;
+	struct mem_cgroup *memcg;
+	int i;
+	struct mem_cgroup_reclaim_cookie reclaim = {
+		.zone = zone,
+		.priority = DEF_PRIORITY,
+	};
+
+	/* List infomation
+		0 - LRU_INACTIVE_ANON = LRU_BASE,
+		1 - LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
+		2 - LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
+		3 - LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
+		4 - LRU_UNEVICTABLE,
+		5 - NR_LRU_LISTS */
+	seq_printf(m, "node %d, zone %8s\n", pgdat->node_id, zone->name);
+	for (i = 0; i < NR_LRU_LISTS; i++) {
+
+		if (i >= LRU_INACTIVE_FILE)
+			continue;
+
+		seq_printf(m, "list number %d\n", i);
+
+		memcg = mem_cgroup_iter(NULL, NULL, &reclaim);
+		do {
+
+			seq_printf(m, "cgroup id %d\n", memcg->css.cgroup->id);
+			lruvec = mem_cgroup_zone_lruvec(zone, memcg);
+			head = &lruvec->lists[i];
+			/* from tail to head 
+			list_for_each_prev_safe(iter, tmp, head) {
+			*/
+			/* from head to tail */
+			list_for_each_safe(iter, tmp, head) {
+				page = osa_lru_to_page(iter);
+
+				seq_printf(m, "owner %d pfn %lx\n", osa_inst_get_page_owner(page, 0),
+						page_to_pfn(page));
+			}
+		} while((memcg = mem_cgroup_iter(NULL, memcg, &reclaim)));
+	}
+	
+}
+
+/* Walk all the zones in a node and print using a callback */
+static void osa_walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,
+		void (*print)(struct seq_file *m, pg_data_t *, struct zone *))
+{
+	struct zone *zone;
+	struct zone *node_zones = pgdat->node_zones;
+	unsigned long flags;
+
+	for (zone = node_zones; zone - node_zones < MAX_NR_ZONES; ++zone) {
+		if (!populated_zone(zone))
+			continue;
+
+		spin_lock_irqsave(&zone->lock, flags);
+		print(m, pgdat, zone);
+		spin_unlock_irqrestore(&zone->lock, flags);
+	}
+}
+
+int osa_dump_lru_show(struct seq_file *m, void *v) 
+{
+	pg_data_t *pgdat = (pg_data_t *)v;
+	osa_walk_zones_in_node(m, pgdat, dump_lru_show_print);
+	return 0;
+}
+
+static void *_dump_lru_seq_start(struct seq_file *m, loff_t *pos)
+{
+	pg_data_t *pgdat;
+	loff_t node = *pos;
+
+	for (pgdat = first_online_pgdat();
+			pgdat && node;
+			pgdat = next_online_pgdat(pgdat))
+		--node;
+
+	return pgdat;
+}
+
+static void *_dump_lru_seq_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	pg_data_t *pgdat = (pg_data_t *)v;
+
+	(*pos)++;
+
+	return next_online_pgdat(pgdat);
+}
+
+static void _dump_lru_seq_stop(struct seq_file *m, void *v)
+{
+}
+
+static const struct seq_operations osa_dump_lru_seq_ops = {
+	.start = _dump_lru_seq_start,
+	.next = _dump_lru_seq_next,
+	.stop = _dump_lru_seq_stop,
+	.show = osa_dump_lru_show
+};
+
+static int osa_dump_lru_proc_open(struct inode *inode, struct file *file) 
+{
+	int err = 0;
+	struct osa_dump_lru_private *dump_lru_private;
+	//struct task_struct *tsk;
+
+	dump_lru_private = __seq_open_private(file, &osa_dump_lru_seq_ops,
+			sizeof(struct osa_dump_lru_private));
+
+	if (!dump_lru_private)
+		return -ENOMEM;
+
+	/*
+	tsk = get_proc_task(inode);
+
+	if (!tsk) {
+		seq_release_private(inode, file);
+		err = -ESRCH;
+		goto out;
+	}
+	*/
+
+	/* NUMA is not supported, only works for nid 0 now */
+	dump_lru_private->_zonelist = node_zonelist(0, GFP_KERNEL);
+	
+	/*
+out:
+	if (!tsk)
+		put_task_struct(tsk);
+	*/
+
+	return err;
+}
+
+static int osa_dump_lru_proc_release(struct inode *inode, struct file *file)
+{
+	return seq_release_private(inode, file);
+}
+
+struct file_operations osa_dump_lru_operations = {
+	.owner = THIS_MODULE,
+	.open = osa_dump_lru_proc_open,
+	.read = seq_read,
+	.release = osa_dump_lru_proc_release,
+};
+
+static int __init osa_hugepage_proc_init(void)
+{
+	/* init procfs */
+	proc_create("dump_lru", 0, NULL, &osa_dump_lru_operations);
+
+	return 0;
+}
+subsys_initcall(osa_hugepage_proc_init);
+
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/osa/hugepage_scan.c linux-4.3-osa/osa/hugepage_scan.c
--- linux-4.3-org/osa/hugepage_scan.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/osa/hugepage_scan.c	2016-11-02 10:48:39.502742365 -0500
@@ -0,0 +1,1838 @@
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/console.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/kallsyms.h>
+#include <linux/bootmem.h>
+#include <linux/mm.h>
+#include <linux/mm_types.h>
+#include <linux/mm_inline.h>
+#include <linux/huge_mm.h>
+#include <linux/page_idle.h>
+#include <linux/ksm.h>
+#include <linux/sysfs.h>
+#include <linux/kobject.h>
+#include <linux/freezer.h>
+#include <linux/compaction.h>
+#include <linux/mmzone.h>
+#include <linux/node.h>
+#include <linux/workqueue.h>
+#include <linux/khugepaged.h>
+#include <linux/hugetlb.h>
+#include <linux/migrate.h>
+#include <linux/balloon_compaction.h>
+#include <linux/pagevec.h>
+#include <linux/random.h>
+#include <asm/uaccess.h>
+#include <asm/current.h>
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+#include <osa/osa.h>
+#include "../../../fs/proc/internal.h" 
+#include "../mm/internal.h"
+
+#define SCAND_WQ 
+
+#define SEC_SCAN_COUNT 0x8
+
+unsigned int hugepage_fairness = 0;
+unsigned long distance_divisor = 0;
+unsigned long active_bpage_count = 0;
+
+DEFINE_SPINLOCK(osa_page_set_lock);
+DEFINE_SPINLOCK(osa_poplmap_lock);
+DECLARE_WAIT_QUEUE_HEAD(osa_hpage_scand_wait);
+DECLARE_WAIT_QUEUE_HEAD(osa_hpage_compactd_wait);
+DECLARE_WAIT_QUEUE_HEAD(osa_aggregationd_wait);
+struct list_head osa_aggregation_list;
+
+#ifdef SCAND_WQ
+static struct workqueue_struct *osa_hpage_scand_wq __read_mostly;
+static struct work_struct osa_hpage_scan_work;
+static struct task_struct *osa_aggregation_kthread __read_mostly;
+#else
+static struct task_struct *osa_hpage_scand_kthread __read_mostly;
+#endif
+static struct task_struct *osa_hpage_compactd_kthread __read_mostly;
+static unsigned int scan_sleep_millisecs = 4000;
+static unsigned int aggregation_sleep_millisecs = 0;
+static unsigned int compact_sleep_millisecs =  0;
+static unsigned long free_contig_pages_consumed;
+/* In a scanning of inactive list, this variable indicates
+ * how many scanning can be passed for huge page */
+unsigned int deferred_mode = 1;
+unsigned int util_threshold = 90;
+struct list_head hugepage_worklist;
+static util_node_t *_util_node[512];
+
+static struct list_head osa_hot_page_set[5];
+static unsigned int freq_scan_count = 0;
+static unsigned int osa_aggr_scan_count = 0;
+static unsigned long count[5];
+
+struct osa_walker_stats {
+	unsigned int hpage_requirement;
+	unsigned int total_hpage_count;
+	unsigned long total_bpage_count;
+	unsigned int idle_hpage_count;
+	unsigned long idle_bpage_count;
+	unsigned int idle_tau; //idle page penalty parameter
+	unsigned int weight;
+	//up-to-here it is the same as osa_hpage_stats
+	//so casting to osa_hpage_stat is safe.
+	unsigned int hit;
+	unsigned int miss;
+	unsigned long nopromote;
+};
+
+int is_member_of_scan_list(struct mm_struct *mm)
+{
+	struct list_head *pos, *tmp;
+	struct mm_struct *m = NULL;
+
+	list_for_each_safe(pos, tmp, &osa_hpage_scan_list) {
+		m = list_entry(pos, struct mm_struct, osa_hpage_scan_link);
+
+		if (m && m == mm)
+			return 1;
+	}
+
+	return 0;
+}
+
+unsigned int osa_compute_fairness_metric(struct mm_struct *mm) 
+{
+	unsigned int fairness_metric, total_hpage_count;
+
+	if (mm->hpage_stats.total_hpage_count == 0)
+		total_hpage_count = 1;
+	else
+		total_hpage_count = mm->hpage_stats.total_hpage_count;
+
+	fairness_metric = (mm->hpage_stats.weight 
+		* (mm->hpage_stats.hpage_requirement * 100))
+		/ total_hpage_count;
+
+	return fairness_metric;
+}
+
+void *osa_popl_node_delete(struct mm_struct *mm, unsigned long address)
+{
+	return radix_tree_delete(&mm->root_popl_map, PAGE_ALIGN_FLOOR(address));
+}
+
+void *osa_popl_node_lookup(struct mm_struct *mm, unsigned long address)
+{
+	return radix_tree_lookup(&mm->root_popl_map, PAGE_ALIGN_FLOOR(address));
+}
+
+int osa_popl_node_insert(struct mm_struct *mm,
+		unsigned long address, popl_node_t *node)
+{
+	return radix_tree_insert(&mm->root_popl_map, 
+			PAGE_ALIGN_FLOOR(address), node);
+}
+
+inline aggr_node_t *osa_aggr_node_alloc(void)
+{
+	aggr_node_t *aggr_node;
+
+	aggr_node = kmem_cache_zalloc(osa_aggregate_node_cachep, GFP_ATOMIC);
+
+	if (!aggr_node)
+		return NULL;
+
+	return aggr_node;
+}
+
+void osa_aggr_node_free(aggr_node_t *aggr_node)
+{
+	kmem_cache_free(osa_aggregate_node_cachep, aggr_node);
+}
+
+///////////////////////////////////////////////////////////////////
+// Guest physical memory aggregation
+
+typedef struct aggregate_control {
+	struct list_head aggregate_list;
+	struct list_head free_list;
+	unsigned int nr_aggregate;
+	unsigned int nr_free;
+} aggregate_control_t;
+
+static struct page *osa_ac_new_page(struct page *page, 
+		unsigned long private, int **unused) 
+{
+	aggregate_control_t *ac;
+	struct page *freepage;
+
+	ac = (aggregate_control_t *)private;
+
+	if (list_empty(&ac->free_list)) {
+		trace_printk("Something Wrong\n");
+		return NULL;
+	}
+
+	freepage = list_entry(ac->free_list.next, struct page, lru);
+	/*
+	trace_printk("alloc page %lx mapcount %d count %d\n", 
+			page_to_pfn(freepage), 
+			atomic_read(&freepage->_mapcount),
+			atomic_read(&freepage->_count));
+	*/
+
+	set_bit(OSA_PF_AGGR, &freepage->osa_flag);
+	list_del(&freepage->lru);
+	ac->nr_free--;
+
+	return freepage;
+	//return alloc_pages(GFP_HIGHUSER_MOVABLE, 0);
+}
+
+static void osa_ac_free_page(struct page *page, 
+		unsigned long private)
+{
+	aggregate_control_t *ac;
+	ac = (aggregate_control_t *)private;
+
+	list_add_tail(&page->lru, &ac->free_list);
+	ac->nr_free++;
+}
+
+static void putback_aggregate_pages(struct list_head *l)
+{
+	struct page *page;
+	struct page *page2;
+
+	list_for_each_entry_safe(page, page2, l, lru) {
+		clear_bit(OSA_PF_AGGR, &page->osa_flag);
+		list_del(&page->lru);
+		dec_zone_page_state(page, NR_ISOLATED_ANON +
+				page_is_file_cache(page));
+		if (unlikely(isolated_balloon_page(page)))
+			balloon_page_putback(page);
+		else
+			putback_lru_page(page);
+	}
+}
+
+static int isolate_aggregate_list(aggregate_control_t *ac)
+{
+	struct page *page;
+	struct list_head *l, *t;
+	int rc;
+	aggr_node_t *aggr_node;
+
+	spin_lock(&osa_page_set_lock);
+
+	list_for_each_safe(l, t, &osa_aggregation_list) {
+		aggr_node = list_entry(l, aggr_node_t, link);
+		if (!aggr_node)
+			continue;
+
+		page = aggr_node->page;
+
+		if (unlikely(!page))
+			continue;
+
+		/* checks whether pages can be aggregated or not */
+		if (page->osa_flag & OSA_PF_AGGR)
+			continue;
+
+		/* skip hugepage (transparent or hugetlb) */
+		if (PageTransCompound(page))
+			continue;
+
+		if (unlikely(!PageLRU(page)))
+			continue;
+
+		/* skip VDSO */
+		if (PageReserved(page))
+			continue;
+
+		/* skip zero pfn */
+		if (is_zero_pfn(page_to_pfn(page)))
+			continue;
+
+		/* skip shared page */
+		if (page_mapcount(page) > 1)
+			continue;
+
+		rc = isolate_lru_page(page);
+		if (rc) {
+			putback_lru_page(page);
+			continue;
+		}
+
+		//trace_printk("isolated page %lx\n", page_to_pfn(page));
+
+		inc_zone_page_state(page, NR_ISOLATED_ANON +
+				page_is_file_cache(page));
+
+		list_add(&page->lru, &ac->aggregate_list);
+		ac->nr_aggregate++;
+
+		list_del(&aggr_node->link);
+		osa_aggr_node_free(aggr_node);
+
+		if (ac->nr_aggregate >= 512)
+			break;
+	}
+
+	spin_unlock(&osa_page_set_lock);
+
+	return 0;
+}
+
+static int osa_gfn_aggregate(int total_nr_aggregate)
+{
+	int rc = 0;
+	struct page *page, *_page;
+	unsigned int i, j, max_page_block = 25;
+	aggregate_control_t ac = {
+		.nr_aggregate = 0,
+		.nr_free = 0,
+	};
+
+	for (i = 0; i < max_page_block; i++) {
+		ac.nr_aggregate = 0;
+		ac.nr_free = 0;
+		INIT_LIST_HEAD(&ac.aggregate_list);
+		INIT_LIST_HEAD(&ac.free_list);
+
+		rc = isolate_aggregate_list(&ac);
+
+		if (ac.nr_aggregate == 512) {
+			gfp_t gfp = GFP_HIGHUSER_MOVABLE;
+			page = alloc_pages(gfp, HPAGE_PMD_ORDER);
+
+			if (!page) {
+				trace_printk("Fail to allocate dst page\n");
+				continue;
+			}
+
+			/*
+			trace_printk("Dest pfn %lx - %lx\n", 
+					page_to_pfn(page), page_to_pfn(page+511));
+			*/
+			for (j = 0, _page = page; j < 512; j++, _page++) {
+				get_page(_page);
+				list_add_tail(&_page->lru, &ac.free_list);
+			}
+
+			migrate_prep();
+
+			rc = migrate_pages(&ac.aggregate_list, 
+					osa_ac_new_page, osa_ac_free_page, 
+					(unsigned long)&ac,
+					MIGRATE_SYNC, MR_GFN_AGGREGATE);
+
+			/*
+			trace_printk("[------] aggregate page %d (failed %d)\n", 
+					ac.nr_aggregate, rc);
+			*/
+
+			// handle migration failed page
+			if (rc) 
+				putback_aggregate_pages(&ac.aggregate_list);
+
+			if(ac.nr_free > 0) {
+				struct page *page, *next;
+
+				list_for_each_entry_safe(page, next, &ac.free_list, lru) {
+					clear_bit(OSA_PF_AGGR, &page->osa_flag);
+					list_del(&page->lru);
+					putback_lru_page(page);
+				}
+			}
+
+			total_nr_aggregate -= 512;
+		} else {
+			putback_aggregate_pages(&ac.aggregate_list);
+		}
+
+		if (total_nr_aggregate < 512)
+			break;
+	}
+
+	return 0;
+}
+
+///////////////////////////////////////////////////////////////////
+// osa_aggregationd
+
+static int osa_aggregationd_has_work(void)
+{
+	return aggregation_sleep_millisecs && 
+		!list_empty(&osa_aggregation_list);
+}
+
+static int osa_aggregationd_wait_event(void)
+{
+	return aggregation_sleep_millisecs && 
+        (!list_empty(&osa_aggregation_list) || kthread_should_stop());
+}
+
+static void osa_aggregation_wait_work(void) 
+{
+	if (osa_aggregationd_has_work()) {
+		wait_event_freezable_timeout(osa_aggregationd_wait,
+				kthread_should_stop(),
+				msecs_to_jiffies(aggregation_sleep_millisecs));
+	} else
+		wait_event_freezable(osa_aggregationd_wait, 
+				osa_aggregationd_wait_event());
+
+	return;
+}
+
+void osa_aggregation_do_scan(void)
+{
+	aggr_node_t *aggr_node, *tmp;
+	unsigned long pfn;
+	struct page *page, *_page;
+	unsigned int nr_aggregate;
+	unsigned int accessed = 0;
+
+	list_for_each_entry_safe(aggr_node, tmp, 
+			&osa_aggregation_list, link)  {
+		page = aggr_node->page;
+
+		if (!page)
+			continue;
+
+		pfn = page_to_pfn(page);
+
+		_page = page_idle_get_page(pfn);
+
+		if (_page) {
+			VM_BUG_ON(page != _page);
+			VM_BUG_ON(PageCompound(_page));
+
+			page_idle_clear_pte_refs(_page);
+
+			bitmap_shift_left(aggr_node->access_bitmap,
+					aggr_node->access_bitmap, 1, AGGR_BITMAP_SIZE);
+
+			if (page_is_idle(_page))  {
+				bitmap_clear(aggr_node->access_bitmap, 0, 1);
+			} else {
+				bitmap_set(aggr_node->access_bitmap, 0, 1);
+				accessed++;
+			}
+
+			set_page_idle(_page);
+			put_page(_page);
+		}
+	}
+
+	osa_aggr_scan_count++;
+
+	if (osa_aggr_scan_count == AGGR_BITMAP_SIZE) {
+		osa_aggr_scan_count = 0;
+		nr_aggregate = 0;
+
+		//check osa_aggregation_list
+		list_for_each_entry_safe(aggr_node, tmp, 
+				&osa_aggregation_list, link)  {
+			if (!aggr_node)
+				continue;
+
+			page = aggr_node->page;
+
+			BUG_ON(!page);
+
+			if (bitmap_weight(aggr_node->access_bitmap, 
+						AGGR_BITMAP_SIZE) >= 3) {
+				nr_aggregate++;
+			} else {
+				list_del(&aggr_node->link);
+				osa_aggr_node_free(aggr_node);
+			}
+		}
+
+		//Perform aggregation
+		if (nr_aggregate >= 512)
+			osa_gfn_aggregate(nr_aggregate);
+
+		//Cleanup osa_aggregation_list
+		list_for_each_entry_safe(aggr_node, tmp, 
+				&osa_aggregation_list, link)  {
+			list_del(&aggr_node->link);
+			osa_aggr_node_free(aggr_node);
+		}
+
+		INIT_LIST_HEAD(&osa_aggregation_list);
+	}
+	return;
+}
+
+static int osa_aggregationd(void *none)
+{
+	set_freezable();
+	set_user_nice(current, MAX_NICE);
+
+	while(!kthread_should_stop()) {
+		osa_aggregation_do_scan();
+		osa_aggregation_wait_work();
+	}
+
+	return 0;
+}
+
+///////////////////////////////////////////////////////////////////
+// osa_aggregationd
+
+/* I did not call get_page before returning page.
+ * So caller must handle the case when page status is changed (e.g. freed)*/
+static struct page *osa_follow_page(struct mm_struct *mm, unsigned long address) 
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *ptep, pte;
+	spinlock_t *ptl;
+	struct page *page;
+	unsigned long pfn;
+
+	pgd = pgd_offset(mm, address);
+	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
+		return NULL;
+
+	pud = pud_offset(pgd, address);
+	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
+		return NULL;
+	// 1GB hugepage
+	if (pud_huge(*pud)) {
+		if (pud_present(*pud)) {
+			return pte_page(*(pte_t *)pud) + ((address & ~PUD_MASK) >> PAGE_SHIFT);
+		} else
+			return NULL;
+	}
+
+	pmd = pmd_offset(pud, address);
+	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
+		return NULL;
+	// 2MB hugepage (hugetlb)
+	if (pmd_huge(*pmd)) {
+		ptl = pmd_lockptr(mm, pmd);
+		spin_lock(ptl);
+		if (pmd_present(*pmd)) {
+			page = pmd_page(*pmd) + ((address & ~PMD_MASK) >> PAGE_SHIFT);
+		} else
+			page = NULL;
+		spin_unlock(ptl);
+
+		return page;
+	}
+
+	// 2MB hugepage (Transparent hugepage)
+	if (pmd_trans_huge(*pmd)) {
+		ptl = pmd_lockptr(mm, pmd);
+		if (likely(!pmd_trans_splitting(*pmd))) {
+			//Follow PMD entry. There is many corner cases 
+			//(e.g., on going of NUMA migration) 
+			//but I simplify the checks only necessary for our case.
+			page = pmd_page(*pmd);
+
+			// checks whether the page is compound or not
+			VM_BUG_ON_PAGE(!PageHead(page), page);
+		} else {
+			page = NULL;
+		}
+		spin_unlock(ptl);
+
+		return page;
+	}
+
+	ptep = pte_offset_map_lock(mm, pmd, address, &ptl);
+	pte = *ptep;
+
+	if (!pte_present(pte)) {
+		page = NULL;
+	} else {
+		pfn = pte_pfn(pte);
+
+		if (unlikely(pfn > highest_memmap_pfn)) {
+			page = NULL;
+		} else {
+			// did not check the case 
+			// 1. where address is from the last page.
+			// In that case, base_address of last page < address
+			// 2. address is used for fixed map
+			// For other detailed checks, refer to vm_normal_page()
+			
+			if (is_zero_pfn(pfn)) {
+				page = NULL;
+			} else {
+				page = pfn_to_page(pfn);
+			}
+		}
+	}
+
+	pte_unmap_unlock(ptep, ptl);
+
+	return page;
+}
+
+/* freq_node API */
+//deprecated
+int osa_util_node_insert(struct mm_struct *mm,
+		unsigned long address, util_node_t *node)
+{
+	return 0;
+}
+
+//deprecated
+void *osa_util_node_delete(struct mm_struct *mm, unsigned long address)
+{
+	return NULL;
+}
+
+void *osa_util_node_lookup(struct mm_struct *mm, unsigned long address)
+{
+	struct page *page = NULL;
+
+	page = osa_follow_page(mm, PAGE_ALIGN_FLOOR(address));
+
+	if (page)
+		return (void *)&page->util_info;
+	else
+		return NULL;
+}
+
+void *osa_util_node_lookup_fast(struct page *page)
+{
+	return (void *)&page->util_info;
+}
+
+void frequency_update(util_node_t *node) 
+{
+    int i;
+
+    if (!node) 
+        return;
+
+#if 0 //ARMA method.
+    //may replace with bitwise operation in future
+    for (i = PRI_HISTORY_SIZE - 1; i > 0; i--)
+        node->frequency[i] = node->frequency[i-1];
+
+    node->frequency[0] = (int32_t)((int32_t)((uint16_t)node->freq_bitmap[0])
+        - (1<<(FREQ_BITMAP_SIZE-1)));
+    //trace_printk("node priority updated: %d\n", node->frequency[0]);
+
+    //to avoid division
+    for (i = 1; i < PRI_HISTORY_SIZE; i++)
+        if (node->frequency[i] >= 0)
+            node->frequency[0] += node->frequency[i] >> i;
+		else
+            node->frequency[0] -= (-node->frequency[i]) >> i;
+#else //EMA method.
+	node->frequency[1] = bitmap_weight(node->freq_bitmap, FREQ_BITMAP_SIZE);
+	node->frequency[1] -= (3 * FREQ_BITMAP_SIZE / 4);
+	node->frequency[0] = (node->frequency[0] * 70) + (node->frequency[1] * 30);
+	node->frequency[0] = node->frequency[0] / 100;
+#endif
+}
+
+static int osa_bpage_pte_walker(pte_t *pte, unsigned long addr,
+		unsigned long end, struct mm_walk *walk)
+{
+    struct page *page = NULL;
+	struct osa_walker_stats *walker_stats;
+	unsigned long pfn;
+    util_node_t *f_node;
+    struct mm_struct *mm;
+	int ret = 0;
+	unsigned long lottery;
+	uint8_t lottery_selected = 0;
+
+	mm = (struct mm_struct *)walk->mm;
+	walker_stats = (struct osa_walker_stats *)walk->private;
+
+    if (pte && !pte_none(*pte))
+        page = pte_page(*pte);
+
+    if (page && !PageTransCompound(page)) {
+		walker_stats->total_bpage_count++;
+
+		pfn = (pte_val(*pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
+		page = page_idle_get_page(pfn);
+
+		if (page) {
+			page_idle_clear_pte_refs(page);
+
+            //f_node = osa_util_node_lookup(mm, PAGE_ALIGN_FLOOR(addr));
+            f_node = osa_util_node_lookup_fast(page);
+			f_node->page = page;
+
+			if (!f_node) 
+				goto out;
+
+			bitmap_shift_right(f_node->freq_bitmap, f_node->freq_bitmap, 1,
+					FREQ_BITMAP_SIZE);
+
+			if (deferred_mode >= 2) {
+				// hot page: run lottery for a random sampling.
+				if (f_node->frequency[0] >= 0) {
+					//get_random_bytes(&lottery, sizeof(unsigned long));
+					get_random_bytes_arch(&lottery, sizeof(unsigned long));
+					if (!active_bpage_count)
+						active_bpage_count++;
+					if (lottery % active_bpage_count > 
+							((active_bpage_count * 20) / 100)) {
+						lottery_selected = 1;
+					} else {
+						lottery_selected = 0;
+						clear_page_idle(page);
+					}
+				} else {
+					// cold page: lottery is always selected.
+					lottery_selected = 1;
+				}
+			} else
+				lottery_selected = 1;
+
+			// Clearing access bit causes a TLB miss of the address.
+			if (lottery_selected) {
+				page_idle_clear_pte_refs(page);
+
+				bitmap_shift_right(f_node->freq_bitmap, f_node->freq_bitmap, 1,
+						FREQ_BITMAP_SIZE);
+			}
+
+			if (page_is_idle(page)) {
+				walker_stats->idle_bpage_count++;
+                bitmap_clear(f_node->freq_bitmap, FREQ_BITMAP_SIZE-1, 1);
+                if (f_node->frequency[0] >= 0) {
+					walker_stats->miss++;
+				} else {
+					walker_stats->hit++;
+				}
+			} else {
+                bitmap_set(f_node->freq_bitmap, FREQ_BITMAP_SIZE-1, 1);
+                if (f_node->frequency[0] < 0) {
+					walker_stats->miss++;
+				} else {
+					walker_stats->hit++;
+				}
+			}
+
+			frequency_update(f_node);
+			set_page_idle(page);
+			put_page(page);
+
+			if ((freq_scan_count % SEC_SCAN_COUNT) == 0) {
+				unsigned int weight = 0, i = 0;
+				if (!spin_trylock(&osa_page_set_lock))
+					goto out;
+
+				//Clear osa_flag to enable re-aggregation
+				if ((freq_scan_count & 0xff) == 0)
+					clear_bit(OSA_PF_AGGR, &page->osa_flag);
+
+				for (i = FREQ_BITMAP_SIZE - 1; i > FREQ_BITMAP_SIZE - 5; i--) 
+					if (test_bit(i, f_node->freq_bitmap))
+						weight++;
+
+				if (weight == 4) {
+					list_add(&f_node->link, &osa_hot_page_set[0]);
+					count[0]++;
+				}
+
+				/* //used for ARMA method.
+				switch(bitmap_weight(f_node->freq_bitmap, FREQ_BITMAP_SIZE)) {
+					case FREQ_BITMAP_SIZE:
+						list_add(&f_node->link, &osa_hot_page_set[0]);
+						count[0]++;
+						break;
+					case FREQ_BITMAP_SIZE - 1:
+						list_add(&f_node->link, &osa_hot_page_set[1]);
+						count[1]++;
+						break;
+					case FREQ_BITMAP_SIZE - 2:
+						list_add(&f_node->link, &osa_hot_page_set[2]);
+						count[2]++;
+						break;
+					case FREQ_BITMAP_SIZE - 3:
+						list_add(&f_node->link, &osa_hot_page_set[3]);
+						count[3]++;
+						break;
+					case FREQ_BITMAP_SIZE - 4:
+						list_add(&f_node->link, &osa_hot_page_set[4]);
+						count[4]++;
+						break;
+				}
+				*/
+
+				spin_unlock(&osa_page_set_lock);
+			}
+		}
+    }
+out:
+    return ret;
+}
+
+static int osa_hpage_pmd_walker(pmd_t *pmd, unsigned long addr,
+		unsigned long end, struct mm_walk *walk)
+{
+	struct osa_walker_stats *walker_stats;
+	struct page *page;
+	unsigned long _addr, pfn;
+    util_node_t *f_node;
+	pte_t *pte;
+    struct mm_struct *mm;
+	int ret = 0;
+
+	mm = (struct mm_struct *)walk->mm;
+	walker_stats = (struct osa_walker_stats *)walk->private;
+
+	if (pmd_trans_huge(*pmd)) {
+		// Count total huge page
+		walker_stats->total_hpage_count++;
+
+		// Count Idle huge page
+		pfn = (pmd_val(*pmd) & PTE_PFN_MASK) >> PAGE_SHIFT;
+		page = page_idle_get_page(pfn);
+
+		if (page) {
+			page_idle_clear_pte_refs(page);
+			//f_node = osa_util_node_lookup(mm, HPAGE_ALIGN_FLOOR(addr));
+
+			VM_BUG_ON(!PageCompound(page));
+			f_node = osa_util_node_lookup_fast(page);
+			f_node->page = page;
+
+			if (!f_node)
+				goto out;
+
+			bitmap_shift_right(f_node->freq_bitmap, f_node->freq_bitmap, 1,
+					FREQ_BITMAP_SIZE);
+
+			if (page_is_idle(page)) {
+				walker_stats->idle_hpage_count++;
+				bitmap_clear(f_node->freq_bitmap, FREQ_BITMAP_SIZE-1, 1);
+				if (f_node->frequency[0] > 0) {
+					walker_stats->miss += 512;
+				} else {
+					walker_stats->hit += 512;
+				}
+			}
+			else {
+				bitmap_set(f_node->freq_bitmap, FREQ_BITMAP_SIZE-1, 1);
+				if (f_node->frequency[0] < 0) {
+					walker_stats->miss += 512;
+				} else {
+					walker_stats->hit += 512;
+				}
+			}
+
+			frequency_update(f_node);
+			set_page_idle(page);
+			put_page(page);
+		}
+	} else {
+		// Walk PTE
+		pte = pte_offset_map(pmd, addr);
+
+		_addr = addr;
+
+		for (;;) {
+			ret = osa_bpage_pte_walker(pte, _addr, _addr + PAGE_SIZE, walk);
+			if (ret)
+				break;
+
+			_addr += PAGE_SIZE;
+			if (_addr == end)
+				break;
+
+			pte++;
+		}
+	}
+
+#if 0  //Experimental features. currently unused.
+	/* frequency based hugepage promotion */
+	if (!pmd_trans_huge(*pmd) && deferred_mode >= 3) {
+		struct vm_area_struct *vma;
+		unsigned long haddr = HPAGE_ALIGN_FLOOR(addr);
+		unsigned long _addr;
+		work_node_t *work_node = NULL;
+		unsigned int i, first, promote = 0;
+		DECLARE_BITMAP(locality_bitmap, FREQ_BITMAP_SIZE);
+
+		vma = find_vma(mm, addr);
+		if (transparent_hugepage_enabled(vma) &&
+			vma_is_anonymous(vma) &&
+			!(haddr < vma->vm_start || haddr + HPAGE_PMD_SIZE > vma->vm_end) &&
+			!(unlikely(khugepaged_enter(vma, vma->vm_flags))) 
+		   ) {
+
+			pfn = (pmd_val(*pmd) & PTE_PFN_MASK) >> PAGE_SHIFT;
+			page = pfn_to_page(pfn);
+
+			VM_BUG_ON(PageCompound(page));
+
+			for (first = i = 0, _addr = haddr; _addr < haddr + HPAGE_SIZE; 
+					_addr += PAGE_SIZE, i++) {
+				_util_node[i] = osa_util_node_lookup(mm, _addr);
+				if (!first && _util_node[i])
+					first = i;
+
+			}
+
+			bitmap_zero(locality_bitmap, FREQ_BITMAP_SIZE);
+			bitmap_or(locality_bitmap, locality_bitmap, 
+					_util_node[first]->freq_bitmap, FREQ_BITMAP_SIZE);
+
+			for (i = first ; i < 512; i++) {
+				if (_util_node[i])
+					bitmap_and(locality_bitmap, locality_bitmap,
+							_util_node[i]->freq_bitmap, FREQ_BITMAP_SIZE);
+
+			}
+
+			if (bitmap_weight(locality_bitmap, FREQ_BITMAP_SIZE) 
+					> (FREQ_BITMAP_SIZE * 60 / 100)) {
+				/* trace_printk("ASK Promote : %lx %d\n", haddr,
+				   bitmap_weight(locality_bitmap, FREQ_BITMAP_SIZE)); */
+				promote = 1;
+			} else {
+				/* trace_printk("NO promote : %lx %d\n", haddr,
+				   bitmap_weight(locality_bitmap, FREQ_BITMAP_SIZE)); */
+				promote = 0;
+				walker_stats->nopromote++;
+			}
+
+			if (promote) {
+				/*
+				trace_printk("go Promote : %lx %d\n", haddr,
+						bitmap_weight(locality_bitmap, FREQ_BITMAP_SIZE)); 
+				*/
+				work_node = osa_work_node_alloc();
+				work_node->mm = mm;
+				work_node->address = haddr;
+
+				spin_lock(&worklist_lock);
+				list_add_tail(&work_node->list, 
+						&hugepage_worklist);
+				spin_unlock(&worklist_lock);
+
+				wake_up_interruptible(&khugepaged_wait);
+			}
+		}
+	}
+#endif
+
+out:
+	return ret;
+}
+
+void osa_hpage_enter_list(struct mm_struct *mm)
+{
+	spin_lock(&osa_hpage_list_lock);
+	list_add_tail_rcu(&mm->osa_hpage_scan_link, &osa_hpage_scan_list);
+	spin_unlock(&osa_hpage_list_lock);
+}
+
+void osa_hpage_exit_list(struct mm_struct *mm)
+{
+	spin_lock(&osa_hpage_list_lock);
+
+	VM_BUG_ON(!mm);
+
+	list_del_rcu(&mm->osa_hpage_scan_link);
+
+	spin_unlock(&osa_hpage_list_lock);
+}
+
+/* scanner kthread */
+static int osa_hpage_do_walk(struct mm_struct *mm, 
+		struct osa_walker_stats *walker_stats) 
+{
+	int err = 0;
+	unsigned int hpage_requirement = 0;
+	unsigned long haddr;
+	unsigned int anon_rss;
+	struct vm_area_struct *vma = NULL;
+	struct mm_walk _hpage_walker = {
+		.pmd_entry = osa_hpage_pmd_walker,
+		.mm = mm,
+		.private = walker_stats,
+	};
+
+	VM_BUG_ON(!mm);
+
+	vma = mm->mmap;
+
+	walker_stats->hpage_requirement = 0;
+	walker_stats->miss = 0;
+	walker_stats->hit = 0;
+	walker_stats->nopromote = 0;
+
+	for ( ;vma != NULL; vma = vma->vm_next) {
+		if (!vma_is_anonymous(vma))
+			continue;
+			
+		//Entire VMA scanning.
+		err = walk_page_vma(vma, &_hpage_walker);
+
+		if (err) {
+			trace_printk("error in vma walk\n");
+			return err;
+		}
+
+		if (transparent_hugepage_enabled(vma)) {
+			for (haddr = HPAGE_ALIGN_FLOOR(vma->vm_start); haddr < vma->vm_end; 
+					haddr += HPAGE_PMD_SIZE) 
+				hpage_requirement++;
+		}
+
+		cond_resched();
+	}
+
+	mm->hpage_stats.total_hpage_count = walker_stats->total_hpage_count;
+	mm->hpage_stats.total_bpage_count = walker_stats->total_bpage_count;
+	mm->hpage_stats.idle_hpage_count = walker_stats->idle_hpage_count;
+	mm->hpage_stats.idle_bpage_count = walker_stats->idle_bpage_count;
+
+	active_bpage_count = walker_stats->total_bpage_count - 
+		walker_stats->idle_bpage_count;
+
+	anon_rss = get_mm_counter(mm, MM_ANONPAGES);
+	/* hpage_requirment is represented in terms of # of huge page */
+	mm->hpage_stats.hpage_requirement = anon_rss / 512;
+
+	return 0;
+}
+
+static void osa_page_cache_scan(struct super_block *sb, void *unused)
+{
+	struct inode *inode, *toput_inode = NULL;
+	pgoff_t indices[PAGEVEC_SIZE];
+	struct pagevec pvec;
+	pgoff_t index = 0, end = -1;
+	struct address_space *mapping;
+	util_node_t *f_node;
+	int i;
+
+	spin_lock(&sb->s_inode_list_lock);
+	list_for_each_entry(inode, &sb->s_inodes, i_sb_list) {
+		spin_lock(&inode->i_lock);
+		if ((inode->i_state & (I_FREEING|I_WILL_FREE|I_NEW)) ||
+		    (inode->i_mapping->nrpages < (300 << 8))) {
+			spin_unlock(&inode->i_lock);
+			continue;
+		}
+		__iget(inode);
+		spin_unlock(&inode->i_lock);
+		spin_unlock(&sb->s_inode_list_lock);
+
+		pagevec_init(&pvec, 0);
+
+		mapping = inode->i_mapping; 
+
+		while (index <= end && pagevec_lookup_entries(&pvec, mapping, index,
+					min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,
+					indices)) {
+			for (i = 0; i < pagevec_count(&pvec); i++) {
+				struct page *_page = pvec.pages[i];
+				struct page *page;
+
+				/* We rely upon deletion not changing page->index */
+				index = indices[i];
+				if (index > end)
+					break;
+
+				if (radix_tree_exceptional_entry(_page)) {
+					//clear_exceptional_entry(mapping, index, page);
+					continue;
+				}
+
+				page = page_idle_get_page(page_to_pfn(_page));
+
+				if (page) {
+					page_idle_clear_pte_refs(page);
+
+					//f_node = osa_util_node_lookup(mm, PAGE_ALIGN_FLOOR(addr));
+					f_node = osa_util_node_lookup_fast(page);
+					f_node->page = page;
+
+					if (!f_node) { 
+						put_page(page);
+						continue;
+					}
+
+					bitmap_shift_right(f_node->freq_bitmap, f_node->freq_bitmap, 1,
+							FREQ_BITMAP_SIZE);
+
+					if (page_is_idle(page)) {
+						bitmap_clear(f_node->freq_bitmap, FREQ_BITMAP_SIZE-1, 1);
+					} else {
+						bitmap_set(f_node->freq_bitmap, FREQ_BITMAP_SIZE-1, 1);
+					}
+
+					frequency_update(f_node);
+					set_page_idle(page);
+					put_page(page);
+
+					if ((freq_scan_count % SEC_SCAN_COUNT) == 0) {
+						unsigned int weight = 0, i = 0;
+						if (!spin_trylock(&osa_page_set_lock))
+							continue;
+
+						//Clear osa_flag to enable re-aggregation
+						if ((freq_scan_count & 0xff) == 0)
+							clear_bit(OSA_PF_AGGR, &page->osa_flag);
+
+						for (i = FREQ_BITMAP_SIZE - 1; i > FREQ_BITMAP_SIZE - 5; i--) 
+							if (test_bit(i, f_node->freq_bitmap))
+								weight++;
+
+						if (weight == 4) {
+							list_add(&f_node->link, &osa_hot_page_set[0]);
+							count[0]++;
+						}
+
+						spin_unlock(&osa_page_set_lock);
+					}
+				}
+			}
+
+			pagevec_remove_exceptionals(&pvec);
+			pagevec_release(&pvec);
+			cond_resched();
+			index++;
+		}
+
+		iput(toput_inode);
+		toput_inode = inode;
+
+		spin_lock(&sb->s_inode_list_lock);
+
+	}
+	spin_unlock(&sb->s_inode_list_lock);
+	iput(toput_inode);
+
+	return ;
+}
+
+void osa_hpage_do_scan(void)
+{
+	struct mm_struct *mm;
+	struct task_struct *tsk;
+	struct osa_walker_stats walker_stats; 
+	int err, i;
+
+	/* check the performance of this function */
+	drain_all_pages(NULL);
+
+	freq_scan_count &= 0xffffffff;
+
+	freq_scan_count++;
+
+	if ((freq_scan_count % SEC_SCAN_COUNT) == 0) {
+		spin_lock(&osa_page_set_lock);
+
+		for (i = 0; i < 5; i++) {
+			INIT_LIST_HEAD(&osa_hot_page_set[i]);
+			count[i] = 0;
+		}
+
+		spin_unlock(&osa_page_set_lock);
+	}
+
+
+	// Scanning per-application anonymous pages
+	list_for_each_entry_rcu(mm, &osa_hpage_scan_list, osa_hpage_scan_link) {
+		if (!mm) 
+			continue;
+
+		if (atomic_read(&mm->mm_users) == 0)
+			continue;
+
+		// for debugging
+		if (!mm->hpage_stats.weight)
+			continue;
+
+		rcu_read_lock();
+		tsk = rcu_dereference(mm->owner);
+
+        if (!tsk) 
+            goto unlock_exit;
+
+		if (atomic_read(&(tsk)->usage) == 0)
+			goto unlock_exit;
+
+		get_task_struct(tsk);
+		mm = get_task_mm(tsk);
+		rcu_read_unlock();
+
+		VM_BUG_ON(!mm);
+
+		memset(&walker_stats, 0, sizeof(struct osa_walker_stats));
+		err = osa_hpage_do_walk(mm, &walker_stats);
+
+		if (!err) {
+			trace_printk("[%d] pid %d: \n\tidle_hpage %u hpage %u idle_bpage %lu bpage %lu\n",
+					current->pid, tsk->pid, 
+					walker_stats.idle_hpage_count,
+					walker_stats.total_hpage_count,
+					walker_stats.idle_bpage_count,
+					walker_stats.total_bpage_count);
+			/*
+			trace_printk("[%d] pid %d: hit %u miss %u\n",
+					current->pid, tsk->pid, 
+					walker_stats.hit, walker_stats.miss);
+			*/
+			/*
+			trace_printk("[%d] pid %d: nopromote %lu\n",
+					current->pid, tsk->pid, 
+					walker_stats.nopromote);
+			trace_printk("count(0) = %lu, count(1) = %lu, count(2) = %lu "
+					"count(3) = %lu, count(4) = %lu\n",
+					count[0], count[1], count[2], count[3], count[4]);
+			*/
+		}
+
+		mmput(mm);
+		put_task_struct(tsk);
+	}
+
+	// Scanning page cache pages for gfn aggregation. currently disabled.
+#if 0
+	{
+		iterate_supers(osa_page_cache_scan, NULL);
+	}
+#endif
+
+	//Create aggregation candidate list for secondary scanning
+	if (aggregation_sleep_millisecs && 
+			((freq_scan_count % SEC_SCAN_COUNT) == 0)) {
+		util_node_t *util_node, *tmp;
+		aggr_node_t *aggr_node;
+		unsigned int count = 0;
+		list_for_each_entry_safe(util_node, tmp, 
+				&osa_hot_page_set[0], link)  {
+
+			//Max limit: 400MB
+			if (count > (300 << 8))
+				break;
+
+			aggr_node = osa_aggr_node_alloc();
+			if (!aggr_node) {
+				trace_printk("Cannot allocate aggr_node\n");
+				continue;
+			}
+
+			aggr_node->page = util_node->page;
+			/*
+			// detach from hotpage set
+			list_del(&util_node->link);
+			*/
+			INIT_LIST_HEAD(&aggr_node->link);
+			list_add(&aggr_node->link, &osa_aggregation_list);
+
+			count++;
+		}
+		trace_printk("trigger secondary scanning with %d pages\n", count);
+		wake_up_interruptible(&osa_aggregationd_wait);
+	}
+
+#if 0
+	//entire physical page scan: only used for debugging
+	{
+	unsigned long pfn;
+	struct page *page;
+	//struct page_ext *page_ext;
+	unsigned long total_file_mapped = 1; //avoid divided by zero
+	unsigned long idle_file_mapped = 0;
+
+	/* Buffer cache scanning */
+	/* It might be overlapped with idle tracking in the above page walker
+	 * when app do mmap with file. Deal with the case */
+	pfn = min_low_pfn;
+
+	while (!pfn_valid(pfn) && (pfn & (MAX_ORDER_NR_PAGES - 1)) != 0)
+		pfn++;
+
+	for (; pfn < max_pfn; pfn++) {
+		if ((pfn & (MAX_ORDER_NR_PAGES - 1)) == 0 && !pfn_valid(pfn)) {
+			pfn += MAX_ORDER_NR_PAGES - 1;
+			continue;
+		}
+
+		/* Check for holes within a MAX_ORDER area */
+		if (!pfn_valid_within(pfn))
+			continue;
+
+		//page = pfn_to_page(pfn);
+		page = page_idle_get_page(pfn);
+
+		// checked only filemapped page
+		if (page && PageMappedToDisk(page)) {
+			//page_ext = lookup_page_ext(page);
+
+			total_file_mapped++;
+			if (page_is_idle(page))
+				idle_file_mapped++;
+			
+			set_page_idle(page);
+			put_page(page);
+		}
+	}
+
+	trace_printk("idle file page %lu, total file page %lu (ratio %ld) \n",
+			idle_file_mapped, total_file_mapped, 
+			(idle_file_mapped * 100) / total_file_mapped);
+	}
+#endif
+
+	return;
+
+unlock_exit:
+	rcu_read_unlock();
+	return;
+}
+
+static int osa_hpage_scand_has_work(void)
+{
+	return scan_sleep_millisecs && !list_empty(&osa_hpage_scan_list);
+}
+
+static int osa_hpage_scand_wait_event(void)
+{
+#ifdef SCAN_WQ
+	return scan_sleep_millisecs && !list_empty(&osa_hpage_scan_list);
+#else
+	return scan_sleep_millisecs && 
+        (!list_empty(&osa_hpage_scan_list) || kthread_should_stop());
+#endif
+}
+
+static void osa_hpage_scand_wait_work(void) 
+{
+	if (osa_hpage_scand_has_work()) {
+		wait_event_freezable_timeout(osa_hpage_scand_wait,
+				0,
+				msecs_to_jiffies(scan_sleep_millisecs));
+	}
+
+	if (thp_enabled)
+		wait_event_freezable(osa_hpage_scand_wait, osa_hpage_scand_wait_event());
+}
+
+#ifdef SCAND_WQ
+static void osa_hpage_scand(struct work_struct *ws)
+{
+	set_freezable();
+	set_user_nice(current, MAX_NICE);
+
+	while(1) {
+		osa_hpage_do_scan();
+		osa_hpage_scand_wait_work();
+	}
+
+	return ;
+}
+
+static int start_stop_osa_hpage_scand(void)
+{
+	int err = 0;
+
+	if (thp_enabled) {
+		if (!osa_hpage_scand_wq) {
+			osa_hpage_scand_wq = create_singlethread_workqueue("osa_hpage_scand");
+
+			if (osa_hpage_scand_wq) {
+				//schedule_work(osa_hpage_scan_work);
+				INIT_WORK(&osa_hpage_scan_work, osa_hpage_scand);
+				queue_work(osa_hpage_scand_wq, &osa_hpage_scan_work);
+			}
+		}
+
+		if (!list_empty(&osa_hpage_scan_list))
+			wake_up_interruptible(&osa_hpage_scand_wait);
+		
+		if (!osa_aggregation_kthread) {
+			osa_aggregation_kthread = kthread_run(osa_aggregationd, NULL, 
+					"osa_aggregationd");
+			if (unlikely(IS_ERR(osa_aggregation_kthread))) {
+				pr_err("osa_aggregationd: kthread_run(osa_aggregationd) failed\n");
+				err = PTR_ERR(osa_aggregation_kthread);
+				osa_aggregation_kthread = NULL;
+				goto fail;
+			}
+		}
+
+		if (!list_empty(&osa_aggregation_list))
+			wake_up_interruptible(&osa_aggregationd_wait);
+
+	} else if (osa_aggregation_kthread) {
+		// TODO: stop workqueue
+		kthread_stop(osa_aggregation_kthread);
+		osa_aggregation_kthread = NULL;
+	}
+
+fail:
+	return err;
+}
+#else
+
+/* Scanning kthread to gather hugepage stat and idle hugepage tracking */
+static int osa_hpage_scand(void *none)
+{
+	set_freezable();
+	set_user_nice(current, MAX_NICE);
+
+	while(!kthread_should_stop()) {
+		osa_hpage_do_scan();
+		osa_hpage_scand_wait_work();
+	}
+
+	return 0;
+}
+
+static int start_stop_osa_hpage_scand(void)
+{
+	int err = 0;
+	if (thp_enabled) {
+		if (!osa_hpage_scand_kthread) {
+			osa_hpage_scand_kthread = kthread_run(osa_hpage_scand, NULL, 
+					"osa_hpage_scand");
+			if (unlikely(IS_ERR(osa_hpage_scand_kthread))) {
+				pr_err("osa_hpage_scand: kthread_run(osa_hpage_scand) failed\n");
+				err = PTR_ERR(osa_hpage_scand_kthread);
+				osa_hpage_scand_kthread = NULL;
+				goto fail;
+			}
+		}
+
+		if (!list_empty(&osa_hpage_scan_list))
+			wake_up_interruptible(&osa_hpage_scand_wait);
+
+		if (!osa_aggregation_kthread) {
+			osa_aggregation_kthread = kthread_run(osa_aggregationd, NULL, 
+					"osa_aggregationd");
+			if (unlikely(IS_ERR(osa_aggregation_kthread))) {
+				pr_err("osa_aggregationd: kthread_run(osa_aggregationd) failed\n");
+				err = PTR_ERR(osa_aggregation_kthread);
+				osa_aggregation_kthread = NULL;
+				goto fail;
+			}
+		}
+
+		if (!list_empty(&osa_aggregation_list))
+			wake_up_interruptible(&osa_aggregationd_wait);
+
+	} else if (osa_hpage_scand_kthread) {
+		kthread_stop(osa_hpage_scand_kthread);
+		osa_hpage_scand_kthread = NULL;
+
+		kthread_stop(osa_aggregation_kthread);
+		osa_aggregation_kthread = NULL;
+	}
+
+fail:
+	return err;
+}
+#endif
+
+static int osa_hpage_compactd_wait_event(void)
+{
+	return compact_sleep_millisecs || kthread_should_stop();
+}
+
+static void osa_hpage_check_and_do_compact(void)
+{
+	pg_data_t *pgdat;
+	loff_t node = 1;
+	struct zone *_zone = NULL; 
+    unsigned long contig_pages_consumed = 0, order_to_compacted = 0;
+    struct contig_page_info info;
+    int i;
+
+    // For each NUMA node.
+	for (pgdat = first_online_pgdat();
+			pgdat && node;
+			pgdat = next_online_pgdat(pgdat)) {
+        for (i = 0; i < pgdat->nr_zones; i++) {
+            // Skip ZONE_DMA
+            if (i < 1)
+                continue;
+            _zone = &pgdat->node_zones[i];
+
+            fill_contig_page_info(_zone, 10, &info);
+            contig_pages_consumed += info.free_pages_suitable;
+            trace_printk("zone %s: %d\n", 
+                    _zone->name, fragmentation_index(_zone, 10));
+        }
+
+        if (free_contig_pages_consumed >= contig_pages_consumed) {
+            trace_printk("C: %lu\n", 
+                    free_contig_pages_consumed - contig_pages_consumed);
+
+            order_to_compacted = 
+                    (free_contig_pages_consumed - contig_pages_consumed) >> 9;
+        } else
+            trace_printk("G: %lu\n", 
+                    contig_pages_consumed - free_contig_pages_consumed);
+
+		compact_pgdat_periodic(pgdat, 9);
+
+        free_contig_pages_consumed = contig_pages_consumed;
+    }
+
+    return;
+}
+
+static void osa_hpage_compactd_wait_work(void)
+{
+    wait_event_freezable_timeout(osa_hpage_compactd_wait,
+            kthread_should_stop(),
+            msecs_to_jiffies(compact_sleep_millisecs));
+    
+    if (!compact_sleep_millisecs)
+		wait_event_freezable(osa_hpage_compactd_wait, 
+                osa_hpage_compactd_wait_event());
+}
+
+static int osa_hpage_compactd(void *none)
+{
+	set_freezable();
+	set_user_nice(current, MAX_NICE);
+
+	while(!kthread_should_stop()) {
+		if (compact_sleep_millisecs)
+			osa_hpage_check_and_do_compact();
+        osa_hpage_compactd_wait_work();
+	}
+
+    return 0;
+}
+
+static int start_stop_osa_hpage_compactd(void)
+{
+	int err = 0;
+
+	if (thp_enabled) {
+		if (!osa_hpage_compactd_kthread) {
+			osa_hpage_compactd_kthread = kthread_run(osa_hpage_compactd, NULL, 
+					"osa_hpage_compactd");
+			if (unlikely(IS_ERR(osa_hpage_compactd_kthread))) {
+				pr_err("osa_hpage_compactd: kthread_run failed\n");
+				err = PTR_ERR(osa_hpage_compactd_kthread);
+				osa_hpage_compactd_kthread = NULL;
+				goto fail;
+			}
+		}
+
+        free_contig_pages_consumed = 0;
+	    wake_up_interruptible(&osa_hpage_compactd_wait);
+
+	} else if (osa_hpage_compactd_kthread) {
+		kthread_stop(osa_hpage_compactd_kthread);
+		osa_hpage_compactd_kthread = NULL;
+	}
+
+fail:
+	return err;
+}
+
+/* sysfs interface */
+#ifdef CONFIG_SYSFS
+static ssize_t distance_divisor_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", distance_divisor);
+}
+static ssize_t distance_divisor_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	unsigned long divisor;
+	int err;
+
+	err = kstrtoul(buf, 10, &divisor);
+	if (err || divisor > UINT_MAX)
+		return -EINVAL;
+
+	distance_divisor = divisor;
+
+	return count;
+}
+static struct kobj_attribute distance_divisor_attr =
+	__ATTR(distance_divisor, 0644, distance_divisor_show, 
+			distance_divisor_store);
+
+static ssize_t compact_sleep_millisecs_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", compact_sleep_millisecs);
+}
+static ssize_t compact_sleep_millisecs_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	unsigned long msecs;
+	int err;
+
+	err = kstrtoul(buf, 10, &msecs);
+	if (err || msecs > UINT_MAX)
+		return -EINVAL;
+
+	compact_sleep_millisecs = msecs;
+	wake_up_interruptible(&osa_hpage_compactd_wait);
+
+	return count;
+}
+static struct kobj_attribute compact_sleep_millisecs_attr =
+	__ATTR(compact_sleep_millisecs, 0644, compact_sleep_millisecs_show, 
+			compact_sleep_millisecs_store);
+
+static ssize_t scan_sleep_millisecs_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", scan_sleep_millisecs);
+}
+static ssize_t scan_sleep_millisecs_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	unsigned long msecs;
+	int err;
+
+	err = kstrtoul(buf, 10, &msecs);
+	if (err || msecs > UINT_MAX)
+		return -EINVAL;
+
+	scan_sleep_millisecs = msecs;
+	wake_up_interruptible(&osa_hpage_scand_wait);
+
+	return count;
+}
+
+static struct kobj_attribute scan_sleep_millisecs_attr =
+	__ATTR(scan_sleep_millisecs, 0644, scan_sleep_millisecs_show, 
+			scan_sleep_millisecs_store);
+
+static ssize_t aggregation_sleep_millisecs_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", aggregation_sleep_millisecs);
+}
+static ssize_t aggregation_sleep_millisecs_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	unsigned long msecs;
+	int err;
+
+	err = kstrtoul(buf, 10, &msecs);
+	if (err || msecs > UINT_MAX)
+		return -EINVAL;
+
+	aggregation_sleep_millisecs = msecs;
+	wake_up_interruptible(&osa_aggregationd_wait);
+
+	return count;
+}
+static struct kobj_attribute aggregation_sleep_millisecs_attr =
+	__ATTR(aggregation_sleep_millisecs, 0644, aggregation_sleep_millisecs_show, 
+			aggregation_sleep_millisecs_store);
+
+static ssize_t util_threshold_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", util_threshold);
+}
+
+static ssize_t util_threshold_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	int value;
+	int err;
+
+	err = kstrtoint(buf, 10, &value);
+	if (err || value > 100 || value < 0)
+		return -EINVAL;
+
+	util_threshold = value;
+
+	return count;
+}
+static struct kobj_attribute util_threshold_attr =
+	__ATTR(util_threshold, 0644, util_threshold_show, util_threshold_store);
+
+static ssize_t deferred_mode_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	switch(deferred_mode) {
+		case 0:
+			return sprintf(buf, "%u - default\n", deferred_mode);
+		case 1:
+			return sprintf(buf, "%u - async. promotion\n", deferred_mode);
+		case 2:
+			return sprintf(buf, "%u - async. promotion & sampling-based scanning\n", deferred_mode);
+		case 3:
+			return sprintf(buf, "%u - async. & frequency-based promotion & " 
+					"sampling-based scanning\n", deferred_mode);
+		default:
+			return sprintf(buf, "%u - unknown\n", deferred_mode);
+	}
+}
+
+static ssize_t deferred_mode_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	unsigned int value;
+	int err;
+
+	err = kstrtouint(buf, 10, &value);
+	if (err || value > 3)
+		return -EINVAL;
+
+	deferred_mode = value;
+
+	return count;
+}
+static struct kobj_attribute deferred_mode_attr =
+	__ATTR(deferred_mode, 0644, deferred_mode_show, deferred_mode_store);
+
+static ssize_t fairness_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	if (hugepage_fairness == 1) {
+		return sprintf(buf, "enabled\n");
+	}
+	else if (hugepage_fairness == 0)
+		return sprintf(buf, "disabled\n");
+	else
+		return sprintf(buf, "invalid value\n");
+
+	return 0;
+}
+static ssize_t fairness_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	int err;
+	unsigned long do_hugepage_fairness;
+
+	err = kstrtoul(buf, 10, &do_hugepage_fairness);
+	if (err || do_hugepage_fairness > 1)
+		return -EINVAL;
+
+	hugepage_fairness = do_hugepage_fairness;
+
+	err = start_stop_osa_hpage_scand();
+	
+	if (err)
+		return err;
+
+	if (do_hugepage_fairness == 1)
+		wake_up_interruptible(&osa_hpage_scand_wait);
+
+	return count;
+}
+static struct kobj_attribute fairness_attr =
+	__ATTR(fairness, 0644, fairness_show, fairness_store);
+
+static struct attribute *osa_hugepage_attr[] = {
+	&fairness_attr.attr,
+	&deferred_mode_attr.attr,
+	&util_threshold_attr.attr,
+	&scan_sleep_millisecs_attr.attr,
+	&aggregation_sleep_millisecs_attr.attr,
+	&compact_sleep_millisecs_attr.attr,
+	&distance_divisor_attr.attr,
+	NULL,
+};
+
+static struct attribute_group osa_hugepage_attr_group = {
+	.attrs = osa_hugepage_attr,
+};
+
+static int osa_hugepage_init_sysfs(struct kobject **hugepage_kobj)
+{
+	int err;
+
+	*hugepage_kobj = kobject_create_and_add("ingens", 
+			sysfs_hugepage_kobj);
+
+	if (unlikely(!*hugepage_kobj)) {
+		pr_err("failed to create ingens sys kobject\n");
+		return -ENOMEM;
+	}
+
+	err = sysfs_create_group(*hugepage_kobj, &osa_hugepage_attr_group);
+	if (err) {
+		pr_err("failed to register ingens sys group\n");
+		goto delete_kobj;
+	}
+
+	return 0;
+
+delete_kobj:
+	kobject_put(*hugepage_kobj);
+	return err;
+}
+
+static void osa_hugepage_exit_sysfs(struct kobject *hugepage_kobj)
+{
+	sysfs_remove_group(hugepage_kobj, &osa_hugepage_attr_group);
+	kobject_put(hugepage_kobj);
+}
+
+#else
+static int osa_hugepage_init_sysfs(struct kobject **hugepage_kobj)
+{
+	return 0;
+}
+
+static void osa_hugepage_exit_sysfs(struct kobject *hugepage_kobj)
+{
+}
+#endif
+
+static int __init osa_hugepage_init(void)
+{
+	int err;
+	struct kobject *hugepage_kobj;
+
+	INIT_LIST_HEAD(&osa_hpage_scan_list);
+	{
+		int i;
+		for (i = 0; i < 5; i++) 
+			INIT_LIST_HEAD(&osa_hot_page_set[i]);
+	}
+
+	INIT_LIST_HEAD(&osa_aggregation_list);
+
+	err = start_stop_osa_hpage_scand();
+	if (err)
+		goto err_sysfs;
+
+	err = start_stop_osa_hpage_compactd();
+	if (err)
+		goto err_sysfs;
+
+	/* init sysfs */
+	err = osa_hugepage_init_sysfs(&hugepage_kobj);
+	if (err)
+		goto err_sysfs;
+
+	return 0;
+
+	/* not need yet */
+	osa_hugepage_exit_sysfs(hugepage_kobj);
+err_sysfs:
+	return err;
+}
+subsys_initcall(osa_hugepage_init);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/osa/hugepage_util.c linux-4.3-osa/osa/hugepage_util.c
--- linux-4.3-org/osa/hugepage_util.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/osa/hugepage_util.c	2016-03-04 19:57:46.469958897 -0600
@@ -0,0 +1,157 @@
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/slab.h>
+#include <linux/device.h>
+#include <asm/pgtable.h>
+#include <asm/uaccess.h>
+#include <osa/osa.h>
+
+static int osa_hpage_pmd_walker(pmd_t *pmd, unsigned long addr,
+		unsigned long end, struct mm_walk *walk)
+{
+	unsigned long *hpage_count;
+	hpage_count = (unsigned long *)walk->private;
+
+	if (pmd_trans_huge(*pmd)) {
+		(*hpage_count)++;
+	}
+	return 0;
+}
+
+unsigned long osa_get_hpage_count(struct mm_struct *mm)
+{
+	unsigned long hpage_count = 0;
+	struct vm_area_struct *vma = NULL;
+	struct mm_walk _hpage_walker = {
+		.pmd_entry = osa_hpage_pmd_walker,
+		.private = &hpage_count,
+	};
+
+	//task = find_task_by_vpid(pid);
+	//mm = get_task_mm(task);
+
+	vma = mm->mmap;
+	_hpage_walker.mm = vma->vm_mm;
+
+	while (vma != NULL) {
+		if (vma->vm_mm) 
+			walk_page_range(vma->vm_start, vma->vm_end, &_hpage_walker);
+
+		vma = vma->vm_next;
+	}
+
+	return hpage_count;
+}
+
+///////////////////////////////////////////////////////////////////////////
+#define DEVICE_NAME "osa_dev"
+#define CLASS_NAME "osa"
+#define OSA_BLOCK_SIZE (16 << 10)
+
+static struct class*  osa_char_class  = NULL; 
+static struct device* osa_char_device = NULL; 
+static int majorNumber;
+static int is_allocated = 0;
+static char **osa_dev_mem = NULL;
+static unsigned int nr_mem_block = ((350 << 20) / OSA_BLOCK_SIZE);
+
+static ssize_t osa_dev_read(struct file *filep, char *buffer, 
+		size_t len, loff_t *offset)
+{
+	return 0;
+}
+
+static ssize_t osa_dev_write(struct file *filep, const char *buffer, 
+		size_t len, loff_t *offset)
+{
+	char message[16] = {0};  
+	unsigned long flag;
+	unsigned int i;
+
+	if (copy_from_user(message, buffer, 16)) 
+		return -EFAULT;
+
+	flag = simple_strtoul(message, NULL, 10);
+
+	switch (flag) {
+		case 0:
+			if (is_allocated) {
+				for (i = 0; i < nr_mem_block; i++) 
+					kfree(osa_dev_mem[i]);
+			}
+			is_allocated = 0;
+			break;
+		case 1:
+			if (!is_allocated) {
+				for (i = 0; i < nr_mem_block; i++)  {
+					osa_dev_mem[i] = kmalloc(OSA_BLOCK_SIZE, GFP_KERNEL);
+					if (!osa_dev_mem[i])
+						printk("osa_dev: fail to alloc %dth block\n", i);
+				}
+
+			}
+			is_allocated = 1;
+			break;
+		default:
+			printk("osa_dev: Invalid command %lu\n", flag);
+	}
+
+	return len;
+}
+
+static int osa_dev_open(struct inode *inodep, struct file *filep)
+{
+	return 0;
+}
+
+static int osa_dev_release(struct inode *inodep, struct file *filep)
+{
+	return 0;
+}
+
+static struct file_operations fops =
+{
+	.open = osa_dev_open,
+	.read = osa_dev_read,
+	.write = osa_dev_write,
+	.release = osa_dev_release,
+};
+
+static int __init osa_alloc_unmovable_memory(void)
+{
+	majorNumber = register_chrdev(0, DEVICE_NAME, &fops);
+	if (majorNumber<0){
+		printk(KERN_ALERT "failed to register a major number\n");
+		return majorNumber;
+	}
+
+	printk(KERN_INFO "osa_dev: registered correctly with major number %d\n",
+			majorNumber);
+
+	// Register the device class
+	osa_char_class = class_create(THIS_MODULE, CLASS_NAME);
+	if (IS_ERR(osa_char_class)) { 
+		unregister_chrdev(majorNumber, DEVICE_NAME);
+		printk(KERN_ALERT "Failed to register device class\n");
+		return PTR_ERR(osa_char_class); 
+	}
+	printk(KERN_INFO "osa_dev: device class registered correctly\n");
+
+	// Register the device driver
+	osa_char_device = device_create(osa_char_class, 
+			NULL, MKDEV(majorNumber, 0), NULL, DEVICE_NAME);
+	if (IS_ERR(osa_char_device)){      
+		class_destroy(osa_char_class);   
+		unregister_chrdev(majorNumber, DEVICE_NAME);
+		printk(KERN_ALERT "Failed to create the device\n");
+		return PTR_ERR(osa_char_device);
+	}
+	printk(KERN_INFO "osa_char: device class created correctly\n");
+
+	osa_dev_mem = kmalloc(sizeof(char *) * nr_mem_block, GFP_KERNEL);
+
+	return 0;
+}
+
+subsys_initcall(osa_alloc_unmovable_memory);
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/osa/Makefile linux-4.3-osa/osa/Makefile
--- linux-4.3-org/osa/Makefile	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/osa/Makefile	2016-11-02 13:51:55.419155566 -0500
@@ -0,0 +1,9 @@
+#
+# Makefile for osa subdirectory.
+#
+# Youngjin Kwon <yjkwon@cs.utexas.edu>
+# 
+
+obj-$(CONFIG_OSA) += hugepage_proc.o
+obj-$(CONFIG_OSA) += hugepage_util.o
+obj-$(CONFIG_OSA) += hugepage_scan.o
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/scripts/gdb/linux/osa.py linux-4.3-osa/scripts/gdb/linux/osa.py
--- linux-4.3-org/scripts/gdb/linux/osa.py	1969-12-31 18:00:00.000000000 -0600
+++ linux-4.3-osa/scripts/gdb/linux/osa.py	2016-04-11 18:03:15.377269349 -0500
@@ -0,0 +1,25 @@
+import gdb
+
+from linux import utils
+
+
+def osa_page_to_pfn(page):
+    gdb.write("page_to_pfn")
+    return
+
+class osaPageUtil(gdb.Command):
+    """ useful tools for page """
+
+    def __init__(self):
+        super(osaPageUtil, self).__init__("lx-page_to_pfn", gdb.COMMAND_DATA,
+                gdb.COMPLETE_EXPRESSION)
+
+    def invoke(self, arg, from_tty):
+        argv = gdb.string_to_argv(arg)
+
+        if len(argv) != 1:
+            raise gdb.GdbError("need arguement of struct page")
+
+        osa_page_to_pfn(argv[0])
+
+osaPageUtil()
diff -urN '--exclude=.git*' '--exclude=*.config*' '--exclude=tag*' '--exclude=*cscope*' '--exclude=*.o' '--exclude=tools*' '--exclude=Documentation*' '--exclude=*vmlinux.scr*' '--exclude=*.tmp' linux-4.3-org/scripts/gdb/vmlinux-gdb.py linux-4.3-osa/scripts/gdb/vmlinux-gdb.py
--- linux-4.3-org/scripts/gdb/vmlinux-gdb.py	2015-11-01 18:05:25.000000000 -0600
+++ linux-4.3-osa/scripts/gdb/vmlinux-gdb.py	2016-09-20 00:21:29.170619531 -0500
@@ -29,3 +29,4 @@
     import linux.tasks
     import linux.cpus
     import linux.lists
+    import linux.osa
